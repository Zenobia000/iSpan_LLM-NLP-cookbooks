{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain 0.3+ Embedding 模型比較\n",
    "比較不同 Embedding 模型的性能與特性\n",
    "\n",
    "需求套件:\n",
    "- langchain>=0.3.0\n",
    "- langchain-community>=0.0.1\n",
    "- sentence-transformers>=2.2.2\n",
    "- openai>=1.1.0\n",
    "- cohere>=4.37\n",
    "- pandas>=2.0.0\n",
    "- numpy>=1.24.0\n",
    "- python-dotenv>=0.19.0\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方法特性分析表\n",
    "\n",
    "| 特性           | OpenAI Embedding | CohereAI Embedding | Jina Embeddings | BGE (BAAI) |\n",
    "|---------------|:----------------:|:------------------:|:---------------:|:----------:|\n",
    "| **語義準確性** |        ○         |         ○          |        △        |     ○      |\n",
    "| **計算成本**   |        △         |         △          |        ○        |     ○      |\n",
    "| **領域適應性** |        ○         |         ○          |        △        |     ○      |\n",
    "| **可擴展性**   |        ○         |         ○          |        ○        |     ○      |\n",
    "| **訓練效率**   |        ○         |         ○          |        △        |     ○      |\n",
    "| **查詢效率**   |        ○         |         ○          |        △        |     ○      |\n",
    "| **多模態支持** |        ○         |         ○          |        ×        |     ○      |\n",
    "\n",
    "# 問題特性分析表\n",
    "\n",
    "| 應用領域        | OpenAI Embedding | CohereAI Embedding | Jina Embeddings | BGE (BAAI) |\n",
    "|-----------------|:----------------:|:------------------:|:---------------:|:----------:|\n",
    "| **資訊檢索（IR）** |        ○         |         ○          |        △        |     ○      |\n",
    "| **推薦系統（RS）** |        ○         |         ○          |        △        |     ○      |\n",
    "| **問答系統（QA）** |        ○         |         ○          |        △        |     ○      |\n",
    "| **文本分類（TC）** |        ○         |         ○          |        ○        |     ○      |\n",
    "| **知識圖譜（KG）** |        ○         |         ○          |        △        |     ○      |\n",
    "| **多模態處理（MM）** |        ○         |         ○          |        ×        |     ○      |\n",
    "\n",
    "# 方法特性 vs. 問題特性 矩陣比較表\n",
    "\n",
    "| 方法特性 / 應用領域 | IR（資訊檢索） | RS（推薦系統） | QA（問答系統） | TC（文本分類） | KG（知識圖譜） | MM（多模態處理） |\n",
    "|--------------------|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n",
    "| **語義準確性**     |       ○       |       ○       |       ○       |       ○       |       ○       |       △       |\n",
    "| **計算成本**       |       △       |       ○       |       △       |       ○       |       △       |       ○       |\n",
    "| **領域適應性**     |       ○       |       ○       |       ○       |       ○       |       ○       |       △       |\n",
    "| **可擴展性**       |       ○       |       ○       |       ○       |       ○       |       △       |       ○       |\n",
    "| **訓練效率**       |       ○       |       ○       |       ○       |       ○       |       ○       |       △       |\n",
    "| **查詢效率**       |       ○       |       ○       |       ○       |       ○       |       △       |       △       |\n",
    "| **多模態支持**     |       ×       |       △       |       △       |       ×       |       △       |       ○       |\n",
    "\n",
    "# 符號意義\n",
    "\n",
    "- **○**：表現優異或高度相關\n",
    "- **△**：表現一般或部分適用\n",
    "- **×**：表現較差或不適用\n",
    "\n",
    "## 指標量化\n",
    "\n",
    "| 指標名稱   | 量化評測方式                          | 可參考基準           |\n",
    "|------------|----------------------------------|------------------|\n",
    "| 語義準確性  | 平均排名（MRR）/NDCG@K           | BEIR、MTEB       |\n",
    "| 計算成本  | FLOPs（浮點運算量）/ 記憶體使用率 | Papers With Code |\n",
    "| 領域適應性  | MMLU領域分數（如醫療/法律/科技）  | MMLU             |\n",
    "| 可擴展性  | 檢索時間 vs. 數據規模（log-scale） | RAG pipeline     |\n",
    "| 訓練效率  | 訓練步數 vs. 目標loss 收斂時間     | LLAMA / GPT 研究 |\n",
    "| 查詢效率  | 查詢平均延遲（毫秒）               | BEIR、MTEB       |\n",
    "| 多模態支持  | 文字-圖片嵌入相似度（CLIP-based）  | CLIP/MultiBench  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法特性分析表\n",
    "特性\t\n",
    "- 語義準確性:\t嵌入模型對於文本語義的理解能力，影響資訊檢索、問答系統等應用。\n",
    "- 計算成本:\t訓練與推理過程中所需的計算資源，關係到模型的運行效率與部署成本。\n",
    "- 領域適應性:\t模型在特定領域（如醫學、法律、金融等）中的表現，影響專業應用的效果。\n",
    "- 可擴展性:\t模型是否能夠有效應對大規模數據處理，例如高併發檢索應用。\n",
    "- 訓練效率:\t模型收斂速度與所需的訓練數據量，影響研發與迭代週期。\n",
    "- 查詢效率:\t模型在檢索與推理時的響應速度，影響即時應用的體驗。\n",
    "- 多模態支持:\t模型是否支援圖像、語音與文本的整合處理，影響多媒體應用場景。\n",
    "\n",
    "問題特性分析表\n",
    "應用領域\t\n",
    "- 資訊檢索（IR）:\t評估模型在搜索系統、企業文檔檢索中的表現，如Google Search、企業知識庫。\n",
    "- 推薦系統（RS）:\t測試嵌入模型在電商、影音平台的個性化推薦效果，如YouTube、Netflix推薦算法。\n",
    "- 問答系統（QA）:\t量測模型在即時問答、技術支援上的準確性，如ChatGPT、客服機器人。\n",
    "- 文本分類（TC）:\t檢驗模型在垃圾郵件檢測、情感分析、主題分類上的表現，如新聞分類、社群監測。\n",
    "- 知識圖譜（KG）:\t測試模型在構建知識圖譜、關係推理、語義搜索上的能力，如維基數據、企業知識管理。\n",
    "- 多模態處理（MM）:\t評估模型在跨模態數據（文本+圖像+音頻）處理的表現，如OCR+文本分析、影像字幕生成。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import logging\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# 設定日誌\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 載入環境變數\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "jina_api_key = os.getenv(\"JINA_API_KEY\")\n",
    "bge_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LLM 嵌入模型 + FAISS 向量資料庫 Benchmark 測試 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:52:23,952 - INFO - 測試 FAISS (模型: openai, 數據集: scifact) ...\n",
      "2025-02-15 17:52:24,255 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:24,803 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:25,428 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:25,732 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:25,979 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:26,601 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:26,897 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:27,592 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:27,885 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:28,512 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:28,981 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:39,613 - INFO - 測試 FAISS (模型: cohere, 數據集: scifact) ...\n",
      "2025-02-15 17:52:39,980 - INFO - HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:40,252 - INFO - HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:40,465 - INFO - HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:40,681 - INFO - HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:40,908 - INFO - HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:41,126 - INFO - HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:41,353 - INFO - HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:41,567 - INFO - HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:41,786 - INFO - HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:42,004 - INFO - HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:42,233 - INFO - HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 17:52:44,538 - INFO - 測試 FAISS (模型: jina, 數據集: scifact) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "測試結果:\n",
      "  Embedding Model  Dataset  Num Samples  Insert Time (s)  Query Time (s)  \\\n",
      "0          openai  scifact           10           0.5520          0.4479   \n",
      "1          cohere  scifact           10           0.4178          0.2212   \n",
      "2            jina  scifact           10           1.6467          0.4676   \n",
      "\n",
      "   Memory Usage (MB)     MRR  NDCG@10  \n",
      "0             616.88  0.3315   0.2931  \n",
      "1             653.95  0.3365   0.3635  \n",
      "2             654.39  0.3657   0.3492  \n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import CohereEmbeddings, JinaEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# 設定日誌\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EmbeddingBenchmark:\n",
    "    \"\"\"LLM 嵌入模型性能測試（使用 FAISS）\"\"\"\n",
    "\n",
    "    def __init__(self, num_samples=1000, embedding_dim=384, model=\"openai\", dataset=\"scifact\"):\n",
    "        \"\"\"初始化測試數據\"\"\"\n",
    "        self.num_samples = num_samples\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.model_name = model\n",
    "        self.dataset_name = dataset\n",
    "        self.embeddings = self.load_embedding_model(model)\n",
    "\n",
    "        # 載入測試數據\n",
    "        self.query_texts, self.index_texts, self.ground_truth = self.load_dataset_samples(dataset, num_samples)\n",
    "\n",
    "    def load_embedding_model(self, model):\n",
    "        \"\"\"載入不同的嵌入模型\"\"\"\n",
    "        if model == \"openai\":\n",
    "            return OpenAIEmbeddings()\n",
    "        elif model == \"cohere\":\n",
    "            from langchain_cohere import CohereEmbeddings\n",
    "            return CohereEmbeddings(model=\"embed-english-light-v3.0\")\n",
    "        elif model == \"jina\":\n",
    "            return JinaEmbeddings()\n",
    "        elif model == \"bge\":\n",
    "            return HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en\")\n",
    "        else:\n",
    "            raise ValueError(f\"不支援的嵌入模型: {model}\")\n",
    "\n",
    "    def load_dataset_samples(self, dataset_name, num_samples):\n",
    "        \"\"\"載入標準數據集樣本，確保查詢 (claims) 和索引 (abstracts) 分開\"\"\"\n",
    "        try:\n",
    "            # 先載入 claims 作為查詢集\n",
    "            claims_dataset = load_dataset(dataset_name, name=\"claims\", split=\"train\", trust_remote_code=True)\n",
    "            queries = claims_dataset[\"claim\"][:num_samples]\n",
    "            \n",
    "            # 再載入 corpus 作為索引庫\n",
    "            corpus_dataset = load_dataset(dataset_name, name=\"corpus\", split=\"train\", trust_remote_code=True)\n",
    "            index_texts = [str(a) for a in corpus_dataset[\"abstract\"][:num_samples]]  \n",
    "\n",
    "            # Ground truth 設定：claims 應該對應 corpus 內的某些 text\n",
    "            ground_truth = index_texts[:num_samples]  # 這裡應根據對應關係調整\n",
    "\n",
    "            return queries, index_texts, ground_truth\n",
    "        except Exception as e:\n",
    "            logger.error(f\"載入數據集 {dataset_name} 時發生錯誤: {str(e)}\")\n",
    "            return [\"Query\" for _ in range(num_samples)], [\"Index\" for _ in range(num_samples)], [\"Ground Truth\" for _ in range(num_samples)]\n",
    "\n",
    "    def memory_usage(self):\n",
    "        \"\"\"取得記憶體使用量 (MB)\"\"\"\n",
    "        return psutil.Process().memory_info().rss / 1024 / 1024\n",
    "\n",
    "    def calculate_mrr(self, results, ground_truth):\n",
    "        \"\"\"計算 MRR（Mean Reciprocal Rank）\"\"\"\n",
    "        ranks = []\n",
    "        for idx, docs in enumerate(results):\n",
    "            for rank, doc in enumerate(docs, start=1):\n",
    "                if doc.page_content == ground_truth[idx]:\n",
    "                    ranks.append(1 / rank)\n",
    "                    break\n",
    "        return sum(ranks) / len(ranks) if ranks else 0\n",
    "\n",
    "    def calculate_ndcg(self, results, ground_truth, k=10):\n",
    "        \"\"\"計算 NDCG@K\"\"\"\n",
    "        y_true = [[1 if doc.page_content == ground_truth[idx] else 0 for doc in docs] for idx, docs in enumerate(results)]\n",
    "        y_score = [[1 / (i+1) for i in range(len(docs))] for docs in results]\n",
    "        return ndcg_score(y_true, y_score, k=k)\n",
    "\n",
    "    def evaluate_faiss(self):\n",
    "        \"\"\"測試 FAISS 向量資料庫\"\"\"\n",
    "        logger.info(f\"測試 FAISS (模型: {self.model_name}, 數據集: {self.dataset_name}) ...\")\n",
    "\n",
    "        # 建立索引\n",
    "        start_time = time.time()\n",
    "        vectorstore = FAISS.from_texts(self.index_texts, embedding=self.embeddings)\n",
    "        insert_time = time.time() - start_time\n",
    "\n",
    "        # 進行查詢\n",
    "        results = []\n",
    "        start_time = time.time()\n",
    "        for query in self.query_texts:\n",
    "            query_vector = self.embeddings.embed_query(query)\n",
    "            top_k_results = vectorstore.similarity_search_by_vector(query_vector, k=10)\n",
    "            results.append(top_k_results)\n",
    "        query_time = (time.time() - start_time) / len(self.query_texts)\n",
    "\n",
    "        # 計算 MRR & NDCG\n",
    "        mrr_score = self.calculate_mrr(results, self.ground_truth)\n",
    "        ndcg_score_k = self.calculate_ndcg(results, self.ground_truth, k=5)\n",
    "\n",
    "        return {\n",
    "            \"Embedding Model\": self.model_name,\n",
    "            \"Dataset\": self.dataset_name,\n",
    "            \"Num Samples\": self.num_samples,\n",
    "            \"Insert Time (s)\": round(insert_time, 4),\n",
    "            \"Query Time (s)\": round(query_time, 4),\n",
    "            \"Memory Usage (MB)\": round(self.memory_usage(), 2),\n",
    "            \"MRR\": round(mrr_score, 4),\n",
    "            \"NDCG@10\": round(ndcg_score_k, 4),\n",
    "        }\n",
    "\n",
    "    def run_benchmark(self):\n",
    "        \"\"\"執行測試\"\"\"\n",
    "        result = self.evaluate_faiss()\n",
    "        df = pd.DataFrame([result])\n",
    "        return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"主程式\"\"\"\n",
    "    print(\"\\n=== LLM 嵌入模型 + FAISS 向量資料庫 Benchmark 測試 ===\\n\")\n",
    "    models = [\"openai\", \"cohere\", \"jina\"]  # 先測試一個模型\n",
    "    datasets = [\"scifact\"]  # 只測試一個數據集\n",
    "    sample_sizes = [10]  # 測試 1000 筆數據\n",
    "\n",
    "    all_results = []\n",
    "    for model in models:\n",
    "        for dataset in datasets:\n",
    "            for num_samples in sample_sizes:\n",
    "                try:\n",
    "                    benchmark = EmbeddingBenchmark(\n",
    "                        num_samples=num_samples,\n",
    "                        embedding_dim=384,\n",
    "                        model=model,\n",
    "                        dataset=dataset\n",
    "                    )\n",
    "                    results = benchmark.run_benchmark()\n",
    "                    all_results.append(results)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"執行 benchmark 時發生錯誤 (model={model}, dataset={dataset}): {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "    if not all_results:\n",
    "        logger.warning(\"沒有成功的測試結果\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(\"\\n測試結果:\")\n",
    "    print(final_df)\n",
    "    return final_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_result = main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
