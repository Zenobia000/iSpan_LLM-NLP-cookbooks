{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain 0.3+ 相似度搜尋方法比較\n",
    "比較不同相似度搜尋方法的性能與特性\n",
    "\n",
    "需求套件:\n",
    "- langchain>=0.3.0\n",
    "- langchain-community>=0.0.1\n",
    "- rank_bm25>=0.2.2\n",
    "- scikit-learn>=1.3.0\n",
    "- pandas>=2.0.0\n",
    "- numpy>=1.24.0\n",
    "- python-dotenv>=0.19.0\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search 檢索技術分析 (基於固定 Embedding Model 與 FAISS 向量資料庫)\n",
    "\n",
    "---\n",
    "\n",
    "## 方法特性分析表\n",
    "\n",
    "| **特性**         | FAISS | Annoy | ScaNN | HNSW | DPR (Dense Retriever) | ColBERT | Contriever | Hybrid Search (BM25+Vectors) |\n",
    "|-----------------|:-----:|:-----:|:-----:|:-----:|:---------------------:|:-------:|:---------:|:--------------------------:|\n",
    "| **檢索準確性**   |   △   |   △   |   ○   |   ○   |           ○           |    ○    |     ○     |             ○              |\n",
    "| **檢索延遲**     |   ○   |   ○   |   ○   |   △   |           △           |    △    |     △     |             △              |\n",
    "| **擴展性**       |   ○   |   ○   |   ○   |   ○   |           △           |    △    |     △     |             △              |\n",
    "| **計算成本**     |   ○   |   ○   |   ○   |   △   |           △           |    △    |     △     |             △              |\n",
    "| **索引更新效率** |   △   |   △   |   ○   |   ○   |           ×           |    ×    |     ×     |             △              |\n",
    "| **查詢效率**     |   ○   |   ○   |   ○   |   △   |           △           |    △    |     △     |             △              |\n",
    "\n",
    "---\n",
    "\n",
    "## 問題特性分析表\n",
    "\n",
    "| **應用領域**        | FAISS | Annoy | ScaNN | HNSW | DPR (Dense Retriever) | ColBERT | Contriever | Hybrid Search (BM25+Vectors) |\n",
    "|-----------------|:-----:|:-----:|:-----:|:-----:|:---------------------:|:-------:|:---------:|:--------------------------:|\n",
    "| **近似最近鄰檢索（ANN）**  |   ○   |   ○   |   ○   |   ○   |           △           |    △    |     △     |             △              |\n",
    "| **大規模數據檢索**   |   ○   |   △   |   ○   |   ○   |           △           |    △    |     △     |             △              |\n",
    "| **低延遲檢索**     |   ○   |   ○   |   ○   |   △   |           △           |    △    |     △     |             △              |\n",
    "| **文本相似性檢索**   |   △   |   △   |   ○   |   ○   |           ○           |    ○    |     ○     |             ○              |\n",
    "| **知識檢索（Knowledge Retrieval）** |   △   |   △   |   ○   |   ○   |           ○           |    ○    |     ○     |             ○              |\n",
    "\n",
    "---\n",
    "\n",
    "## 方法特性 vs. 問題特性 矩陣比較表\n",
    "\n",
    "| **方法特性 / 應用領域** | **ANN（近似最近鄰）** | **大規模數據檢索** | **低延遲檢索** | **文本相似性檢索** | **知識檢索** |\n",
    "|----------------------|:----------------:|:----------------:|:--------------:|:----------------:|:--------------:|\n",
    "| **檢索準確性**       |        △        |        ○        |        △        |        ○        |        ○        |\n",
    "| **檢索延遲**         |        ○        |        △        |        ○        |        △        |        △        |\n",
    "| **擴展性**           |        ○        |        ○        |        △        |        △        |        △        |\n",
    "| **計算成本**         |        ○        |        ○        |        △        |        △        |        △        |\n",
    "| **索引更新效率**     |        △        |        △        |        △        |        ×        |        ×        |\n",
    "| **查詢效率**         |        ○        |        ○        |        ○        |        △        |        △        |\n",
    "\n",
    "---\n",
    "\n",
    "## LLM RAG 內部檢索技術影響分析\n",
    "\n",
    "LLM RAG 本質上依賴向量檢索技術來進行檢索擴充，以下是不同檢索技術在 LLM RAG 內的影響：\n",
    "\n",
    "| **檢索技術** | **影響點** |\n",
    "|-------------|-----------|\n",
    "| **FAISS** | 高效檢索速度，適合大規模知識檢索，但無法進行語義推理 |\n",
    "| **ScaNN** | 速度優於 FAISS，適合低延遲檢索應用 |\n",
    "| **DPR (Dense Retriever)** | 語義檢索準確度高，但擴展性受限，對大規模數據檢索成本較高 |\n",
    "| **ColBERT** | 高語義檢索能力，適合精準知識檢索，但查詢延遲較高 |\n",
    "| **Hybrid Search (BM25+Vectors)** | 提供關鍵字檢索+語義檢索混合方案，適合通用應用 |\n",
    "  \n",
    "---\n",
    "\n",
    "## 符號意義\n",
    "- **○**：表現優異或高度相關  \n",
    "- **△**：表現一般或部分適用  \n",
    "- **×**：表現較差或不適用  \n",
    "\n",
    "---\n",
    "\n",
    "## 指標量化\n",
    "\n",
    "| **指標名稱**    | **量化評測方式**                      | **可參考基準**          |\n",
    "|----------------|--------------------------------|------------------|\n",
    "| **檢索準確性**  | Recall@K / Precision@K / MRR | BEIR、MTEB       |\n",
    "| **檢索延遲**    | 查詢平均延遲（毫秒）         | BEIR、MTEB       |\n",
    "| **擴展性**      | 檢索時間 vs. 數據規模（log-scale） | FAISS Benchmark  |\n",
    "| **計算成本**    | FLOPs（浮點運算量）/ 記憶體使用率 | Papers With Code |\n",
    "| **索引更新效率** | 新數據插入對查詢效率的影響 | FAISS Benchmark  |\n",
    "| **查詢效率**    | 每秒查詢數（QPS）             | BEIR、MTEB       |\n",
    "\n",
    "---\n",
    "\n",
    "## 適用場景推薦\n",
    "\n",
    "| **應用場景** | **推薦技術** |\n",
    "|-------------|-------------|\n",
    "| **近似最近鄰檢索（ANN）** | FAISS, ScaNN, HNSW |\n",
    "| **大規模數據檢索** | FAISS, ScaNN, HNSW |\n",
    "| **低延遲檢索** | FAISS, Annoy, ScaNN |\n",
    "| **文本相似性檢索** | DPR, ColBERT, Contriever, Hybrid Search |\n",
    "| **知識檢索** | DPR, ColBERT, Contriever, Hybrid Search |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "from annoy import AnnoyIndex\n",
    "from sklearn.preprocessing import normalize\n",
    "import hnswlib\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定日誌\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 載入環境變數\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "jina_api_key = os.getenv(\"JINA_API_KEY\")\n",
    "bge_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SearchMetrics:\n",
    "    \"\"\"搜尋方法評估指標\"\"\"\n",
    "    name: str\n",
    "    search_time: float\n",
    "    precision_at_k: float\n",
    "    recall_at_k: float\n",
    "    mrr: float\n",
    "    ndcg: float\n",
    "    diversity: float\n",
    "    explanation: str\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    \"\"\"搜尋結果\"\"\"\n",
    "    content: str\n",
    "    score: float\n",
    "    metadata: Dict[str, Any]\n",
    "    method: str\n",
    "    search_time: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data() -> Tuple[List[str], List[str], List[List[int]]]:\n",
    "    \"\"\"準備測試資料集\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[List[str], List[str], List[List[int]]]: \n",
    "            (文檔列表, 查詢列表, 相關度矩陣)\n",
    "    \"\"\"\n",
    "    # 文檔集合：不同長度、主題和複雜度\n",
    "    documents = [\n",
    "        # 科技類 (0-7)\n",
    "        \"台積電是全球最大的晶圓代工企業，總部位於新竹科學園區。公司專注於先進製程技術研發，在3奈米、5奈米等製程居於領先地位。\",\n",
    "        \"蘋果公司新款iPhone採用台積電的4奈米製程晶片，具備更強大的運算能力和更低的功耗。預計年底在台灣量產。\",\n",
    "        \"聯發科發表最新5G晶片，採用先進製程，整合AI運算單元，主打中高階手機市場。預計在第三季開始出貨。\",\n",
    "        \"華碩推出新一代電競筆電，搭載最新的NVIDIA RTX顯示卡，採用獨特的散熱設計，螢幕更新率達360Hz。\",\n",
    "        \"大立光是全球最大的手機鏡頭供應商，為iPhone等高階手機提供光學元件。公司持續投資研發，布局AR/VR領域。\",\n",
    "        \"群創光電發表全球首款摺疊式OLED面板，可向內外摺疊，解析度達4K，將用於下一代摺疊手機。\",\n",
    "        \"研華科技推出工業物聯網平台，整合邊緣運算和雲端服務，協助製造業實現智慧化轉型。\",\n",
    "        \"緯創資通在台中設立新廠，主要生產伺服器和網路設備，預計創造上千個就業機會。\",\n",
    "        \n",
    "        # 旅遊類 (8-15)\n",
    "        \"台北101是台灣最高的摩天大樓，高達509.2公尺。大樓內有觀景台、購物中心和米其林餐廳。每年跨年煙火是重要景點。\",\n",
    "        \"日月潭是台灣最大的淡水湖泊，環湖步道景色優美。春季可以看櫻花，夏季可以遊湖，秋季可以賞楓葉，冬季可以觀星。\",\n",
    "        \"阿里山森林遊樂區以日出、雲海、森林鐵路、巨木和晚霞聞名。每年三月賞櫻季是最熱門的旅遊時節。\",\n",
    "        \"太魯閣國家公園以峽谷地形著稱，清水斷崖高聳入雲，砂卡礑步道沿著溪流蜿蜒，是台灣最受歡迎的國家公園之一。\",\n",
    "        \"九份老街保留完整的日治時期建築，茶樓、紅燈籠與石階街道，充滿懷舊氛圍。夜景尤其迷人，常吸引大量觀光客。\",\n",
    "        \"墾丁國家公園擁有潔白的沙灘和湛藍的海水，是台灣最南端的度假勝地。每年春天的音樂節吸引許多年輕人參加。\",\n",
    "        \"七星潭是花蓮最著名的景點之一，彎月形的海灣佈滿黑色卵石，清晨可以欣賞日出，傍晚可以漫步海岸。\",\n",
    "        \"陽明山國家公園以溫泉、花季和火山地形聞名，大屯山、七星山等火山群環繞，春天的杜鵑花季最為壯觀。\",\n",
    "        \n",
    "        # 美食類 (16-23)\n",
    "        \"鼎泰豐的小籠包在米其林指南獲得推薦，以18摺的完美褶皺聞名，湯汁豐富，外皮有嚼勁。\",\n",
    "        \"臭豆腐是台灣夜市必吃小吃，外酥內嫩，搭配泡菜和蒜蓉辣醬最對味。以大腸麵線和珍珠奶茶聞名。\",\n",
    "        \"牛肉麵的湯頭以紅燒和清燉為主，牛肉軟嫩，麵條Q彈。台北市舉辦的牛肉麵節是年度美食盛事。\",\n",
    "        \"度小月擔仔麵是台南百年老店，以乾麵為主，配上新鮮蝦仁，湯頭鮮美，是台南必吃小吃。\",\n",
    "        \"阿宗麵線是台北西門町的人氣小吃，以大腸麵線聞名，湯頭濃郁，加上香菜和黑醋更添風味。\",\n",
    "        \"雪王冰淇淋是台南老字號，以水果冰淇淋聞名，芒果、荔枝等口味都使用新鮮水果製作。\",\n",
    "        \"高雄六合夜市的海鮮粥使用新鮮海產，香菇、蝦仁、魚肉豐富，是高雄必吃美食。\",\n",
    "        \"基隆廟口夜市的天婦羅、藥燉排骨、鹹酥雞都是招牌小吃，在地人從小吃到大。\",\n",
    "        \n",
    "        # 文化類 (24-31)\n",
    "        \"故宮博物院收藏大量中華文物，包括翠玉白菜、毛公鼎等國寶。每年吸引數百萬遊客參觀。定期舉辦特展。\",\n",
    "        \"台北當代藝術館位於舊市政廳，定期展出現代藝術作品，推廣台灣當代藝術，舉辦藝術教育活動。\",\n",
    "        \"林家花園是板橋知名古蹟，建於清朝，是台灣最完整的園林建築，體現傳統閩南建築特色。\",\n",
    "        \"十鼓仁糖文創園區由廢棄糖廠改建，結合擊鼓表演與工業遺址，展現台灣文創轉型的成功案例。\",\n",
    "        \"鶯歌陶瓷博物館展示台灣陶瓷發展史，提供陶藝DIY體驗，是認識台灣陶瓷文化的重要場所。\",\n",
    "        \"台南孔廟是台灣最古老的孔廟，建於1665年，每年舉行祭孔大典，展現傳統文化儀式。\",\n",
    "        \"蘭陽博物館以獨特的建築設計聞名，展示宜蘭的人文歷史，是台灣新型態博物館的代表。\",\n",
    "        \"台灣文學館位於台南古蹟建築內，收藏豐富的台灣文學史料，定期舉辦文學講座和展覽。\"\n",
    "    ]\n",
    "    \n",
    "    # 查詢及其預期相關文檔\n",
    "    queries = [\n",
    "        \"台灣最先進的半導體製程技術\",  # 科技類查詢\n",
    "        \"最新的顯示器技術發展\",        # 科技類查詢\n",
    "        \"台北最受歡迎的觀光景點\",      # 旅遊類查詢\n",
    "        \"台灣最美的自然風景區\",        # 旅遊類查詢\n",
    "        \"台灣最有名的小吃有哪些\",      # 美食類查詢\n",
    "        \"台南最具特色的美食\",          # 美食類查詢\n",
    "        \"哪裡可以看到珍貴文物\",        # 文化類查詢\n",
    "        \"台灣最具代表性的古蹟\"         # 文化類查詢\n",
    "    ]\n",
    "    \n",
    "    # 相關度矩陣（0-2分，2分最相關）\n",
    "    relevance = [\n",
    "        # 半導體查詢的相關度 \"台灣最先進的半導體製程技術\"\n",
    "        [2, 2, 1, 0, 1, 0, 0, 0,  # 科技類 (0-7)\n",
    "         0, 0, 0, 0, 0, 0, 0, 0,  # 旅遊類 (8-15)\n",
    "         0, 0, 0, 0, 0, 0, 0, 0,  # 美食類 (16-23)\n",
    "         0, 0, 0, 0, 0, 0, 0, 0], # 文化類 (24-31)\n",
    "        \n",
    "        # 顯示器查詢的相關度 \"最新的顯示器技術發展\"\n",
    "        [0, 0, 0, 1, 0, 2, 0, 0,  # 科技類\n",
    "         0, 0, 0, 0, 0, 0, 0, 0,  # 旅遊類\n",
    "         0, 0, 0, 0, 0, 0, 0, 0,  # 美食類\n",
    "         0, 0, 0, 0, 0, 0, 0, 0], # 文化類\n",
    "        \n",
    "        # 台北景點查詢的相關度 \"台北最受歡迎的觀光景點\"\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0,  # 科技類\n",
    "         2, 0, 0, 0, 1, 0, 0, 2,  # 旅遊類\n",
    "         0, 0, 0, 0, 0, 0, 0, 0,  # 美食類\n",
    "         0, 0, 0, 0, 0, 0, 0, 0], # 文化類\n",
    "        \n",
    "        # 自然風景查詢的相關度 \"台灣最美的自然風景區\"\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0,  # 科技類\n",
    "         0, 2, 2, 2, 0, 1, 1, 1,  # 旅遊類\n",
    "         0, 0, 0, 0, 0, 0, 0, 0,  # 美食類\n",
    "         0, 0, 0, 0, 0, 0, 0, 0], # 文化類\n",
    "        \n",
    "        # 台灣小吃查詢的相關度 \"台灣最有名的小吃有哪些\"\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0,  # 科技類\n",
    "         0, 0, 0, 0, 0, 0, 0, 0,  # 旅遊類\n",
    "         2, 2, 1, 1, 2, 1, 1, 2,  # 美食類\n",
    "         0, 0, 0, 0, 0, 0, 0, 0], # 文化類\n",
    "        \n",
    "        # 台南美食查詢的相關度 \"台南最具特色的美食\"\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0,  # 科技類\n",
    "         0, 0, 0, 0, 0, 0, 0, 0,  # 旅遊類\n",
    "         0, 0, 0, 2, 0, 2, 0, 0,  # 美食類\n",
    "         0, 0, 0, 0, 0, 0, 0, 0], # 文化類\n",
    "        \n",
    "        # 文物查詢的相關度 \"哪裡可以看到珍貴文物\"\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0,  # 科技類\n",
    "         0, 0, 0, 0, 0, 0, 0, 0,  # 旅遊類\n",
    "         0, 0, 0, 0, 0, 0, 0, 0,  # 美食類\n",
    "         2, 1, 1, 0, 1, 1, 1, 0], # 文化類\n",
    "        \n",
    "        # 古蹟查詢的相關度 \"台灣最具代表性的古蹟\"\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0,  # 科技類\n",
    "         0, 0, 0, 0, 0, 0, 0, 0,  # 旅遊類\n",
    "         0, 0, 0, 0, 0, 0, 0, 0,  # 美食類\n",
    "         1, 0, 2, 1, 0, 2, 0, 1]  # 文化類\n",
    "    ]\n",
    "    \n",
    "    return documents, queries, relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchMethodEvaluator:\n",
    "    \"\"\"相似度搜尋方法評估器\"\"\"\n",
    "    def __init__(self):\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        # 將 method_descriptions 移到類別內部\n",
    "        self.method_descriptions = {\n",
    "            \"FAISS\": \"基於向量近似最近鄰的快速檢索\",\n",
    "            \"HNSW\": \"基於分層導航小世界圖的向量檢索\",\n",
    "            \"BM25\": \"基於詞頻與文檔長度的檢索模型\",\n",
    "            \"Hybrid\": \"結合 BM25 與向量檢索的混合模型\"\n",
    "        }\n",
    "\n",
    "    def calculate_precision(self, results: List[int], relevance: List[int], k: int) -> float:\n",
    "        \"\"\"計算準確率，考慮不同相關度級別\n",
    "        \n",
    "        Args:\n",
    "            results: 搜尋結果的索引列表\n",
    "            relevance: 相關度列表（0-2分）\n",
    "            k: 評估的結果數量\n",
    "            \n",
    "        Returns:\n",
    "            float: 正規化的準確率（0-1）\n",
    "        \"\"\"\n",
    "        max_score = 2.0  # 最高相關度分數\n",
    "        actual_score = sum(relevance[i] for i in results[:k])\n",
    "        max_possible = k * max_score  # k個結果的最高可能分數\n",
    "        return actual_score / max_possible if max_possible > 0 else 0.0\n",
    "\n",
    "    def calculate_recall(self, results: List[int], relevance: List[int], k: int) -> float:\n",
    "        \"\"\"計算召回率，考慮不同相關度級別\n",
    "        \n",
    "        Args:\n",
    "            results: 搜尋結果的索引列表\n",
    "            relevance: 相關度列表（0-2分）\n",
    "            k: 評估的結果數量\n",
    "            \n",
    "        Returns:\n",
    "            float: 正規化的召回率（0-1）\n",
    "        \"\"\"\n",
    "        total_relevant_score = sum(relevance)  # 所有文檔的相關度總分\n",
    "        if total_relevant_score == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        retrieved_score = sum(relevance[i] for i in results[:k])\n",
    "        return retrieved_score / total_relevant_score\n",
    "\n",
    "    def calculate_mrr(self, results: List[int], relevance: List[int]) -> float:\n",
    "        \"\"\"計算 MRR，考慮不同相關度級別\n",
    "        \n",
    "        Args:\n",
    "            results: 搜尋結果的索引列表\n",
    "            relevance: 相關度列表（0-2分）\n",
    "            \n",
    "        Returns:\n",
    "            float: MRR 分數（0-1）\n",
    "        \"\"\"\n",
    "        # 找到第一個相關（分數>0）的文檔位置\n",
    "        for rank, doc_id in enumerate(results):\n",
    "            if relevance[doc_id] > 0:\n",
    "                # 根據相關度調整 MRR\n",
    "                rel_score = relevance[doc_id]\n",
    "                return (rel_score / 2.0) * (1.0 / (rank + 1))\n",
    "        return 0.0\n",
    "\n",
    "    def calculate_ndcg(self, results: List[int], relevance: List[int], k: int) -> float:\n",
    "        \"\"\"計算 NDCG，考慮不同相關度級別\n",
    "        \n",
    "        Args:\n",
    "            results: 搜尋結果的索引列表\n",
    "            relevance: 相關度列表（0-2分）\n",
    "            k: 評估的結果數量\n",
    "            \n",
    "        Returns:\n",
    "            float: NDCG 分數（0-1）\n",
    "        \"\"\"\n",
    "        # 計算 DCG\n",
    "        dcg = sum(\n",
    "            (relevance[i] / 2.0) / np.log2(rank + 2)  # 正規化相關度到 0-1\n",
    "            for rank, i in enumerate(results[:k])\n",
    "        )\n",
    "        \n",
    "        # 計算理想 DCG（將相關度排序）\n",
    "        ideal_results = sorted(range(len(relevance)), \n",
    "                             key=lambda i: relevance[i], \n",
    "                             reverse=True)\n",
    "        idcg = sum(\n",
    "            (relevance[i] / 2.0) / np.log2(rank + 2)\n",
    "            for rank, i in enumerate(ideal_results[:k])\n",
    "        )\n",
    "        \n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    def calculate_diversity(self, results: List[int], documents: List[str]) -> float:\n",
    "        \"\"\"計算搜尋結果的多樣性\n",
    "        \n",
    "        使用 Jaccard 距離計算文檔間的差異度\n",
    "        返回值範圍: 0-1，越大表示結果越多樣化\n",
    "        \"\"\"\n",
    "        if len(results) <= 1:\n",
    "            return 0.0\n",
    "        \n",
    "        def jaccard_distance(doc1: str, doc2: str) -> float:\n",
    "            \"\"\"計算兩個文檔的 Jaccard 距離\"\"\"\n",
    "            tokens1 = set(doc1.split())\n",
    "            tokens2 = set(doc2.split())\n",
    "            intersection = len(tokens1.intersection(tokens2))\n",
    "            union = len(tokens1.union(tokens2))\n",
    "            return 1 - (intersection / union if union > 0 else 0)\n",
    "        \n",
    "        # 計算所有結果對之間的距離\n",
    "        distances = []\n",
    "        for i in range(len(results)):\n",
    "            for j in range(i + 1, len(results)):\n",
    "                doc1 = documents[results[i]]\n",
    "                doc2 = documents[results[j]]\n",
    "                distances.append(jaccard_distance(doc1, doc2))\n",
    "        \n",
    "        # 返回平均距離\n",
    "        return np.mean(distances) if distances else 0.0\n",
    "\n",
    "    def evaluate_search_method(self, method_name: str, search_results: List[int], \n",
    "                             relevance: List[int], search_time: float, documents: List[str]) -> SearchMetrics:\n",
    "        \"\"\"評估搜尋方法的效果\"\"\"\n",
    "        k = 5  # 評估前 k 個結果\n",
    "        precision = self.calculate_precision(search_results, relevance, k)\n",
    "        recall = self.calculate_recall(search_results, relevance, k)\n",
    "        mrr = self.calculate_mrr(search_results, relevance)\n",
    "        ndcg = self.calculate_ndcg(search_results, relevance, k)\n",
    "        diversity = self.calculate_diversity(search_results, documents)\n",
    "        \n",
    "        return SearchMetrics(\n",
    "            name=method_name,\n",
    "            search_time=search_time,\n",
    "            precision_at_k=precision,\n",
    "            recall_at_k=recall,\n",
    "            mrr=mrr,\n",
    "            ndcg=ndcg,\n",
    "            diversity=diversity,\n",
    "            explanation=self.method_descriptions.get(method_name, \"未知搜尋方法\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorSearchEvaluator:\n",
    "    \"\"\"向量搜尋評估器\"\"\"\n",
    "    \n",
    "    def __init__(self, documents: List[str], embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.documents = documents\n",
    "        # self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        # 預計算所有文檔的向量表示\n",
    "        self.doc_vectors = self._compute_embeddings(documents)\n",
    "        self.vector_dim = len(self.doc_vectors[0])\n",
    "        \n",
    "    def _compute_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"計算文本的向量表示\"\"\"\n",
    "        vectors = self.embeddings.embed_documents(texts)\n",
    "        return normalize(np.array(vectors))  # 正規化向量\n",
    "    \n",
    "    def evaluate_faiss(self, query: str, k: int = 3) -> List[SearchResult]:\n",
    "        \"\"\"評估 FAISS 搜尋\"\"\"\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # 建立 FAISS 索引\n",
    "        index = faiss.IndexFlatIP(self.vector_dim)  # 使用內積相似度\n",
    "        index.add(self.doc_vectors)\n",
    "        \n",
    "        # 搜尋\n",
    "        query_vector = self._compute_embeddings([query])[0]\n",
    "        scores, indices = index.search(query_vector.reshape(1, -1), k)\n",
    "        \n",
    "        search_time = time.perf_counter() - start_time\n",
    "        \n",
    "        return [\n",
    "            SearchResult(\n",
    "                content=self.documents[idx],\n",
    "                score=score,\n",
    "                metadata={\"index\": idx},\n",
    "                method=\"FAISS\",\n",
    "                search_time=search_time\n",
    "            )\n",
    "            for score, idx in zip(scores[0], indices[0])\n",
    "        ]\n",
    "    \n",
    "    def evaluate_annoy(self, query: str, k: int = 3) -> List[SearchResult]:\n",
    "        \"\"\"評估 Annoy 搜尋\"\"\"\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # 建立 Annoy 索引\n",
    "        index = AnnoyIndex(self.vector_dim, 'angular')  # 使用角度距離\n",
    "        for i, vector in enumerate(self.doc_vectors):\n",
    "            index.add_item(i, vector)\n",
    "        index.build(10)  # 建立 10 棵樹\n",
    "        \n",
    "        # 搜尋\n",
    "        query_vector = self._compute_embeddings([query])[0]\n",
    "        indices, distances = index.get_nns_by_vector(\n",
    "            query_vector, k, \n",
    "            include_distances=True\n",
    "        )\n",
    "        \n",
    "        search_time = time.perf_counter() - start_time\n",
    "        \n",
    "        # 將距離轉換為相似度分數\n",
    "        scores = 1 - np.array(distances) / 2  # 角度距離轉換為相似度\n",
    "        \n",
    "        return [\n",
    "            SearchResult(\n",
    "                content=self.documents[idx],\n",
    "                score=score,\n",
    "                metadata={\"index\": idx},\n",
    "                method=\"Annoy\",\n",
    "                search_time=search_time\n",
    "            )\n",
    "            for score, idx in zip(scores, indices)\n",
    "        ]\n",
    "    \n",
    "    def evaluate_hybrid(self, query: str, k: int = 3) -> List[SearchResult]:\n",
    "        \"\"\"評估混合搜尋 (BM25 + 向量)\"\"\"\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # BM25 搜尋\n",
    "        bm25 = BM25Okapi([doc.split() for doc in self.documents])\n",
    "        bm25_scores = bm25.get_scores(query.split())\n",
    "        \n",
    "        # 向量搜尋\n",
    "        query_vector = self._compute_embeddings([query])[0]\n",
    "        vector_scores = np.dot(self.doc_vectors, query_vector)\n",
    "        \n",
    "        # 結合分數 (簡單加權平均)\n",
    "        combined_scores = 0.3 * normalize(bm25_scores.reshape(1, -1))[0] + \\\n",
    "                         0.7 * vector_scores\n",
    "        \n",
    "        # 取得前 k 個結果\n",
    "        top_k_idx = np.argsort(combined_scores)[-k:][::-1]\n",
    "        top_k_scores = combined_scores[top_k_idx]\n",
    "        \n",
    "        search_time = time.perf_counter() - start_time\n",
    "        \n",
    "        return [\n",
    "            SearchResult(\n",
    "                content=self.documents[idx],\n",
    "                score=score,\n",
    "                metadata={\n",
    "                    \"index\": idx,\n",
    "                    \"bm25_score\": bm25_scores[idx],\n",
    "                    \"vector_score\": vector_scores[idx]\n",
    "                },\n",
    "                method=\"Hybrid\",\n",
    "                search_time=search_time\n",
    "            )\n",
    "            for score, idx in zip(top_k_scores, top_k_idx)\n",
    "        ]\n",
    "\n",
    "    def evaluate_hnsw(self, query: str, k: int = 3) -> List[SearchResult]:\n",
    "        \"\"\"評估 HNSW 搜尋\"\"\"\n",
    "        start_time = time.perf_counter()\n",
    "        try:\n",
    "            # 初始化 HNSW 索引\n",
    "            dim = self.vector_dim\n",
    "            num_elements = len(self.documents)\n",
    "            \n",
    "            # 調整 HNSW 參數\n",
    "            index = hnswlib.Index(space='cosine', dim=dim)\n",
    "            index.init_index(\n",
    "                max_elements=num_elements,\n",
    "                ef_construction=200,  # 增加建構時的精確度\n",
    "                M=16,                # 每個節點的最大連接數\n",
    "            )\n",
    "            \n",
    "            # 添加文檔向量\n",
    "            index.add_items(self.doc_vectors)\n",
    "            \n",
    "            # 設定搜尋參數\n",
    "            index.set_ef(num_elements)  # 增加搜尋時的精確度\n",
    "            \n",
    "            # 搜尋\n",
    "            query_vector = self._compute_embeddings([query])[0]\n",
    "            scores, indices = index.knn_query(query_vector.reshape(1, -1), k=k)\n",
    "            \n",
    "            search_time = time.perf_counter() - start_time\n",
    "            \n",
    "            # 正規化分數到 0-1 範圍\n",
    "            normalized_scores = 1 - scores[0]  # 因為使用 cosine 距離，轉換為相似度\n",
    "            \n",
    "            # 將索引轉換為整數類型\n",
    "            indices = indices[0].astype(int)\n",
    "            \n",
    "            return [\n",
    "                SearchResult(\n",
    "                    content=self.documents[idx],\n",
    "                    score=score,\n",
    "                    metadata={\"index\": int(idx)},  # 確保索引是整數\n",
    "                    method=\"HNSW\",\n",
    "                    search_time=search_time\n",
    "                )\n",
    "                for score, idx in zip(normalized_scores, indices)\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"HNSW 搜尋失敗: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "    \n",
    "    def evaluate_bm25(self, query: str, k: int = 3) -> List[SearchResult]:\n",
    "        \"\"\"評估純 BM25 搜尋\"\"\"\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # 建立 BM25 模型\n",
    "        tokenized_docs = [doc.split() for doc in self.documents]\n",
    "        bm25 = BM25Okapi(tokenized_docs)\n",
    "        \n",
    "        # 搜尋\n",
    "        tokenized_query = query.split()\n",
    "        scores = bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # 取得前 k 個結果\n",
    "        top_k_idx = np.argsort(-scores)[:k]\n",
    "        top_k_scores = scores[top_k_idx]\n",
    "        \n",
    "        search_time = time.perf_counter() - start_time\n",
    "        \n",
    "        return [\n",
    "            SearchResult(\n",
    "                content=self.documents[idx],\n",
    "                score=score,\n",
    "                metadata={\"index\": idx},\n",
    "                method=\"BM25\",\n",
    "                search_time=search_time\n",
    "            )\n",
    "            for score, idx in zip(top_k_scores, top_k_idx)\n",
    "        ]\n",
    "\n",
    "def compare_search_methods(documents: List[str], queries: List[str], \n",
    "                         relevance: List[List[int]]) -> pd.DataFrame:\n",
    "    \"\"\"比較不同搜尋方法的效果\"\"\"\n",
    "    logger.info(\"開始評估搜尋方法\")\n",
    "    evaluator = SearchMethodEvaluator()\n",
    "    vector_evaluator = VectorSearchEvaluator(documents)\n",
    "    \n",
    "    # 定義要評估的方法\n",
    "    search_methods = {\n",
    "        \"FAISS\": vector_evaluator.evaluate_faiss,\n",
    "        \"HNSW\": vector_evaluator.evaluate_hnsw,\n",
    "        \"BM25\": vector_evaluator.evaluate_bm25,\n",
    "        \"Hybrid\": vector_evaluator.evaluate_hybrid\n",
    "    }\n",
    "    \n",
    "    # 初始化結果列表\n",
    "    evaluation_results = []\n",
    "    \n",
    "    # 對每個查詢評估所有方法\n",
    "    for query_idx, (query, rel) in enumerate(zip(queries, relevance), 1):\n",
    "        logger.info(f\"\\n評估查詢 {query_idx}/{len(queries)}: {query}\")\n",
    "        \n",
    "        for method_name, search_func in search_methods.items():\n",
    "            logger.info(f\"使用 {method_name} 方法搜尋\")\n",
    "            \n",
    "            # 執行搜尋\n",
    "            try:\n",
    "                results = search_func(query)\n",
    "                result_indices = [r.metadata[\"index\"] for r in results]\n",
    "                \n",
    "                # 計算評估指標\n",
    "                metrics = evaluator.evaluate_search_method(\n",
    "                    method_name=method_name,\n",
    "                    search_results=result_indices,\n",
    "                    relevance=rel,\n",
    "                    search_time=results[0].search_time,\n",
    "                    documents=documents\n",
    "                )\n",
    "                \n",
    "                # logger.info(\n",
    "                #     f\"{method_name} 搜尋結果:\\n\"\n",
    "                #     f\"- 查詢時間: {metrics.search_time*1000:.2f}ms\\n\"\n",
    "                #     f\"- 準確率@K: {metrics.precision_at_k:.3f}\\n\"\n",
    "                #     f\"- 召回率@K: {metrics.recall_at_k:.3f}\\n\"\n",
    "                #     f\"- MRR: {metrics.mrr:.3f}\\n\"\n",
    "                #     f\"- NDCG: {metrics.ndcg:.3f}\\n\"\n",
    "                #     f\"- 多樣性: {metrics.diversity:.3f}\"\n",
    "                # )\n",
    "                \n",
    "                # 添加到結果列表\n",
    "                evaluation_results.append({\n",
    "                    \"Method\": metrics.name,  # 改用英文欄位名\n",
    "                    \"QueryTime\": round(metrics.search_time * 1000, 2),\n",
    "                    \"Precision\": round(metrics.precision_at_k, 3),\n",
    "                    \"Recall\": round(metrics.recall_at_k, 3),\n",
    "                    \"MRR\": round(metrics.mrr, 3),\n",
    "                    \"NDCG\": round(metrics.ndcg, 3),\n",
    "                    \"Diversity\": round(metrics.diversity, 3),\n",
    "                    \"Description\": metrics.explanation\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"{method_name} 評估失敗: {str(e)}\", exc_info=True)\n",
    "                continue\n",
    "    \n",
    "    # 轉換為 DataFrame 並計算平均值\n",
    "    results_df = pd.DataFrame(evaluation_results)\n",
    "    if not results_df.empty:\n",
    "        average_metrics = results_df.groupby(\"Method\").agg({\n",
    "            \"QueryTime\": \"mean\",\n",
    "            \"Precision\": \"mean\",\n",
    "            \"Recall\": \"mean\",\n",
    "            \"MRR\": \"mean\",\n",
    "            \"NDCG\": \"mean\",\n",
    "            \"Diversity\": \"mean\"\n",
    "        }).round(3)\n",
    "        \n",
    "        logger.info(\"\\n評估完成，平均指標:\")\n",
    "        # logger.info(\"\\n\" + str(average_metrics))\n",
    "        \n",
    "        return average_metrics\n",
    "    else:\n",
    "        logger.warning(\"沒有成功的評估結果\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 14:04:24,389 - INFO - 開始執行搜尋方法評估\n",
      "2025-02-16 14:04:24,390 - INFO - 載入測試數據: 32 文檔, 8 查詢\n",
      "2025-02-16 14:04:24,390 - INFO - 開始評估搜尋方法\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 搜尋方法評估結果 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 14:04:25,842 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:26,572 - INFO - \n",
      "評估查詢 1/8: 台灣最先進的半導體製程技術\n",
      "2025-02-16 14:04:26,572 - INFO - 使用 FAISS 方法搜尋\n",
      "2025-02-16 14:04:26,913 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:26,919 - INFO - 使用 HNSW 方法搜尋\n",
      "2025-02-16 14:04:27,252 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:27,255 - INFO - 使用 BM25 方法搜尋\n",
      "2025-02-16 14:04:27,256 - INFO - 使用 Hybrid 方法搜尋\n",
      "2025-02-16 14:04:27,567 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:27,569 - INFO - \n",
      "評估查詢 2/8: 最新的顯示器技術發展\n",
      "2025-02-16 14:04:27,569 - INFO - 使用 FAISS 方法搜尋\n",
      "2025-02-16 14:04:28,031 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:28,033 - INFO - 使用 HNSW 方法搜尋\n",
      "2025-02-16 14:04:28,291 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:28,293 - INFO - 使用 BM25 方法搜尋\n",
      "2025-02-16 14:04:28,293 - INFO - 使用 Hybrid 方法搜尋\n",
      "2025-02-16 14:04:28,687 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:28,689 - INFO - \n",
      "評估查詢 3/8: 台北最受歡迎的觀光景點\n",
      "2025-02-16 14:04:28,689 - INFO - 使用 FAISS 方法搜尋\n",
      "2025-02-16 14:04:29,149 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:29,151 - INFO - 使用 HNSW 方法搜尋\n",
      "2025-02-16 14:04:29,535 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:29,537 - INFO - 使用 BM25 方法搜尋\n",
      "2025-02-16 14:04:29,537 - INFO - 使用 Hybrid 方法搜尋\n",
      "2025-02-16 14:04:30,132 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:30,133 - INFO - \n",
      "評估查詢 4/8: 台灣最美的自然風景區\n",
      "2025-02-16 14:04:30,134 - INFO - 使用 FAISS 方法搜尋\n",
      "2025-02-16 14:04:30,487 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:30,488 - INFO - 使用 HNSW 方法搜尋\n",
      "2025-02-16 14:04:30,774 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:30,777 - INFO - 使用 BM25 方法搜尋\n",
      "2025-02-16 14:04:30,777 - INFO - 使用 Hybrid 方法搜尋\n",
      "2025-02-16 14:04:31,081 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:31,082 - INFO - \n",
      "評估查詢 5/8: 台灣最有名的小吃有哪些\n",
      "2025-02-16 14:04:31,083 - INFO - 使用 FAISS 方法搜尋\n",
      "2025-02-16 14:04:31,350 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:31,352 - INFO - 使用 HNSW 方法搜尋\n",
      "2025-02-16 14:04:31,580 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:31,582 - INFO - 使用 BM25 方法搜尋\n",
      "2025-02-16 14:04:31,583 - INFO - 使用 Hybrid 方法搜尋\n",
      "2025-02-16 14:04:31,823 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:31,828 - INFO - \n",
      "評估查詢 6/8: 台南最具特色的美食\n",
      "2025-02-16 14:04:31,828 - INFO - 使用 FAISS 方法搜尋\n",
      "2025-02-16 14:04:32,098 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:32,103 - INFO - 使用 HNSW 方法搜尋\n",
      "2025-02-16 14:04:32,489 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:32,490 - INFO - 使用 BM25 方法搜尋\n",
      "2025-02-16 14:04:32,491 - INFO - 使用 Hybrid 方法搜尋\n",
      "2025-02-16 14:04:32,728 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:32,730 - INFO - \n",
      "評估查詢 7/8: 哪裡可以看到珍貴文物\n",
      "2025-02-16 14:04:32,730 - INFO - 使用 FAISS 方法搜尋\n",
      "2025-02-16 14:04:33,017 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:33,020 - INFO - 使用 HNSW 方法搜尋\n",
      "2025-02-16 14:04:33,397 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:33,399 - INFO - 使用 BM25 方法搜尋\n",
      "2025-02-16 14:04:33,400 - INFO - 使用 Hybrid 方法搜尋\n",
      "2025-02-16 14:04:33,689 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:33,691 - INFO - \n",
      "評估查詢 8/8: 台灣最具代表性的古蹟\n",
      "2025-02-16 14:04:33,691 - INFO - 使用 FAISS 方法搜尋\n",
      "2025-02-16 14:04:33,971 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:33,975 - INFO - 使用 HNSW 方法搜尋\n",
      "2025-02-16 14:04:34,256 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:34,259 - INFO - 使用 BM25 方法搜尋\n",
      "2025-02-16 14:04:34,260 - INFO - 使用 Hybrid 方法搜尋\n",
      "2025-02-16 14:04:34,507 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-16 14:04:34,518 - INFO - \n",
      "評估完成，平均指標:\n",
      "2025-02-16 14:04:34,521 - INFO - 評估完成\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "平均評估指標:\n",
      "        QueryTime  Precision  Recall    MRR   NDCG  Diversity\n",
      "Method                                                       \n",
      "BM25        0.094      0.075   0.113  0.219  0.130        1.0\n",
      "FAISS     342.403      0.362   0.608  0.781  0.676        1.0\n",
      "HNSW      317.851      0.075   0.125  0.125  0.127        0.0\n",
      "Hybrid    329.171      0.362   0.608  0.781  0.676        1.0\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"主程式：執行搜尋方法評估\"\"\"\n",
    "    logger.info(\"開始執行搜尋方法評估\")\n",
    "    try:\n",
    "        documents, queries, relevance = prepare_test_data()\n",
    "        logger.info(f\"載入測試數據: {len(documents)} 文檔, {len(queries)} 查詢\")\n",
    "        \n",
    "        # 評估所有方法\n",
    "        print(\"\\n=== 搜尋方法評估結果 ===\")\n",
    "        average_metrics = compare_search_methods(documents, queries, relevance)\n",
    "        print(\"\\n平均評估指標:\")\n",
    "        print(average_metrics)\n",
    "        \n",
    "        logger.info(\"評估完成\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"執行過程發生錯誤: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
