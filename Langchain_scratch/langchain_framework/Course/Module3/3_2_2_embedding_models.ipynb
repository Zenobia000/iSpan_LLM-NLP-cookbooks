{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain 0.3+ Embedding 模型比較\n",
    "比較不同 Embedding 模型的性能與特性\n",
    "\n",
    "需求套件:\n",
    "- langchain>=0.3.0\n",
    "- langchain-community>=0.0.1\n",
    "- sentence-transformers>=2.2.2\n",
    "- openai>=1.1.0\n",
    "- cohere>=4.37\n",
    "- pandas>=2.0.0\n",
    "- numpy>=1.24.0\n",
    "- python-dotenv>=0.19.0\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方法特性分析表\n",
    "\n",
    "| 特性           | OpenAI Embedding | CohereAI Embedding | Jina Embeddings | BGE (BAAI) |\n",
    "|---------------|:----------------:|:------------------:|:---------------:|:----------:|\n",
    "| **語義準確性** |        ○         |         ○          |        △        |     ○      |\n",
    "| **計算成本**   |        △         |         △          |        ○        |     ○      |\n",
    "| **領域適應性** |        ○         |         ○          |        △        |     ○      |\n",
    "| **可擴展性**   |        ○         |         ○          |        ○        |     ○      |\n",
    "| **訓練效率**   |        ○         |         ○          |        △        |     ○      |\n",
    "| **查詢效率**   |        ○         |         ○          |        △        |     ○      |\n",
    "| **多模態支持** |        ○         |         ○          |        ×        |     ○      |\n",
    "\n",
    "# 問題特性分析表\n",
    "\n",
    "| 應用領域        | OpenAI Embedding | CohereAI Embedding | Jina Embeddings | BGE (BAAI) |\n",
    "|-----------------|:----------------:|:------------------:|:---------------:|:----------:|\n",
    "| **資訊檢索（IR）** |        ○         |         ○          |        △        |     ○      |\n",
    "| **推薦系統（RS）** |        ○         |         ○          |        △        |     ○      |\n",
    "| **問答系統（QA）** |        ○         |         ○          |        △        |     ○      |\n",
    "| **文本分類（TC）** |        ○         |         ○          |        ○        |     ○      |\n",
    "| **知識圖譜（KG）** |        ○         |         ○          |        △        |     ○      |\n",
    "| **多模態處理（MM）** |        ○         |         ○          |        ×        |     ○      |\n",
    "\n",
    "# 方法特性 vs. 問題特性 矩陣比較表\n",
    "\n",
    "| 方法特性 / 應用領域 | IR（資訊檢索） | RS（推薦系統） | QA（問答系統） | TC（文本分類） | KG（知識圖譜） | MM（多模態處理） |\n",
    "|--------------------|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n",
    "| **語義準確性**     |       ○       |       ○       |       ○       |       ○       |       ○       |       △       |\n",
    "| **計算成本**       |       △       |       ○       |       △       |       ○       |       △       |       ○       |\n",
    "| **領域適應性**     |       ○       |       ○       |       ○       |       ○       |       ○       |       △       |\n",
    "| **可擴展性**       |       ○       |       ○       |       ○       |       ○       |       △       |       ○       |\n",
    "| **訓練效率**       |       ○       |       ○       |       ○       |       ○       |       ○       |       △       |\n",
    "| **查詢效率**       |       ○       |       ○       |       ○       |       ○       |       △       |       △       |\n",
    "| **多模態支持**     |       ×       |       △       |       △       |       ×       |       △       |       ○       |\n",
    "\n",
    "# 符號意義\n",
    "\n",
    "- **○**：表現優異或高度相關\n",
    "- **△**：表現一般或部分適用\n",
    "- **×**：表現較差或不適用\n",
    "\n",
    "## 指標量化\n",
    "\n",
    "| 指標名稱   | 量化評測方式                          | 可參考基準           |\n",
    "|------------|----------------------------------|------------------|\n",
    "| 語義準確性  | 平均排名（MRR）/NDCG@K           | BEIR、MTEB       |\n",
    "| 計算成本  | FLOPs（浮點運算量）/ 記憶體使用率 | Papers With Code |\n",
    "| 領域適應性  | MMLU領域分數（如醫療/法律/科技）  | MMLU             |\n",
    "| 可擴展性  | 檢索時間 vs. 數據規模（log-scale） | RAG pipeline     |\n",
    "| 訓練效率  | 訓練步數 vs. 目標loss 收斂時間     | LLAMA / GPT 研究 |\n",
    "| 查詢效率  | 查詢平均延遲（毫秒）               | BEIR、MTEB       |\n",
    "| 多模態支持  | 文字-圖片嵌入相似度（CLIP-based）  | CLIP/MultiBench  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法特性分析表\n",
    "特性\t\n",
    "- 語義準確性:\t嵌入模型對於文本語義的理解能力，影響資訊檢索、問答系統等應用。\n",
    "- 計算成本:\t訓練與推理過程中所需的計算資源，關係到模型的運行效率與部署成本。\n",
    "- 領域適應性:\t模型在特定領域（如醫學、法律、金融等）中的表現，影響專業應用的效果。\n",
    "- 可擴展性:\t模型是否能夠有效應對大規模數據處理，例如高併發檢索應用。\n",
    "- 訓練效率:\t模型收斂速度與所需的訓練數據量，影響研發與迭代週期。\n",
    "- 查詢效率:\t模型在檢索與推理時的響應速度，影響即時應用的體驗。\n",
    "- 多模態支持:\t模型是否支援圖像、語音與文本的整合處理，影響多媒體應用場景。\n",
    "\n",
    "問題特性分析表\n",
    "應用領域\t\n",
    "- 資訊檢索（IR）:\t評估模型在搜索系統、企業文檔檢索中的表現，如Google Search、企業知識庫。\n",
    "- 推薦系統（RS）:\t測試嵌入模型在電商、影音平台的個性化推薦效果，如YouTube、Netflix推薦算法。\n",
    "- 問答系統（QA）:\t量測模型在即時問答、技術支援上的準確性，如ChatGPT、客服機器人。\n",
    "- 文本分類（TC）:\t檢驗模型在垃圾郵件檢測、情感分析、主題分類上的表現，如新聞分類、社群監測。\n",
    "- 知識圖譜（KG）:\t測試模型在構建知識圖譜、關係推理、語義搜索上的能力，如維基數據、企業知識管理。\n",
    "- 多模態處理（MM）:\t評估模型在跨模態數據（文本+圖像+音頻）處理的表現，如OCR+文本分析、影像字幕生成。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import logging\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# 設定日誌\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 載入環境變數\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "jina_api_key = os.getenv(\"JINA_API_KEY\")\n",
    "bge_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LLM 嵌入模型 + FAISS 向量資料庫 Benchmark 測試 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 23:36:00,576 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-13 23:36:01,421 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-13 23:36:01,422 - INFO - 測試 FAISS (模型: openai) ...\n",
      "2025-02-13 23:36:02,504 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-13 23:36:03,591 - INFO - 測試 FAISS (模型: cohere) ...\n",
      "2025-02-13 23:36:05,867 - INFO - 測試 FAISS (模型: jina) ...\n",
      "2025-02-13 23:36:06,661 - INFO - Use pytorch device_name: cuda\n",
      "2025-02-13 23:36:06,662 - INFO - Load pretrained SentenceTransformer: BAAI/bge-base-en\n",
      "2025-02-13 23:36:09,251 - INFO - 測試 FAISS (模型: bge) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Embedding Model  Insert Time (s)  Query Time (s)  Memory Usage (MB)  \\\n",
      "0          openai           1.3542           0.000            1277.23   \n",
      "1          cohere           0.2461           0.000            1277.87   \n",
      "2            jina           0.7907           0.001            1277.88   \n",
      "3             bge           0.0085           0.000            1287.05   \n",
      "\n",
      "   Recall (%)  \n",
      "0      100.00  \n",
      "1      100.00  \n",
      "2       66.67  \n",
      "3       66.67  \n"
     ]
    }
   ],
   "source": [
    "# 設定日誌\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EmbeddingBenchmark:\n",
    "    \"\"\"LLM 嵌入模型性能測試（使用 FAISS）\"\"\"\n",
    "\n",
    "    def __init__(self, num_articles=50, embedding_dim=384, model=\"openai\"):\n",
    "        \"\"\"初始化測試數據\"\"\"\n",
    "        self.num_articles = num_articles\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.model_name = model\n",
    "        self.embeddings = self.load_embedding_model(model)\n",
    "\n",
    "        # 測試文本與查詢向量\n",
    "        self.test_texts = [\n",
    "            \"Artificial Intelligence\", \"Machine Learning\", \"Deep Learning\",\n",
    "            \"Natural Language Processing\", \"Neural Networks\", \"Quantum Computing\",\n",
    "            \"Blockchain\", \"Cloud Computing\", \"Cybersecurity\", \"Data Science\"\n",
    "        ]\n",
    "        self.test_vectors = self.texts_to_embeddings(self.test_texts)\n",
    "        self.query_vector = self.texts_to_embeddings([\"Artificial Intelligence\"])[0]  # 測試用查詢\n",
    "\n",
    "    def load_embedding_model(self, model):\n",
    "        \"\"\"載入不同的嵌入模型\"\"\"\n",
    "        if model == \"openai\":\n",
    "            return OpenAIEmbeddings()\n",
    "        elif model == \"cohere\":\n",
    "            from langchain_cohere import CohereEmbeddings  # 確保使用最新的 CohereEmbeddings\n",
    "            return CohereEmbeddings(cohere_api_key=cohere_api_key, model=\"embed-english-light-v3.0\")\n",
    "        elif model == \"jina\":\n",
    "            from langchain_community.embeddings import JinaEmbeddings\n",
    "            return JinaEmbeddings(jina_api_key=jina_api_key)\n",
    "        elif model == \"bge\":\n",
    "            from langchain_huggingface import HuggingFaceEmbeddings\n",
    "            return HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en\")\n",
    "        else:\n",
    "            raise ValueError(f\"不支援的嵌入模型: {model}\")\n",
    "\n",
    "    def texts_to_embeddings(self, texts):\n",
    "        \"\"\"將文本轉換為向量嵌入\"\"\"\n",
    "        return self.embeddings.embed_documents(texts)\n",
    "\n",
    "    def memory_usage(self):\n",
    "        \"\"\"取得記憶體使用量 (MB)\"\"\"\n",
    "        return psutil.Process().memory_info().rss / 1024 / 1024\n",
    "\n",
    "    def calculate_recall(self, results):\n",
    "        \"\"\"計算查詢 Recall\"\"\"\n",
    "        retrieved_texts = [doc.page_content for doc in results]\n",
    "        correct_answers = {\"Artificial Intelligence\", \"Machine Learning\", \"Deep Learning\"}\n",
    "        matched = sum(1 for ans in correct_answers if any(ans in text for text in retrieved_texts))\n",
    "        \n",
    "        recall = min((matched / len(correct_answers)) * 100, 100)\n",
    "        return recall\n",
    "\n",
    "    def evaluate_faiss(self):\n",
    "        \"\"\"測試 FAISS 向量資料庫\"\"\"\n",
    "        logger.info(f\"測試 FAISS (模型: {self.model_name}) ...\")\n",
    "\n",
    "        # 建立索引\n",
    "        start_time = time.time()\n",
    "        vectorstore = FAISS.from_texts(self.test_texts, embedding=self.embeddings)\n",
    "        insert_time = time.time() - start_time\n",
    "\n",
    "        # 進行查詢\n",
    "        start_time = time.time()\n",
    "        results = vectorstore.similarity_search_by_vector(self.query_vector, k=3)\n",
    "        query_time = time.time() - start_time\n",
    "\n",
    "        recall = self.calculate_recall(results)\n",
    "\n",
    "        return {\n",
    "            \"Embedding Model\": self.model_name,\n",
    "            \"Insert Time (s)\": round(insert_time, 4),\n",
    "            \"Query Time (s)\": round(query_time, 4),\n",
    "            \"Memory Usage (MB)\": round(self.memory_usage(), 2),\n",
    "            \"Recall (%)\": round(recall, 2)\n",
    "        }\n",
    "\n",
    "    def run_benchmark(self):\n",
    "        \"\"\"執行測試\"\"\"\n",
    "        result = self.evaluate_faiss()\n",
    "        df = pd.DataFrame([result])\n",
    "        # print(df)\n",
    "        return df\n",
    "\n",
    "def main():\n",
    "    \"\"\"主程式\"\"\"\n",
    "    print(\"\\n=== LLM 嵌入模型 + FAISS 向量資料庫 Benchmark 測試 ===\\n\")\n",
    "    models = [\"openai\", \"cohere\", \"jina\", \"bge\"]\n",
    "    # models = [\"bge\"]\n",
    "    all_results = []\n",
    "\n",
    "    for model in models:\n",
    "        benchmark = EmbeddingBenchmark(num_articles=50, embedding_dim=384, model=model)\n",
    "        results = benchmark.run_benchmark()\n",
    "        all_results.append(results)\n",
    "\n",
    "    final_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(final_df)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_result = main()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LLM 嵌入模型 + FAISS 向量資料庫 Benchmark 測試 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 00:58:32,931 - INFO - 測試 FAISS (模型: openai, 數據集: scifact) ...\n",
      "2025-02-14 00:58:33,318 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-14 00:58:34,227 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-14 00:58:34,509 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-14 00:58:35,071 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-14 00:58:35,414 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-14 00:58:35,670 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-14 00:58:36,084 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-14 00:58:36,340 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-14 00:58:37,018 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-14 00:58:37,607 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-14 00:58:38,417 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "測試結果:\n",
      "  Embedding Model  Dataset  Num Samples  Insert Time (s)  Query Time (s)  \\\n",
      "0          openai  scifact           10            0.751          0.4737   \n",
      "\n",
      "   Memory Usage (MB)     MRR  NDCG@10  \n",
      "0             692.14  0.3315   0.2931  \n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import CohereEmbeddings, JinaEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# 設定日誌\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EmbeddingBenchmark:\n",
    "    \"\"\"LLM 嵌入模型性能測試（使用 FAISS）\"\"\"\n",
    "\n",
    "    def __init__(self, num_samples=1000, embedding_dim=384, model=\"openai\", dataset=\"scifact\"):\n",
    "        \"\"\"初始化測試數據\"\"\"\n",
    "        self.num_samples = num_samples\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.model_name = model\n",
    "        self.dataset_name = dataset\n",
    "        self.embeddings = self.load_embedding_model(model)\n",
    "\n",
    "        # 載入測試數據\n",
    "        self.query_texts, self.index_texts, self.ground_truth = self.load_dataset_samples(dataset, num_samples)\n",
    "\n",
    "    def load_embedding_model(self, model):\n",
    "        \"\"\"載入不同的嵌入模型\"\"\"\n",
    "        if model == \"openai\":\n",
    "            return OpenAIEmbeddings()\n",
    "        elif model == \"cohere\":\n",
    "            from langchain_cohere import CohereEmbeddings\n",
    "            return CohereEmbeddings(model=\"embed-english-light-v3.0\")\n",
    "        elif model == \"jina\":\n",
    "            return JinaEmbeddings()\n",
    "        elif model == \"bge\":\n",
    "            return HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en\")\n",
    "        else:\n",
    "            raise ValueError(f\"不支援的嵌入模型: {model}\")\n",
    "\n",
    "    def load_dataset_samples(self, dataset_name, num_samples):\n",
    "        \"\"\"載入標準數據集樣本，確保查詢 (claims) 和索引 (abstracts) 分開\"\"\"\n",
    "        try:\n",
    "            # 先載入 claims 作為查詢集\n",
    "            claims_dataset = load_dataset(dataset_name, name=\"claims\", split=\"train\", trust_remote_code=True)\n",
    "            queries = claims_dataset[\"claim\"][:num_samples]\n",
    "            \n",
    "            # 再載入 corpus 作為索引庫\n",
    "            corpus_dataset = load_dataset(dataset_name, name=\"corpus\", split=\"train\", trust_remote_code=True)\n",
    "            index_texts = [str(a) for a in corpus_dataset[\"abstract\"][:num_samples]]  \n",
    "\n",
    "            # Ground truth 設定：claims 應該對應 corpus 內的某些 text\n",
    "            ground_truth = index_texts[:num_samples]  # 這裡應根據對應關係調整\n",
    "\n",
    "            return queries, index_texts, ground_truth\n",
    "        except Exception as e:\n",
    "            logger.error(f\"載入數據集 {dataset_name} 時發生錯誤: {str(e)}\")\n",
    "            return [\"Query\" for _ in range(num_samples)], [\"Index\" for _ in range(num_samples)], [\"Ground Truth\" for _ in range(num_samples)]\n",
    "\n",
    "    def memory_usage(self):\n",
    "        \"\"\"取得記憶體使用量 (MB)\"\"\"\n",
    "        return psutil.Process().memory_info().rss / 1024 / 1024\n",
    "\n",
    "    def calculate_mrr(self, results, ground_truth):\n",
    "        \"\"\"計算 MRR（Mean Reciprocal Rank）\"\"\"\n",
    "        ranks = []\n",
    "        for idx, docs in enumerate(results):\n",
    "            for rank, doc in enumerate(docs, start=1):\n",
    "                if doc.page_content == ground_truth[idx]:\n",
    "                    ranks.append(1 / rank)\n",
    "                    break\n",
    "        return sum(ranks) / len(ranks) if ranks else 0\n",
    "\n",
    "    def calculate_ndcg(self, results, ground_truth, k=10):\n",
    "        \"\"\"計算 NDCG@K\"\"\"\n",
    "        y_true = [[1 if doc.page_content == ground_truth[idx] else 0 for doc in docs] for idx, docs in enumerate(results)]\n",
    "        y_score = [[1 / (i+1) for i in range(len(docs))] for docs in results]\n",
    "        return ndcg_score(y_true, y_score, k=k)\n",
    "\n",
    "    def evaluate_faiss(self):\n",
    "        \"\"\"測試 FAISS 向量資料庫\"\"\"\n",
    "        logger.info(f\"測試 FAISS (模型: {self.model_name}, 數據集: {self.dataset_name}) ...\")\n",
    "\n",
    "        # 建立索引\n",
    "        start_time = time.time()\n",
    "        vectorstore = FAISS.from_texts(self.index_texts, embedding=self.embeddings)\n",
    "        insert_time = time.time() - start_time\n",
    "\n",
    "        # 進行查詢\n",
    "        results = []\n",
    "        start_time = time.time()\n",
    "        for query in self.query_texts:\n",
    "            query_vector = self.embeddings.embed_query(query)\n",
    "            top_k_results = vectorstore.similarity_search_by_vector(query_vector, k=10)\n",
    "            results.append(top_k_results)\n",
    "        query_time = (time.time() - start_time) / len(self.query_texts)\n",
    "\n",
    "        # 計算 MRR & NDCG\n",
    "        mrr_score = self.calculate_mrr(results, self.ground_truth)\n",
    "        ndcg_score_k = self.calculate_ndcg(results, self.ground_truth, k=5)\n",
    "\n",
    "        return {\n",
    "            \"Embedding Model\": self.model_name,\n",
    "            \"Dataset\": self.dataset_name,\n",
    "            \"Num Samples\": self.num_samples,\n",
    "            \"Insert Time (s)\": round(insert_time, 4),\n",
    "            \"Query Time (s)\": round(query_time, 4),\n",
    "            \"Memory Usage (MB)\": round(self.memory_usage(), 2),\n",
    "            \"MRR\": round(mrr_score, 4),\n",
    "            \"NDCG@10\": round(ndcg_score_k, 4),\n",
    "        }\n",
    "\n",
    "    def run_benchmark(self):\n",
    "        \"\"\"執行測試\"\"\"\n",
    "        result = self.evaluate_faiss()\n",
    "        df = pd.DataFrame([result])\n",
    "        return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"主程式\"\"\"\n",
    "    print(\"\\n=== LLM 嵌入模型 + FAISS 向量資料庫 Benchmark 測試 ===\\n\")\n",
    "    models = [\"openai\"]  # 先測試一個模型\n",
    "    datasets = [\"scifact\"]  # 只測試一個數據集\n",
    "    sample_sizes = [10]  # 測試 1000 筆數據\n",
    "\n",
    "    all_results = []\n",
    "    for model in models:\n",
    "        for dataset in datasets:\n",
    "            for num_samples in sample_sizes:\n",
    "                try:\n",
    "                    benchmark = EmbeddingBenchmark(\n",
    "                        num_samples=num_samples,\n",
    "                        embedding_dim=384,\n",
    "                        model=model,\n",
    "                        dataset=dataset\n",
    "                    )\n",
    "                    results = benchmark.run_benchmark()\n",
    "                    all_results.append(results)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"執行 benchmark 時發生錯誤 (model={model}, dataset={dataset}): {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "    if not all_results:\n",
    "        logger.warning(\"沒有成功的測試結果\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(\"\\n測試結果:\")\n",
    "    print(final_df)\n",
    "    return final_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_result = main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corpus example:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column train not in the dataset. Current columns in the dataset: ['doc_id', 'title', 'abstract', 'structured']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 查看資料\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCorpus example:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcorpus\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# 顯示第一個文件\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuery example:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(queries[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# 顯示第一個查詢\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xdxd2\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:2861\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2860\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2861\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xdxd2\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:2845\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2843\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m   2844\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m-> 2845\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2846\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[0;32m   2847\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[0;32m   2848\u001b[0m )\n\u001b[0;32m   2849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Users\\xdxd2\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\formatting\\formatting.py:584\u001b[0m, in \u001b[0;36mquery_table\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m    582\u001b[0m         _raise_bad_key_type(key)\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 584\u001b[0m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    586\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[1;32mc:\\Users\\xdxd2\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\formatting\\formatting.py:521\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[1;34m(key, columns)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[1;32m--> 521\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Column train not in the dataset. Current columns in the dataset: ['doc_id', 'title', 'abstract', 'structured']\""
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 載入兩個配置\n",
    "corpus = load_dataset(\"scifact\", \"corpus\", split=\"train\")\n",
    "queries = load_dataset(\"scifact\", \"claims\", split=\"train\")\n",
    "\n",
    "# 查看資料\n",
    "print(\"\\nCorpus example:\")\n",
    "print(corpus['train'][0])  # 顯示第一個文件\n",
    "\n",
    "print(\"\\nQuery example:\")\n",
    "print(queries['train'][0])  # 顯示第一個查詢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
