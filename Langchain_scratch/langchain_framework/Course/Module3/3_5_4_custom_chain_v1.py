"""
LangChain v1.0+ è‡ªå®šç¾© Chain ç¯„ä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨ v1.0+ çš„ LCEL (LangChain Expression Language) å»ºç«‹è‡ªå®šç¾©è™•ç†æµç¨‹ï¼š
1. åŸºç¤Ž LCEL Chain
2. å¤šéšŽæ®µè™•ç† Chain
3. æ¢ä»¶åˆ†æ”¯ Chain
4. ä¸¦è¡Œè™•ç† Chain
"""

import os
import logging
from typing import List, Dict, Any, Optional
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import (
    RunnablePassthrough,
    RunnableLambda,
    RunnableParallel,
    RunnableBranch
)
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import asyncio
import json

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

def create_basic_chain():
    """å»ºç«‹åŸºç¤Ž v1.0+ LCEL Chain"""

    model = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.1
    )

    prompt = ChatPromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ–‡æ¡ˆå¯«æ‰‹ã€‚
    è«‹æ ¹æ“šä»¥ä¸‹è¦æ±‚æ’°å¯«å…§å®¹ï¼š{request}

    é¢¨æ ¼è¦æ±‚ï¼š{style}
    å­—æ•¸é™åˆ¶ï¼š{word_limit} å­—ä»¥å…§

    è«‹æä¾›é«˜å“è³ªçš„å…§å®¹ã€‚
    """)

    output_parser = StrOutputParser()

    # v1.0+ LCEL èªžæ³•
    chain = prompt | model | output_parser

    return chain

def create_multi_stage_chain():
    """å»ºç«‹å¤šéšŽæ®µè™•ç† Chain"""

    model = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)

    # ç¬¬ä¸€éšŽæ®µï¼šåˆ†æžå…§å®¹
    analysis_prompt = ChatPromptTemplate.from_template("""
    è«‹åˆ†æžä»¥ä¸‹æ–‡æœ¬çš„ä¸»è¦ç‰¹é»žï¼š{text}

    è«‹ä»¥ JSON æ ¼å¼å›žç­”ï¼ŒåŒ…å«ï¼š
    - topic: ä¸»é¡Œ
    - tone: èªžèª¿
    - key_points: é‡é»žåˆ—è¡¨
    """)

    # ç¬¬äºŒéšŽæ®µï¼šæ”¹å¯«å…§å®¹
    rewrite_prompt = ChatPromptTemplate.from_template("""
    æ ¹æ“šåˆ†æžçµæžœï¼š{analysis}

    è«‹å°‡åŽŸæ–‡ï¼š{text}

    æ”¹å¯«æˆæ›´å°ˆæ¥­çš„ç‰ˆæœ¬ï¼Œä¿æŒåŽŸæ„ä½†æå‡å“è³ªã€‚
    """)

    def parse_analysis(text: str) -> dict:
        """è§£æžåˆ†æžçµæžœ"""
        try:
            return json.loads(text)
        except:
            return {"topic": "æœªçŸ¥", "tone": "ä¸­æ€§", "key_points": []}

    # å»ºç«‹å¤šéšŽæ®µ Chain
    chain = (
        # è¼¸å…¥è™•ç†
        {"text": RunnablePassthrough()}
        # ç¬¬ä¸€éšŽæ®µï¼šåˆ†æž
        | RunnableParallel({
            "text": lambda x: x["text"],
            "analysis": analysis_prompt | model | StrOutputParser() | RunnableLambda(parse_analysis)
        })
        # ç¬¬äºŒéšŽæ®µï¼šæ”¹å¯«
        | rewrite_prompt | model | StrOutputParser()
    )

    return chain

def create_branching_chain():
    """å»ºç«‹æ¢ä»¶åˆ†æ”¯ Chain"""

    model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    # åˆ¤æ–·èªžè¨€çš„å‡½æ•¸
    def detect_language(text: str) -> str:
        """ç°¡å–®çš„èªžè¨€æª¢æ¸¬"""
        if any('\u4e00' <= char <= '\u9fff' for char in text):
            return "chinese"
        else:
            return "english"

    # ä¸­æ–‡è™•ç† Chain
    chinese_prompt = ChatPromptTemplate.from_template("""
    è«‹å°‡ä»¥ä¸‹ä¸­æ–‡å…§å®¹ç¸½çµæˆé‡é»žï¼š{text}

    è¦æ±‚ï¼šæ¢åˆ—å¼ï¼Œæ¯é»žä¸è¶…éŽ20å­—
    """)

    # è‹±æ–‡è™•ç† Chain
    english_prompt = ChatPromptTemplate.from_template("""
    Please summarize the following English text into key points: {text}

    Requirements: Bullet points, max 20 words per point
    """)

    chinese_chain = chinese_prompt | model | StrOutputParser()
    english_chain = english_prompt | model | StrOutputParser()

    # v1.0+ æ¢ä»¶åˆ†æ”¯èªžæ³•
    branching_chain = RunnableBranch(
        # æ¢ä»¶ 1: ä¸­æ–‡
        (lambda x: detect_language(x["text"]) == "chinese", chinese_chain),
        # æ¢ä»¶ 2: è‹±æ–‡
        (lambda x: detect_language(x["text"]) == "english", english_chain),
        # é è¨­
        chinese_chain
    )

    return branching_chain

def create_parallel_chain():
    """å»ºç«‹ä¸¦è¡Œè™•ç† Chain"""

    model = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)

    # ä¸åŒçš„è™•ç†ä»»å‹™
    summary_prompt = ChatPromptTemplate.from_template("è«‹ç¸½çµï¼š{text}")
    sentiment_prompt = ChatPromptTemplate.from_template("è«‹åˆ†æžæƒ…æ„Ÿå‚¾å‘ï¼š{text}")
    keywords_prompt = ChatPromptTemplate.from_template("è«‹æå–é—œéµè©žï¼š{text}")

    # ä¸¦è¡Œè™•ç† Chain
    parallel_chain = RunnableParallel({
        "summary": summary_prompt | model | StrOutputParser(),
        "sentiment": sentiment_prompt | model | StrOutputParser(),
        "keywords": keywords_prompt | model | StrOutputParser(),
        "original": RunnablePassthrough()
    })

    return parallel_chain

async def create_async_chain():
    """å»ºç«‹ç•°æ­¥è™•ç† Chain"""

    model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    prompt = ChatPromptTemplate.from_template("è«‹åˆ†æžï¼š{text}")

    # v1.0+ ç•°æ­¥ Chain
    async_chain = prompt | model | StrOutputParser()

    return async_chain

def main():
    """å±•ç¤º v1.0+ è‡ªå®šç¾© Chain çš„ä½¿ç”¨"""

    if not os.getenv("OPENAI_API_KEY"):
        logger.error("è«‹å…ˆè¨­å®š OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸ï¼")
        return

    print("ðŸ”— LangChain v1.0+ è‡ªå®šç¾© Chain ç¯„ä¾‹")
    print("âœ¨ ä½¿ç”¨æœ€æ–°çš„ LCEL (LangChain Expression Language)")

    # æ¸¬è©¦è¼¸å…¥
    test_inputs = {
        "basic": {
            "request": "å¯«ä¸€ç¯‡é—œæ–¼AIçš„ä»‹ç´¹",
            "style": "å°ˆæ¥­ä¸”æ˜“æ‡‚",
            "word_limit": "200"
        },
        "multi_stage": {
            "text": "äººå·¥æ™ºæ…§æ­£åœ¨æ”¹è®Šæˆ‘å€‘çš„ç”Ÿæ´»ï¼Œå¾žæ™ºæ…§æ‰‹æ©Ÿåˆ°è‡ªå‹•é§•é§›æ±½è»Šã€‚"
        },
        "branching": {
            "text": "Artificial intelligence is transforming our daily lives."
        },
        "parallel": {
            "text": "ä»Šå¤©å¤©æ°£å¾ˆå¥½ï¼Œå¿ƒæƒ…æ„‰å¿«ï¼Œæº–å‚™åŽ»å…¬åœ’æ•£æ­¥ã€‚"
        }
    }

    try:
        # 1. æ¸¬è©¦åŸºç¤Ž Chain
        logger.info("ðŸ”¸ æ¸¬è©¦åŸºç¤Ž LCEL Chain...")
        basic_chain = create_basic_chain()
        basic_result = basic_chain.invoke(test_inputs["basic"])
        print(f"\nåŸºç¤Ž Chain çµæžœï¼š\n{basic_result[:100]}...")

        # 2. æ¸¬è©¦å¤šéšŽæ®µ Chain
        logger.info("ðŸ”¸ æ¸¬è©¦å¤šéšŽæ®µ Chain...")
        multi_stage_chain = create_multi_stage_chain()
        multi_stage_result = multi_stage_chain.invoke(test_inputs["multi_stage"])
        print(f"\nå¤šéšŽæ®µ Chain çµæžœï¼š\n{multi_stage_result[:100]}...")

        # 3. æ¸¬è©¦åˆ†æ”¯ Chain
        logger.info("ðŸ”¸ æ¸¬è©¦æ¢ä»¶åˆ†æ”¯ Chain...")
        branching_chain = create_branching_chain()
        branch_result = branching_chain.invoke(test_inputs["branching"])
        print(f"\nåˆ†æ”¯ Chain çµæžœï¼š\n{branch_result}")

        # 4. æ¸¬è©¦ä¸¦è¡Œ Chain
        logger.info("ðŸ”¸ æ¸¬è©¦ä¸¦è¡Œ Chain...")
        parallel_chain = create_parallel_chain()
        parallel_result = parallel_chain.invoke(test_inputs["parallel"])
        print(f"\nä¸¦è¡Œ Chain çµæžœï¼š")
        for key, value in parallel_result.items():
            if key != "original":
                print(f"  {key}: {value[:50]}...")

        logger.info("âœ… æ‰€æœ‰ v1.0+ Chain æ¸¬è©¦å®Œæˆï¼")

    except Exception as e:
        logger.error(f"åŸ·è¡ŒéŽç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        raise

if __name__ == "__main__":
    main()