{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03_å¯¦æˆ°å°ˆæ¡ˆ\n",
    "\n",
    "## ğŸ¯ å­¸ç¿’ç›®æ¨™\n",
    "- é–‹ç™¼çœŸå¯¦çš„é•·æ–‡å¯«ä½œæ‡‰ç”¨å ´æ™¯\n",
    "- å»ºç«‹å¯éƒ¨ç½²çš„å®Œæ•´ç³»çµ±\n",
    "- æŒæ¡ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²æŒ‡å—\n",
    "- å®Œæˆæ•´å€‹èª²ç¨‹çš„å¯¦æˆ°é©—è­‰\n",
    "- å…·å‚™ä¼æ¥­ç´šæ‡‰ç”¨é–‹ç™¼èƒ½åŠ›\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å¯¦æˆ°å°ˆæ¡ˆè¨­è¨ˆ\n",
    "\n",
    "### å°ˆæ¡ˆä¸€ï¼šLinkedIn å°ˆæ¥­æ–‡ç« ç”Ÿæˆå™¨\n",
    "\n",
    "**æ‡‰ç”¨å ´æ™¯**ï¼šç‚ºå°ˆæ¥­äººå£«è‡ªå‹•ç”Ÿæˆé«˜å“è³ªçš„ LinkedIn æ–‡ç« \n",
    "\n",
    "**æ ¸å¿ƒåŠŸèƒ½**ï¼š\n",
    "- æ ¹æ“šè¡Œæ¥­å’Œå°ˆæ¥­èƒŒæ™¯ç”Ÿæˆç›¸é—œä¸»é¡Œ\n",
    "- åˆ†æç›®æ¨™å—çœ¾å’Œå¸‚å ´è¶¨å‹¢\n",
    "- ç”Ÿæˆå…·æœ‰å°ˆæ¥­æ´å¯Ÿçš„é•·æ–‡å…§å®¹\n",
    "- å„ªåŒ–å…§å®¹ä»¥æé«˜ç¤¾äº¤åª’é«”äº’å‹•ç‡\n",
    "\n",
    "**æŠ€è¡“ç‰¹è‰²**ï¼š\n",
    "- å€‹æ€§åŒ–å…§å®¹ç”Ÿæˆ\n",
    "- ç¤¾äº¤åª’é«”å„ªåŒ–\n",
    "- å¤šç¨®å°ˆæ¥­é ˜åŸŸæ”¯æ´\n",
    "- äº’å‹•å…ƒç´ æ•´åˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å°ˆæ¡ˆäºŒï¼šå­¸è¡“ç ”ç©¶å ±å‘Šç”Ÿæˆç³»çµ±\n",
    "\n",
    "**æ‡‰ç”¨å ´æ™¯**ï¼šå”åŠ©ç ”ç©¶è€…å¿«é€Ÿç”Ÿæˆé«˜å“è³ªçš„ç ”ç©¶å ±å‘Š\n",
    "\n",
    "**æ ¸å¿ƒåŠŸèƒ½**ï¼š\n",
    "- æ–‡ç»å›é¡§èˆ‡åˆ†æ\n",
    "- ç ”ç©¶æ–¹æ³•è«–æ•´åˆ\n",
    "- æ•¸æ“šåˆ†æçµæœè§£é‡‹\n",
    "- ç¬¦åˆå­¸è¡“æ¨™æº–çš„æ ¼å¼åŒ–\n",
    "\n",
    "**æŠ€è¡“ç‰¹è‰²**ï¼š\n",
    "- å­¸è¡“è³‡æ–™åº«æ•´åˆ\n",
    "- å¼•ç”¨ç®¡ç†è‡ªå‹•åŒ–\n",
    "- ç ”ç©¶æ–¹æ³•è«–æŒ‡å°\n",
    "- åŒå„•è©•è­°æ”¯æ´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å°ˆæ¡ˆä¸‰ï¼šä¼æ¥­å…§å®¹ç‡Ÿé‹å¹³å°\n",
    "\n",
    "**æ‡‰ç”¨å ´æ™¯**ï¼šä¼æ¥­ç´šå…§å®¹ç”Ÿç”¢èˆ‡ç®¡ç†å¹³å°\n",
    "\n",
    "**æ ¸å¿ƒåŠŸèƒ½**ï¼š\n",
    "- å“ç‰Œè²éŸ³ä¸€è‡´æ€§æ§åˆ¶\n",
    "- å¤šæ ¼å¼å…§å®¹è¼¸å‡º\n",
    "- å…§å®¹å¯©æ ¸èˆ‡ç™¼å¸ƒæµç¨‹\n",
    "- æ•ˆæœè¿½è¹¤èˆ‡å„ªåŒ–\n",
    "\n",
    "**æŠ€è¡“ç‰¹è‰²**ï¼š\n",
    "- ä¼æ¥­ç´šå®‰å…¨æ€§\n",
    "- å·¥ä½œæµç¨‹æ•´åˆ\n",
    "- å¤šç”¨æˆ¶å”ä½œ\n",
    "- API æ¥å£æä¾›"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. å°ˆæ¡ˆä¸€å¯¦ä½œï¼šLinkedIn æ–‡ç« ç”Ÿæˆå™¨\n",
    "\n",
    "### ç’°å¢ƒæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥å¿…è¦å¥—ä»¶\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# CrewAI æ ¸å¿ƒ\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ç’°å¢ƒè®Šæ•¸\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ– LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "print(\"âœ… LinkedIn æ–‡ç« ç”Ÿæˆå™¨ç’°å¢ƒæº–å‚™å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å»ºç«‹ LinkedIn å°ˆç”¨å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"linkedin_trend_analyzer\")\n",
    "def linkedin_trend_analyzer(industry: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze current LinkedIn trends and popular topics in a specific industry.\n",
    "    Returns trending topics, engagement patterns, and content recommendations.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # æ¨¡æ“¬è¶¨å‹¢åˆ†æï¼ˆå¯¦éš›ç’°å¢ƒä¸­æœƒé€£æ¥çœŸå¯¦ APIï¼‰\n",
    "        trending_topics = {\n",
    "            'technology': ['AI/MLç™¼å±•', 'é›²ç«¯é‹ç®—', 'ç¶²è·¯å®‰å…¨', 'æ•¸ä½è½‰å‹', 'å‰µæ–°ç®¡ç†'],\n",
    "            'finance': ['é‡‘èç§‘æŠ€', 'ESGæŠ•è³‡', 'æ•¸ä½è²¨å¹£', 'é¢¨éšªç®¡ç†', 'æ°¸çºŒé‡‘è'],\n",
    "            'healthcare': ['æ•¸ä½å¥åº·', 'é ç«¯é†«ç™‚', 'AIè¨ºæ–·', 'ç²¾æº–é†«ç™‚', 'å¥åº·ç§‘æŠ€'],\n",
    "            'education': ['ç·šä¸Šå­¸ç¿’', 'AIæ•™è‚²', 'æ•¸ä½ç´ é¤Š', 'æ•™è‚²ç§‘æŠ€', 'çµ‚èº«å­¸ç¿’'],\n",
    "            'marketing': ['ç¤¾ç¾¤è¡ŒéŠ·', 'å…§å®¹è¡ŒéŠ·', 'æ•¸æ“šåˆ†æ', 'å€‹äººåŒ–è¡ŒéŠ·', 'å“ç‰Œå»ºç«‹']\n",
    "        }\n",
    "        \n",
    "        industry_lower = industry.lower()\n",
    "        topics = trending_topics.get(industry_lower, ['æ•¸ä½å‰µæ–°', 'æœªä¾†è¶¨å‹¢', 'å°ˆæ¥­ç™¼å±•'])\n",
    "        \n",
    "        analysis = f\"\"\"\n",
    "# {industry} ç”¢æ¥­ LinkedIn è¶¨å‹¢åˆ†æ\n",
    "\n",
    "## ç†±é–€ä¸»é¡Œ\n",
    "{\"\".join([f\"- {topic}\" + \"\\n\" for topic in topics])}\n",
    "\n",
    "## å…§å®¹å»ºè­°\n",
    "- å°ˆæ¥­æ´å¯Ÿåˆ†äº«ï¼šçµåˆå€‹äººç¶“é©—èˆ‡è¡Œæ¥­è¶¨å‹¢\n",
    "- æ¡ˆä¾‹ç ”ç©¶ï¼šå…·é«”çš„æˆåŠŸæ¡ˆä¾‹åˆ†æ\n",
    "- æœªä¾†å±•æœ›ï¼šå°ç”¢æ¥­æœªä¾†ç™¼å±•çš„é æ¸¬\n",
    "- å¯¦ç”¨å»ºè­°ï¼šè®€è€…å¯ä»¥ç«‹å³è¡Œå‹•çš„å…·é«”å»ºè­°\n",
    "\n",
    "## æœ€ä½³ç™¼å¸ƒæ™‚é–“\n",
    "- å·¥ä½œæ—¥æ—©ä¸Š 8-9 é»\n",
    "- å·¥ä½œæ—¥ä¸‹åˆ 1-2 é»\n",
    "- é¿å…å‘¨æœ«å’Œå‡æ—¥\n",
    "\n",
    "## äº’å‹•å„ªåŒ–å»ºè­°\n",
    "- ä½¿ç”¨å¼•äººæ€è€ƒçš„å•é¡Œä½œçµå°¾\n",
    "- åŒ…å«å€‹äººç¶“é©—åˆ†äº«\n",
    "- é©åº¦ä½¿ç”¨ç›¸é—œ hashtags\n",
    "- é¼“å‹µè®€è€…åˆ†äº«çœ‹æ³•å’Œç¶“é©—\n",
    "\"\"\"\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Trend analysis failed: {str(e)}\"\n",
    "\n",
    "@tool(\"content_engagement_optimizer\")\n",
    "def content_engagement_optimizer(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Optimize content for LinkedIn engagement by analyzing and improving elements\n",
    "    that drive professional audience interaction.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # åˆ†æå…§å®¹ç‰¹å¾µ\n",
    "        word_count = len(content.split())\n",
    "        paragraph_count = len([p for p in content.split('\\n\\n') if p.strip()])\n",
    "        question_count = content.count('ï¼Ÿ') + content.count('?')\n",
    "        hashtag_count = content.count('#')\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        # é•·åº¦å„ªåŒ–\n",
    "        if word_count < 300:\n",
    "            recommendations.append(\"å»ºè­°å¢åŠ å…§å®¹æ·±åº¦ï¼ŒLinkedIn é•·æ–‡é€šå¸¸ 500-1500 å­—è¼ƒèƒ½å¼•èµ·é—œæ³¨\")\n",
    "        elif word_count > 2000:\n",
    "            recommendations.append(\"å…§å®¹è¼ƒé•·ï¼Œå»ºè­°åˆ†æ®µç™¼å¸ƒæˆ–ç²¾ç°¡é‡é»\")\n",
    "        \n",
    "        # çµæ§‹å„ªåŒ–\n",
    "        if paragraph_count < 3:\n",
    "            recommendations.append(\"å»ºè­°å¢åŠ æ®µè½åˆ†éš”ï¼Œæå‡é–±è®€é«”é©—\")\n",
    "        \n",
    "        # äº’å‹•å„ªåŒ–\n",
    "        if question_count == 0:\n",
    "            recommendations.append(\"å»ºè­°åŠ å…¥æ€è€ƒæ€§å•é¡Œï¼Œé¼“å‹µè®€è€…äº’å‹•\")\n",
    "        \n",
    "        # å¯è¦‹æ€§å„ªåŒ–\n",
    "        if hashtag_count < 3:\n",
    "            recommendations.append(\"å»ºè­°é©åº¦æ·»åŠ ç›¸é—œ hashtags æå‡å¯è¦‹æ€§\")\n",
    "        elif hashtag_count > 10:\n",
    "            recommendations.append(\"Hashtags éå¤šï¼Œå»ºè­°ç²¾ç°¡ç‚º 3-5 å€‹é‡é»æ¨™ç±¤\")\n",
    "        \n",
    "        # ç”Ÿæˆå„ªåŒ–å»ºè­°\n",
    "        optimization_report = f\"\"\"\n",
    "# LinkedIn å…§å®¹å„ªåŒ–å»ºè­°\n",
    "\n",
    "## å…§å®¹åˆ†æ\n",
    "- æ–‡ç« å­—æ•¸ï¼š{word_count} å­—\n",
    "- æ®µè½æ•¸é‡ï¼š{paragraph_count} æ®µ\n",
    "- äº’å‹•å•é¡Œï¼š{question_count} å€‹\n",
    "- Hashtagsï¼š{hashtag_count} å€‹\n",
    "\n",
    "## å„ªåŒ–å»ºè­°\n",
    "{\"\".join([f\"- {rec}\" + \"\\n\" for rec in recommendations]) if recommendations else \"- âœ… å…§å®¹çµæ§‹è‰¯å¥½ï¼Œç„¡éœ€ç‰¹åˆ¥èª¿æ•´\\n\"}\n",
    "\n",
    "## LinkedIn ç‰¹è‰²å…ƒç´ å»ºè­°\n",
    "- ğŸ”¥ é–‹é ­ Hookï¼šç”¨å¼•äººæ³¨ç›®çš„çµ±è¨ˆæˆ–å•é¡Œé–‹å ´\n",
    "- ğŸ’¡ å€‹äººè¦‹è§£ï¼šåŠ å…¥ç¨ç‰¹çš„å°ˆæ¥­è§€é»\n",
    "- ğŸ“Š å¯¦ä¾‹æ”¯æ’ï¼šä½¿ç”¨å…·é«”æ¡ˆä¾‹æˆ–æ•¸æ“š\n",
    "- ğŸ¤ è¡Œå‹•å‘¼ç±²ï¼šé¼“å‹µè®€è€…åƒèˆ‡è¨è«–æˆ–è¡Œå‹•\n",
    "- ğŸ·ï¸ ç›¸é—œæ¨™ç±¤ï¼šä½¿ç”¨ 3-5 å€‹ç›¸é—œçš„å°ˆæ¥­ hashtags\n",
    "\n",
    "## ç™¼å¸ƒæ™‚æ©Ÿå»ºè­°\n",
    "- æœ€ä½³æ™‚æ®µï¼šé€±äºŒ-é€±å››ï¼Œä¸Šåˆ 8-10 é»æˆ–ä¸‹åˆ 1-3 é»\n",
    "- é¿å…æ™‚æ®µï¼šé€±æœ«ã€ç¯€å‡æ—¥ã€æ·±å¤œæ™‚æ®µ\n",
    "\"\"\"\n",
    "        \n",
    "        return optimization_report\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Engagement optimization failed: {str(e)}\"\n",
    "\n",
    "print(\"âœ… LinkedIn å°ˆç”¨å·¥å…·å·²å»ºç«‹ï¼š\")\n",
    "print(\"   ğŸ“ˆ linkedin_trend_analyzer - LinkedIn è¶¨å‹¢åˆ†æå·¥å…·\")\n",
    "print(\"   ğŸ¯ content_engagement_optimizer - å…§å®¹äº’å‹•å„ªåŒ–å·¥å…·\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å»ºç«‹ LinkedIn å°ˆæ¥­æ™ºèƒ½é«”åœ˜éšŠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinkedIn å¸‚å ´ç ”ç©¶æ™ºèƒ½é«”\n",
    "linkedin_researcher = Agent(\n",
    "    role='LinkedIn Market Research Specialist',\n",
    "    goal='Research trending topics and audience preferences in professional social media',\n",
    "    backstory=\"\"\"\n",
    "    You are a LinkedIn market research specialist with deep understanding of professional \n",
    "    social media trends. You excel at:\n",
    "    \n",
    "    - Identifying trending topics in various industries\n",
    "    - Understanding LinkedIn audience behavior and preferences\n",
    "    - Analyzing engagement patterns and content performance\n",
    "    - Providing insights on professional content strategy\n",
    "    \n",
    "    Your research helps create content that resonates with professional audiences \n",
    "    and drives meaningful engagement.\n",
    "    \"\"\",\n",
    "    tools=[linkedin_trend_analyzer],\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# LinkedIn å…§å®¹å‰µä½œæ™ºèƒ½é«”\n",
    "linkedin_writer = Agent(\n",
    "    role='Professional LinkedIn Content Creator',\n",
    "    goal='Create engaging, professional LinkedIn articles that drive engagement and build thought leadership',\n",
    "    backstory=\"\"\"\n",
    "    You are an expert LinkedIn content creator with a proven track record of creating \n",
    "    viral professional content. Your expertise includes:\n",
    "    \n",
    "    - Crafting compelling professional narratives\n",
    "    - Balancing personal insights with industry knowledge\n",
    "    - Creating content that sparks professional discussions\n",
    "    - Understanding LinkedIn's content format and best practices\n",
    "    \n",
    "    Your content consistently receives high engagement and helps professionals \n",
    "    build their thought leadership and network.\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# LinkedIn å…§å®¹å„ªåŒ–æ™ºèƒ½é«”\n",
    "linkedin_optimizer = Agent(\n",
    "    role='LinkedIn Engagement Optimizer',\n",
    "    goal='Optimize LinkedIn content for maximum professional engagement and reach',\n",
    "    backstory=\"\"\"\n",
    "    You are a LinkedIn engagement optimization specialist who understands the platform's \n",
    "    algorithm and user behavior patterns. You excel at:\n",
    "    \n",
    "    - Optimizing content format for LinkedIn's algorithm\n",
    "    - Enhancing content elements that drive professional engagement\n",
    "    - Crafting compelling calls-to-action for professional audiences\n",
    "    - Timing and hashtag strategy optimization\n",
    "    \n",
    "    Your optimizations consistently increase content reach, engagement, and \n",
    "    professional network growth.\n",
    "    \"\"\",\n",
    "    tools=[content_engagement_optimizer],\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "print(\"âœ… LinkedIn å°ˆæ¥­åœ˜éšŠå·²å»ºç«‹ï¼š\")\n",
    "print(\"   ğŸ“Š Market Research Specialist - å¸‚å ´ç ”ç©¶å°ˆå®¶\")\n",
    "print(\"   âœï¸ Professional Content Creator - å°ˆæ¥­å…§å®¹å‰µä½œè€…\")\n",
    "print(\"   ğŸ¯ Engagement Optimizer - äº’å‹•å„ªåŒ–å°ˆå®¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinkedIn æ–‡ç« ç”Ÿæˆç³»çµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedInArticleGenerator:\n",
    "    \"\"\"LinkedIn å°ˆæ¥­æ–‡ç« ç”Ÿæˆç³»çµ±\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = llm\n",
    "        self.agents = {\n",
    "            'researcher': linkedin_researcher,\n",
    "            'writer': linkedin_writer,\n",
    "            'optimizer': linkedin_optimizer\n",
    "        }\n",
    "        \n",
    "        print(\"ğŸŒŸ LinkedIn æ–‡ç« ç”Ÿæˆç³»çµ±å·²åˆå§‹åŒ–\")\n",
    "    \n",
    "    def generate_linkedin_article(self, profile: Dict[str, str], \n",
    "                                 preferences: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"ç”Ÿæˆ LinkedIn å°ˆæ¥­æ–‡ç« \"\"\"\n",
    "        \n",
    "        print(f\"ğŸ“ ç‚º {profile['name']} ({profile['industry']}) ç”Ÿæˆ LinkedIn æ–‡ç« \")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # é è¨­åå¥½è¨­å®š\n",
    "        if preferences is None:\n",
    "            preferences = {\n",
    "                'article_length': 'medium',  # short, medium, long\n",
    "                'tone': 'professional',      # professional, conversational, thought_leadership\n",
    "                'include_personal_story': True,\n",
    "                'target_engagement': 'high',  # low, medium, high\n",
    "                'hashtag_count': 5\n",
    "            }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # å»ºç«‹ LinkedIn ç‰¹åŒ–ä»»å‹™\n",
    "            research_task = self._create_linkedin_research_task(profile, preferences)\n",
    "            writing_task = self._create_linkedin_writing_task(profile, preferences)\n",
    "            optimization_task = self._create_linkedin_optimization_task(profile, preferences)\n",
    "            \n",
    "            # å»ºç«‹ LinkedIn å°ˆæ¥­åœ˜éšŠ\n",
    "            linkedin_crew = Crew(\n",
    "                agents=[self.agents['researcher'], self.agents['writer'], self.agents['optimizer']],\n",
    "                tasks=[research_task, writing_task, optimization_task],\n",
    "                process=Process.sequential,\n",
    "                verbose=2\n",
    "            )\n",
    "            \n",
    "            print(\"\\nğŸš€ å•Ÿå‹• LinkedIn æ–‡ç« ç”Ÿæˆæµç¨‹...\")\n",
    "            \n",
    "            # åŸ·è¡Œç”Ÿæˆæµç¨‹\n",
    "            result = linkedin_crew.kickoff()\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # è™•ç†çµæœ\n",
    "            final_result = self._process_linkedin_result(\n",
    "                result, profile, preferences, processing_time\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nğŸ‰ LinkedIn æ–‡ç« ç”Ÿæˆå®Œæˆï¼\")\n",
    "            print(f\"   â±ï¸ è™•ç†æ™‚é–“ï¼š{processing_time:.1f} ç§’\")\n",
    "            print(f\"   ğŸ“ æ–‡ç« å­—æ•¸ï¼š{final_result.get('word_count', 0)} å­—\")\n",
    "            print(f\"   ğŸ¯ é æœŸäº’å‹•ï¼š{preferences['target_engagement']}\")\n",
    "            \n",
    "            return final_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_time = time.time() - start_time\n",
    "            print(f\"âŒ LinkedIn æ–‡ç« ç”Ÿæˆå¤±æ•—ï¼š{str(e)}\")\n",
    "            return {'error': str(e), 'processing_time': error_time}\n",
    "    \n",
    "    def _create_linkedin_research_task(self, profile: Dict[str, str], \n",
    "                                     preferences: Dict[str, Any]) -> Task:\n",
    "        \"\"\"å»ºç«‹ LinkedIn ç ”ç©¶ä»»å‹™\"\"\"\n",
    "        return Task(\n",
    "            description=f\"\"\"\n",
    "Research trending topics and content opportunities for LinkedIn article creation.\n",
    "\n",
    "Professional Profile:\n",
    "- Name: {profile['name']}\n",
    "- Industry: {profile['industry']}\n",
    "- Position: {profile['position']}\n",
    "- Experience: {profile['experience']}\n",
    "\n",
    "Research Tasks:\n",
    "1. Use linkedin_trend_analyzer to analyze current trends in {profile['industry']}\n",
    "2. Identify 3-5 relevant topics that align with the professional's expertise\n",
    "3. Analyze what type of content performs well in this industry\n",
    "4. Research recent developments and news in the field\n",
    "5. Identify potential angles for thought leadership content\n",
    "\n",
    "Provide comprehensive research that will inform the creation of a compelling LinkedIn article.\n",
    "\"\"\",\n",
    "            expected_output=\"\"\"\n",
    "A comprehensive research report containing:\n",
    "- Industry trend analysis\n",
    "- Recommended article topics (3-5 options)\n",
    "- Content performance insights\n",
    "- Recent industry developments\n",
    "- Thought leadership opportunities\n",
    "- Target audience analysis\n",
    "\"\"\",\n",
    "            agent=self.agents['researcher']\n",
    "        )\n",
    "    \n",
    "    def _create_linkedin_writing_task(self, profile: Dict[str, str], \n",
    "                                    preferences: Dict[str, Any]) -> Task:\n",
    "        \"\"\"å»ºç«‹ LinkedIn å¯«ä½œä»»å‹™\"\"\"\n",
    "        \n",
    "        length_guide = {\n",
    "            'short': '800-1200 words',\n",
    "            'medium': '1200-1800 words',\n",
    "            'long': '1800-2500 words'\n",
    "        }\n",
    "        \n",
    "        return Task(\n",
    "            description=f\"\"\"\n",
    "Create a compelling LinkedIn article based on the research findings.\n",
    "\n",
    "Author Profile:\n",
    "- {profile['name']}, {profile['position']} with {profile['experience']}\n",
    "- Industry: {profile['industry']}\n",
    "\n",
    "Article Requirements:\n",
    "- Length: {length_guide[preferences['article_length']]}\n",
    "- Tone: {preferences['tone']}\n",
    "- Include personal story: {preferences['include_personal_story']}\n",
    "- Target engagement level: {preferences['target_engagement']}\n",
    "\n",
    "Article Structure:\n",
    "1. **Compelling Hook**: Start with an engaging opening (question, statistic, or story)\n",
    "2. **Professional Context**: Establish credibility and relevance\n",
    "3. **Main Content**: 2-3 key sections with insights and analysis\n",
    "4. **Personal Insights**: Include unique professional perspectives\n",
    "5. **Actionable Takeaways**: Provide practical advice or next steps\n",
    "6. **Engagement Call-to-Action**: End with questions to encourage discussion\n",
    "\n",
    "Writing Guidelines:\n",
    "- Use first person perspective to build personal connection\n",
    "- Include specific examples and real-world applications\n",
    "- Balance professional expertise with accessibility\n",
    "- Incorporate relevant industry terminology naturally\n",
    "- Create content that positions the author as a thought leader\n",
    "\"\"\",\n",
    "            expected_output=f\"\"\"\n",
    "A complete LinkedIn article ({length_guide[preferences['article_length']]}) featuring:\n",
    "- Engaging title optimized for LinkedIn\n",
    "- Compelling opening hook\n",
    "- Well-structured main content with professional insights\n",
    "- Personal anecdotes and expertise integration\n",
    "- Actionable takeaways for readers\n",
    "- Strong call-to-action for engagement\n",
    "- Professional formatting suitable for LinkedIn\n",
    "\"\"\",\n",
    "            agent=self.agents['writer']\n",
    "        )\n",
    "    \n",
    "    def _create_linkedin_optimization_task(self, profile: Dict[str, str], \n",
    "                                         preferences: Dict[str, Any]) -> Task:\n",
    "        \"\"\"å»ºç«‹ LinkedIn å„ªåŒ–ä»»å‹™\"\"\"\n",
    "        return Task(\n",
    "            description=f\"\"\"\n",
    "Optimize the LinkedIn article for maximum engagement and professional impact.\n",
    "\n",
    "Optimization Focus:\n",
    "- Target engagement level: {preferences['target_engagement']}\n",
    "- Hashtag count target: {preferences['hashtag_count']}\n",
    "- Professional industry: {profile['industry']}\n",
    "\n",
    "Optimization Tasks:\n",
    "1. Use content_engagement_optimizer to analyze the article\n",
    "2. Enhance elements that drive LinkedIn engagement:\n",
    "   - Improve opening hook for maximum attention\n",
    "   - Optimize paragraph structure for mobile reading\n",
    "   - Enhance call-to-action effectiveness\n",
    "   - Add strategic line breaks and formatting\n",
    "\n",
    "3. Add LinkedIn-specific optimizations:\n",
    "   - Relevant hashtags for {profile['industry']} ({preferences['hashtag_count']} total)\n",
    "   - Engaging questions to encourage comments\n",
    "   - Professional but approachable tone\n",
    "   - Strategic emoji usage (if appropriate)\n",
    "\n",
    "4. Final quality assurance:\n",
    "   - Ensure professional standards\n",
    "   - Verify industry accuracy\n",
    "   - Confirm engagement potential\n",
    "\n",
    "Deliver a publication-ready LinkedIn article optimized for professional engagement.\n",
    "\"\"\",\n",
    "            expected_output=\"\"\"\n",
    "Final optimized LinkedIn article with:\n",
    "- Enhanced engagement elements\n",
    "- Strategic hashtags and formatting\n",
    "- Professional polish and accuracy\n",
    "- Optimization summary report\n",
    "- Publishing recommendations (timing, strategy)\n",
    "\"\"\",\n",
    "            agent=self.agents['optimizer']\n",
    "        )\n",
    "    \n",
    "    def _process_linkedin_result(self, result: str, profile: Dict[str, str], \n",
    "                               preferences: Dict[str, Any], processing_time: float) -> Dict[str, Any]:\n",
    "        \"\"\"è™•ç† LinkedIn ç”Ÿæˆçµæœ\"\"\"\n",
    "        \n",
    "        content = str(result)\n",
    "        word_count = len(content.split())\n",
    "        \n",
    "        # åˆ†æå…§å®¹ç‰¹å¾µ\n",
    "        hashtag_count = content.count('#')\n",
    "        question_count = content.count('ï¼Ÿ') + content.count('?')\n",
    "        engagement_indicators = content.count('ä½ ') + content.count('æ‚¨') + content.count('æˆ‘å€‘')\n",
    "        \n",
    "        return {\n",
    "            'author_profile': profile,\n",
    "            'content': content,\n",
    "            'word_count': word_count,\n",
    "            'processing_time': processing_time,\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'platform': 'LinkedIn',\n",
    "            'preferences': preferences,\n",
    "            \n",
    "            'engagement_analysis': {\n",
    "                'hashtag_count': hashtag_count,\n",
    "                'question_count': question_count,\n",
    "                'engagement_indicators': engagement_indicators,\n",
    "                'estimated_read_time': f\"{word_count // 200 + 1} åˆ†é˜\"\n",
    "            },\n",
    "            \n",
    "            'publishing_recommendations': {\n",
    "                'best_time': 'é€±äºŒä¸Šåˆ 9:00 æˆ–é€±å››ä¸‹åˆ 2:00',\n",
    "                'target_audience': f\"{profile['industry']} å°ˆæ¥­äººå£«\",\n",
    "                'expected_reach': 'Medium to High' if word_count > 1000 else 'Medium',\n",
    "                'follow_up_strategy': 'ä¸»å‹•å›æ‡‰è©•è«–ï¼Œæ„Ÿè¬åˆ†äº«ï¼Œå»¶çºŒè¨è«–'\n",
    "            }\n",
    "        }\n",
    "\n",
    "# å»ºç«‹ LinkedIn ç”Ÿæˆå™¨\n",
    "linkedin_generator = LinkedInArticleGenerator()\n",
    "print(\"\\nğŸŒŸ LinkedIn æ–‡ç« ç”Ÿæˆç³»çµ±æº–å‚™å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŸ·è¡Œ LinkedIn æ–‡ç« ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾©ç”¨æˆ¶æª”æ¡ˆ\n",
    "user_profile = {\n",
    "    'name': 'å¼µåšå£«',\n",
    "    'position': 'æ•¸æ“šç§‘å­¸ç¸½ç›£',\n",
    "    'industry': 'technology',\n",
    "    'experience': '10å¹´æ•¸æ“šåˆ†æèˆ‡AIæ‡‰ç”¨ç¶“é©—',\n",
    "    'expertise': ['æ©Ÿå™¨å­¸ç¿’', 'æ•¸æ“šç­–ç•¥', 'åœ˜éšŠç®¡ç†']\n",
    "}\n",
    "\n",
    "# æ–‡ç« åå¥½è¨­å®š\n",
    "article_preferences = {\n",
    "    'article_length': 'medium',\n",
    "    'tone': 'professional',\n",
    "    'include_personal_story': True,\n",
    "    'target_engagement': 'high',\n",
    "    'hashtag_count': 5\n",
    "}\n",
    "\n",
    "print(f\"ğŸ‘¤ ç”¨æˆ¶æª”æ¡ˆï¼š{user_profile['name']} - {user_profile['position']}\")\n",
    "print(f\"ğŸ¯ åå¥½è¨­å®šï¼š{article_preferences}\")\n",
    "print(\"\\nğŸš€ é–‹å§‹ç”Ÿæˆ LinkedIn å°ˆæ¥­æ–‡ç« ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸ·è¡Œ LinkedIn æ–‡ç« ç”Ÿæˆ\n",
    "linkedin_result = linkedin_generator.generate_linkedin_article(\n",
    "    profile=user_profile,\n",
    "    preferences=article_preferences\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ†æ LinkedIn ç”Ÿæˆçµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æ LinkedIn ç”Ÿæˆçµæœ\n",
    "if 'error' not in linkedin_result:\n",
    "    print(\"ğŸ“Š LinkedIn æ–‡ç« åˆ†æå ±å‘Šï¼š\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # åŸºæœ¬è³‡è¨Š\n",
    "    print(f\"ğŸ‘¤ ä½œè€…ï¼š{linkedin_result['author_profile']['name']}\")\n",
    "    print(f\"ğŸ¢ ç”¢æ¥­ï¼š{linkedin_result['author_profile']['industry']}\")\n",
    "    print(f\"ğŸ“ å­—æ•¸ï¼š{linkedin_result['word_count']} å­—\")\n",
    "    print(f\"â±ï¸ ç”Ÿæˆæ™‚é–“ï¼š{linkedin_result['processing_time']:.1f} ç§’\")\n",
    "    print(f\"ğŸ“– é ä¼°é–±è®€æ™‚é–“ï¼š{linkedin_result['engagement_analysis']['estimated_read_time']}\")\n",
    "    \n",
    "    # äº’å‹•åˆ†æ\n",
    "    engagement = linkedin_result['engagement_analysis']\n",
    "    print(f\"\\nğŸ¯ äº’å‹•åˆ†æï¼š\")\n",
    "    print(f\"   ğŸ·ï¸ Hashtagsï¼š{engagement['hashtag_count']} å€‹\")\n",
    "    print(f\"   â“ äº’å‹•å•é¡Œï¼š{engagement['question_count']} å€‹\")\n",
    "    print(f\"   ğŸ¤ äº’å‹•æŒ‡æ¨™ï¼š{engagement['engagement_indicators']} å€‹\")\n",
    "    \n",
    "    # ç™¼å¸ƒå»ºè­°\n",
    "    publishing = linkedin_result['publishing_recommendations']\n",
    "    print(f\"\\nğŸ“… ç™¼å¸ƒå»ºè­°ï¼š\")\n",
    "    print(f\"   â° æœ€ä½³æ™‚é–“ï¼š{publishing['best_time']}\")\n",
    "    print(f\"   ğŸ¯ ç›®æ¨™å—çœ¾ï¼š{publishing['target_audience']}\")\n",
    "    print(f\"   ğŸ“ˆ é æœŸè§¸åŠï¼š{publishing['expected_reach']}\")\n",
    "    print(f\"   ğŸ”„ å¾ŒçºŒç­–ç•¥ï¼š{publishing['follow_up_strategy']}\")\n",
    "    \n",
    "    # å…§å®¹é è¦½\n",
    "    print(f\"\\nğŸ“– æ–‡ç« é è¦½ï¼ˆå‰300å­—ï¼‰ï¼š\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    words = linkedin_result['content'].split()\n",
    "    preview_words = words[:300] if len(words) > 300 else words\n",
    "    preview = ' '.join(preview_words)\n",
    "    \n",
    "    print(preview)\n",
    "    if len(words) > 300:\n",
    "        print(\"\\n... [å…§å®¹ç¹¼çºŒ] ...\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ LinkedIn æ–‡ç« ç”Ÿæˆå¤±æ•—ï¼š{linkedin_result.get('error', 'æœªçŸ¥éŒ¯èª¤')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. å¤šå°ˆæ¡ˆæ•´åˆèˆ‡éƒ¨ç½²æº–å‚™\n",
    "\n",
    "### å»ºç«‹çµ±ä¸€çš„å°ˆæ¡ˆç®¡ç†ç³»çµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalSTORMPlatform:\n",
    "    \"\"\"é€šç”¨ STORM å¹³å° - æ•´åˆå¤šç¨®æ‡‰ç”¨å ´æ™¯\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = llm\n",
    "        self.project_history = []\n",
    "        \n",
    "        # æ”¯æ´çš„å°ˆæ¡ˆé¡å‹\n",
    "        self.supported_projects = {\n",
    "            'linkedin': {\n",
    "                'name': 'LinkedIn å°ˆæ¥­æ–‡ç« ',\n",
    "                'target_length': '1200-1800 å­—',\n",
    "                'focus': 'å°ˆæ¥­äº’å‹•èˆ‡æ€æƒ³é ˜å°',\n",
    "                'format': 'ç¤¾äº¤åª’é«”å„ªåŒ–æ ¼å¼'\n",
    "            },\n",
    "            'academic': {\n",
    "                'name': 'å­¸è¡“ç ”ç©¶å ±å‘Š',\n",
    "                'target_length': '2000-3000 å­—',\n",
    "                'focus': 'ç ”ç©¶æ·±åº¦èˆ‡å­¸è¡“åƒ¹å€¼',\n",
    "                'format': 'å­¸è¡“è«–æ–‡æ ¼å¼'\n",
    "            },\n",
    "            'business': {\n",
    "                'name': 'å•†æ¥­åˆ†æå ±å‘Š',\n",
    "                'target_length': '1500-2500 å­—',\n",
    "                'focus': 'å•†æ¥­åƒ¹å€¼èˆ‡ç­–ç•¥å»ºè­°',\n",
    "                'format': 'ä¼æ¥­å ±å‘Šæ ¼å¼'\n",
    "            },\n",
    "            'blog': {\n",
    "                'name': 'å°ˆæ¥­éƒ¨è½æ ¼æ–‡ç« ',\n",
    "                'target_length': '1000-2000 å­—',\n",
    "                'focus': 'çŸ¥è­˜åˆ†äº«èˆ‡è®€è€…åƒ¹å€¼',\n",
    "                'format': 'ç¶²è·¯ç™¼å¸ƒæ ¼å¼'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"ğŸŒ é€šç”¨ STORM å¹³å°åˆå§‹åŒ–å®Œæˆ\")\n",
    "        print(f\"   ğŸ“ æ”¯æ´å°ˆæ¡ˆé¡å‹ï¼š{len(self.supported_projects)} ç¨®\")\n",
    "        \n",
    "    def create_project(self, project_type: str, topic: str, \n",
    "                      custom_config: Dict[str, Any] = None) -> str:\n",
    "        \"\"\"å»ºç«‹æ–°å°ˆæ¡ˆ\"\"\"\n",
    "        \n",
    "        if project_type not in self.supported_projects:\n",
    "            raise ValueError(f\"ä¸æ”¯æ´çš„å°ˆæ¡ˆé¡å‹ï¼š{project_type}\")\n",
    "        \n",
    "        project_info = self.supported_projects[project_type]\n",
    "        project_id = f\"{project_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        project_data = {\n",
    "            'project_id': project_id,\n",
    "            'project_type': project_type,\n",
    "            'topic': topic,\n",
    "            'project_info': project_info,\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'status': 'created',\n",
    "            'config': custom_config or {}\n",
    "        }\n",
    "        \n",
    "        self.project_history.append(project_data)\n",
    "        \n",
    "        print(f\"ğŸ“ å°ˆæ¡ˆå·²å»ºç«‹ï¼š{project_id}\")\n",
    "        print(f\"   é¡å‹ï¼š{project_info['name']}\")\n",
    "        print(f\"   ä¸»é¡Œï¼š{topic}\")\n",
    "        print(f\"   ç›®æ¨™é•·åº¦ï¼š{project_info['target_length']}\")\n",
    "        \n",
    "        return project_id\n",
    "    \n",
    "    def get_project_templates(self) -> str:\n",
    "        \"\"\"ç²å–å°ˆæ¡ˆæ¨¡æ¿è³‡è¨Š\"\"\"\n",
    "        templates = \"\\nğŸ“‹ å¯ç”¨çš„å°ˆæ¡ˆæ¨¡æ¿ï¼š\\n\" + \"=\" * 50 + \"\\n\"\n",
    "        \n",
    "        for proj_type, info in self.supported_projects.items():\n",
    "            templates += f\"\\nğŸ”¸ **{proj_type.upper()}** - {info['name']}\\n\"\n",
    "            templates += f\"   ğŸ“ é•·åº¦ï¼š{info['target_length']}\\n\"\n",
    "            templates += f\"   ğŸ¯ é‡é»ï¼š{info['focus']}\\n\"\n",
    "            templates += f\"   ğŸ“„ æ ¼å¼ï¼š{info['format']}\\n\"\n",
    "        \n",
    "        return templates\n",
    "    \n",
    "    def generate_project_summary(self) -> str:\n",
    "        \"\"\"ç”Ÿæˆå°ˆæ¡ˆçµ±è¨ˆæ‘˜è¦\"\"\"\n",
    "        if not self.project_history:\n",
    "            return \"æš«ç„¡å°ˆæ¡ˆè¨˜éŒ„\"\n",
    "        \n",
    "        # çµ±è¨ˆåˆ†æ\n",
    "        total_projects = len(self.project_history)\n",
    "        project_types = {}\n",
    "        \n",
    "        for project in self.project_history:\n",
    "            proj_type = project['project_type']\n",
    "            project_types[proj_type] = project_types.get(proj_type, 0) + 1\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "# å°ˆæ¡ˆçµ±è¨ˆæ‘˜è¦\n",
    "\n",
    "## åŸºæœ¬çµ±è¨ˆ\n",
    "- ç¸½å°ˆæ¡ˆæ•¸ï¼š{total_projects}\n",
    "- å°ˆæ¡ˆé¡å‹åˆ†ä½ˆï¼š\n",
    "{\"\".join([f\"  - {ptype}: {count} å€‹\" + \"\\n\" for ptype, count in project_types.items()])}\n",
    "\n",
    "## æœ€è¿‘å°ˆæ¡ˆ\n",
    "{\"\".join([f\"- {p['created_at'][:10]}: {p['topic'][:50]}...\" + \"\\n\" for p in self.project_history[-3:]])}\n",
    "\"\"\"\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# å»ºç«‹é€šç”¨å¹³å°\n",
    "universal_platform = UniversalSTORMPlatform()\n",
    "\n",
    "# é¡¯ç¤ºå¯ç”¨æ¨¡æ¿\n",
    "print(universal_platform.get_project_templates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²æŒ‡å—\n",
    "\n",
    "### éƒ¨ç½²æ¶æ§‹è¨­è¨ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿç”¢ç’°å¢ƒé…ç½®æ¨¡æ¿\n",
    "production_config_template = {\n",
    "    \"environment\": \"production\",\n",
    "    \"api_settings\": {\n",
    "        \"openai_api_key\": \"${OPENAI_API_KEY}\",\n",
    "        \"anthropic_api_key\": \"${ANTHROPIC_API_KEY}\",\n",
    "        \"serper_api_key\": \"${SERPER_API_KEY}\",\n",
    "        \"rate_limit_per_minute\": 60,\n",
    "        \"timeout_seconds\": 300,\n",
    "        \"retry_attempts\": 3\n",
    "    },\n",
    "    \"system_settings\": {\n",
    "        \"max_concurrent_requests\": 10,\n",
    "        \"enable_caching\": True,\n",
    "        \"cache_ttl_seconds\": 3600,\n",
    "        \"enable_logging\": True,\n",
    "        \"log_level\": \"INFO\"\n",
    "    },\n",
    "    \"quality_settings\": {\n",
    "        \"min_quality_threshold\": 0.8,\n",
    "        \"enable_fact_checking\": True,\n",
    "        \"enable_plagiarism_check\": True,\n",
    "        \"auto_retry_on_low_quality\": True\n",
    "    },\n",
    "    \"monitoring\": {\n",
    "        \"enable_metrics\": True,\n",
    "        \"enable_alerts\": True,\n",
    "        \"performance_tracking\": True,\n",
    "        \"error_reporting\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# ä¿å­˜ç”Ÿç”¢é…ç½®æ¨¡æ¿\n",
    "with open('production_config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(production_config_template, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"âœ… ç”Ÿç”¢ç’°å¢ƒé…ç½®æ¨¡æ¿å·²å»ºç«‹ï¼šproduction_config.json\")\n",
    "\n",
    "# é¡¯ç¤ºé…ç½®å…§å®¹\n",
    "print(\"\\nğŸ“‹ ç”Ÿç”¢é…ç½®æ¨¡æ¿å…§å®¹ï¼š\")\n",
    "print(\"=\" * 50)\n",
    "for category, settings in production_config_template.items():\n",
    "    print(f\"\\nğŸ”¸ **{category.upper()}**:\")\n",
    "    if isinstance(settings, dict):\n",
    "        for key, value in settings.items():\n",
    "            print(f\"   - {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"   - {settings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker éƒ¨ç½²é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ Dockerfile\n",
    "dockerfile_content = \"\"\"\n",
    "# STORM é•·æ–‡å¯«ä½œç³»çµ± Dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# è¨­å®šå·¥ä½œç›®éŒ„\n",
    "WORKDIR /app\n",
    "\n",
    "# å®‰è£ç³»çµ±ä¾è³´\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    gcc \\\n",
    "    g++ \\\n",
    "    curl \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# è¤‡è£½éœ€æ±‚æª”æ¡ˆ\n",
    "COPY requirements.txt .\n",
    "\n",
    "# å®‰è£ Python ä¾è³´\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# è¤‡è£½æ‡‰ç”¨ç¨‹å¼ä»£ç¢¼\n",
    "COPY . .\n",
    "\n",
    "# è¨­å®šç’°å¢ƒè®Šæ•¸\n",
    "ENV PYTHONPATH=/app\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "\n",
    "# æš´éœ²ç«¯å£\n",
    "EXPOSE 8000\n",
    "\n",
    "# å¥åº·æª¢æŸ¥\n",
    "HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\n",
    "  CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# å•Ÿå‹•æ‡‰ç”¨\n",
    "CMD [\"python\", \"app.py\"]\n",
    "\"\"\"\n",
    "\n",
    "# å»ºç«‹ requirements.txt\n",
    "requirements_content = \"\"\"\n",
    "crewai==0.55.2\n",
    "crewai-tools==0.8.3\n",
    "langchain-openai==0.1.7\n",
    "langchain==0.2.5\n",
    "python-dotenv==1.0.0\n",
    "requests==2.31.0\n",
    "beautifulsoup4==4.12.2\n",
    "duckduckgo-search==6.1.5\n",
    "fastapi==0.111.0\n",
    "uvicorn==0.29.0\n",
    "pydantic==2.7.1\n",
    "\"\"\".strip()\n",
    "\n",
    "# å»ºç«‹ docker-compose.yml\n",
    "docker_compose_content = \"\"\"\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  storm-api:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - OPENAI_API_KEY=${OPENAI_API_KEY}\n",
    "      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n",
    "      - ENVIRONMENT=production\n",
    "    volumes:\n",
    "      - ./data:/app/data\n",
    "      - ./logs:/app/logs\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "\n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "    restart: unless-stopped\n",
    "\n",
    "volumes:\n",
    "  redis_data:\n",
    "\"\"\".strip()\n",
    "\n",
    "# ä¿å­˜éƒ¨ç½²æª”æ¡ˆ\n",
    "files_to_create = [\n",
    "    ('Dockerfile', dockerfile_content),\n",
    "    ('requirements.txt', requirements_content),\n",
    "    ('docker-compose.yml', docker_compose_content)\n",
    "]\n",
    "\n",
    "for filename, content in files_to_create:\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        print(f\"âœ… {filename} å·²å»ºç«‹\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {filename} å»ºç«‹å¤±æ•—ï¼š{str(e)}\")\n",
    "\n",
    "print(\"\\nğŸ³ Docker éƒ¨ç½²æª”æ¡ˆå·²æº–å‚™å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API æœå‹™æ¶æ§‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ FastAPI æ‡‰ç”¨ç¨‹å¼æ¶æ§‹\n",
    "fastapi_app_content = \"\"\"\n",
    "# FastAPI STORM æœå‹™\n",
    "from fastapi import FastAPI, BackgroundTasks, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, Any, Optional, List\n",
    "import asyncio\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# å»ºç«‹ FastAPI æ‡‰ç”¨\n",
    "app = FastAPI(\n",
    "    title=\"STORM é•·æ–‡å¯«ä½œ API\",\n",
    "    description=\"å¤šæ™ºèƒ½é«”é•·æ–‡å¯«ä½œç³»çµ± API æœå‹™\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# è³‡æ–™æ¨¡å‹\n",
    "class ArticleRequest(BaseModel):\n",
    "    topic: str\n",
    "    project_type: str = \"academic\"  # linkedin, academic, business, blog\n",
    "    target_length: Optional[int] = 2000\n",
    "    style_preferences: Optional[Dict[str, Any]] = None\n",
    "    priority: str = \"normal\"  # low, normal, high\n",
    "\n",
    "class ArticleResponse(BaseModel):\n",
    "    task_id: str\n",
    "    status: str\n",
    "    message: str\n",
    "    estimated_time: Optional[int] = None\n",
    "\n",
    "class ArticleResult(BaseModel):\n",
    "    task_id: str\n",
    "    topic: str\n",
    "    content: str\n",
    "    word_count: int\n",
    "    quality_score: float\n",
    "    generated_at: str\n",
    "    processing_time: float\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "# ä»»å‹™ç‹€æ…‹ç®¡ç†\n",
    "task_storage = {}\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\n",
    "        \"service\": \"STORM é•·æ–‡å¯«ä½œ API\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"status\": \"running\",\n",
    "        \"endpoints\": {\n",
    "            \"POST /generate\": \"ç”Ÿæˆæ–‡ç« è«‹æ±‚\",\n",
    "            \"GET /status/{task_id}\": \"æŸ¥è©¢ä»»å‹™ç‹€æ…‹\",\n",
    "            \"GET /result/{task_id}\": \"ç²å–ç”Ÿæˆçµæœ\",\n",
    "            \"GET /health\": \"å¥åº·æª¢æŸ¥\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\", \"timestamp\": datetime.now().isoformat()}\n",
    "\n",
    "@app.post(\"/generate\", response_model=ArticleResponse)\n",
    "async def generate_article(request: ArticleRequest, background_tasks: BackgroundTasks):\n",
    "    \"\"\"æäº¤æ–‡ç« ç”Ÿæˆè«‹æ±‚\"\"\"\n",
    "    \n",
    "    # ç”Ÿæˆä»»å‹™ ID\n",
    "    task_id = str(uuid.uuid4())\n",
    "    \n",
    "    # åˆå§‹åŒ–ä»»å‹™ç‹€æ…‹\n",
    "    task_storage[task_id] = {\n",
    "        \"status\": \"queued\",\n",
    "        \"topic\": request.topic,\n",
    "        \"project_type\": request.project_type,\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"progress\": 0\n",
    "    }\n",
    "    \n",
    "    # ä¼°ç®—è™•ç†æ™‚é–“\n",
    "    estimated_time = {\n",
    "        \"linkedin\": 180,   # 3 åˆ†é˜\n",
    "        \"academic\": 300,   # 5 åˆ†é˜\n",
    "        \"business\": 240,   # 4 åˆ†é˜\n",
    "        \"blog\": 200        # 3.5 åˆ†é˜\n",
    "    }.get(request.project_type, 300)\n",
    "    \n",
    "    # èƒŒæ™¯ä»»å‹™åŸ·è¡Œ\n",
    "    background_tasks.add_task(\n",
    "        process_article_generation, \n",
    "        task_id, \n",
    "        request.dict()\n",
    "    )\n",
    "    \n",
    "    return ArticleResponse(\n",
    "        task_id=task_id,\n",
    "        status=\"queued\",\n",
    "        message=\"æ–‡ç« ç”Ÿæˆä»»å‹™å·²æ’å…¥éšŠåˆ—\",\n",
    "        estimated_time=estimated_time\n",
    "    )\n",
    "\n",
    "@app.get(\"/status/{task_id}\")\n",
    "async def get_task_status(task_id: str):\n",
    "    \"\"\"æŸ¥è©¢ä»»å‹™ç‹€æ…‹\"\"\"\n",
    "    \n",
    "    if task_id not in task_storage:\n",
    "        raise HTTPException(status_code=404, detail=\"ä»»å‹™æœªæ‰¾åˆ°\")\n",
    "    \n",
    "    task_info = task_storage[task_id]\n",
    "    \n",
    "    return {\n",
    "        \"task_id\": task_id,\n",
    "        \"status\": task_info[\"status\"],\n",
    "        \"topic\": task_info[\"topic\"],\n",
    "        \"progress\": task_info[\"progress\"],\n",
    "        \"created_at\": task_info[\"created_at\"],\n",
    "        \"updated_at\": task_info.get(\"updated_at\", task_info[\"created_at\"])\n",
    "    }\n",
    "\n",
    "@app.get(\"/result/{task_id}\", response_model=ArticleResult)\n",
    "async def get_article_result(task_id: str):\n",
    "    \"\"\"ç²å–æ–‡ç« ç”Ÿæˆçµæœ\"\"\"\n",
    "    \n",
    "    if task_id not in task_storage:\n",
    "        raise HTTPException(status_code=404, detail=\"ä»»å‹™æœªæ‰¾åˆ°\")\n",
    "    \n",
    "    task_info = task_storage[task_id]\n",
    "    \n",
    "    if task_info[\"status\"] != \"completed\":\n",
    "        raise HTTPException(\n",
    "            status_code=202, \n",
    "            detail=f\"ä»»å‹™å°šæœªå®Œæˆï¼Œç•¶å‰ç‹€æ…‹ï¼š{task_info['status']}\"\n",
    "        )\n",
    "    \n",
    "    if \"result\" not in task_info:\n",
    "        raise HTTPException(status_code=500, detail=\"ä»»å‹™çµæœéºå¤±\")\n",
    "    \n",
    "    return ArticleResult(**task_info[\"result\"])\n",
    "\n",
    "async def process_article_generation(task_id: str, request_data: Dict[str, Any]):\n",
    "    \"\"\"èƒŒæ™¯è™•ç†æ–‡ç« ç”Ÿæˆ\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # æ›´æ–°ç‹€æ…‹ç‚ºè™•ç†ä¸­\n",
    "        task_storage[task_id].update({\n",
    "            \"status\": \"processing\",\n",
    "            \"progress\": 10,\n",
    "            \"updated_at\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # æ¨¡æ“¬è™•ç†éç¨‹ï¼ˆå¯¦éš›ç’°å¢ƒä¸­æœƒå‘¼å« STORM ç³»çµ±ï¼‰\n",
    "        await asyncio.sleep(2)  # æ¨¡æ“¬ç ”ç©¶éšæ®µ\n",
    "        task_storage[task_id][\"progress\"] = 40\n",
    "        \n",
    "        await asyncio.sleep(2)  # æ¨¡æ“¬å¯«ä½œéšæ®µ\n",
    "        task_storage[task_id][\"progress\"] = 80\n",
    "        \n",
    "        await asyncio.sleep(1)  # æ¨¡æ“¬å„ªåŒ–éšæ®µ\n",
    "        task_storage[task_id][\"progress\"] = 100\n",
    "        \n",
    "        # æ¨¡æ“¬ç”Ÿæˆçµæœ\n",
    "        result = {\n",
    "            \"task_id\": task_id,\n",
    "            \"topic\": request_data[\"topic\"],\n",
    "            \"content\": f\"# {request_data['topic']}\\n\\n[ç”± STORM ç³»çµ±ç”Ÿæˆçš„å®Œæ•´æ–‡ç« å…§å®¹...]\\n\\n*æœ¬æ–‡ç”± CrewAI STORM ç³»çµ±ç”Ÿæˆ*\",\n",
    "            \"word_count\": 1650,\n",
    "            \"quality_score\": 0.87,\n",
    "            \"generated_at\": datetime.now().isoformat(),\n",
    "            \"processing_time\": 5.0,\n",
    "            \"metadata\": {\n",
    "                \"project_type\": request_data[\"project_type\"],\n",
    "                \"target_length\": request_data[\"target_length\"],\n",
    "                \"agents_used\": [\"researcher\", \"writer\", \"optimizer\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # å®Œæˆä»»å‹™\n",
    "        task_storage[task_id].update({\n",
    "            \"status\": \"completed\",\n",
    "            \"progress\": 100,\n",
    "            \"result\": result,\n",
    "            \"completed_at\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        # è™•ç†éŒ¯èª¤\n",
    "        task_storage[task_id].update({\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e),\n",
    "            \"failed_at\": datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\"\"\"\n",
    "\n",
    "# ä¿å­˜ API æ‡‰ç”¨ç¨‹å¼\n",
    "with open('app.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(fastapi_app_content)\n",
    "\n",
    "with open('requirements.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "with open('docker-compose.yml', 'w', encoding='utf-8') as f:\n",
    "    f.write(docker_compose_content)\n",
    "\n",
    "with open('Dockerfile', 'w', encoding='utf-8') as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"âœ… API æœå‹™æª”æ¡ˆå·²å»ºç«‹ï¼š\")\n",
    "print(\"   ğŸ app.py - FastAPI ä¸»æ‡‰ç”¨ç¨‹å¼\")\n",
    "print(\"   ğŸ“¦ requirements.txt - Python ä¾è³´æ¸…å–®\")\n",
    "print(\"   ğŸ³ Dockerfile - Docker æ˜ åƒæª”é…ç½®\")\n",
    "print(\"   ğŸ”§ docker-compose.yml - Docker Compose é…ç½®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. éƒ¨ç½²æŒ‡å—èˆ‡ç¶­è­·\n",
    "\n",
    "### éƒ¨ç½²æ­¥é©ŸæŒ‡å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹éƒ¨ç½²æŒ‡å—\n",
    "deployment_guide = \"\"\"\n",
    "# STORM é•·æ–‡å¯«ä½œç³»çµ±éƒ¨ç½²æŒ‡å—\n",
    "\n",
    "## ğŸš€ å¿«é€Ÿéƒ¨ç½²\n",
    "\n",
    "### å…ˆæ±ºæ¢ä»¶\n",
    "- Docker å’Œ Docker Compose å·²å®‰è£\n",
    "- å…·å‚™æœ‰æ•ˆçš„ OpenAI API é‡‘é‘°\n",
    "- è‡³å°‘ 2GB RAM å’Œ 10GB å„²å­˜ç©ºé–“\n",
    "\n",
    "### éƒ¨ç½²æ­¥é©Ÿ\n",
    "\n",
    "1. **ç’°å¢ƒæº–å‚™**\n",
    "```bash\n",
    "# è¤‡è£½å°ˆæ¡ˆæª”æ¡ˆ\n",
    "git clone <repository-url>\n",
    "cd storm-writing-system\n",
    "\n",
    "# è¨­å®šç’°å¢ƒè®Šæ•¸\n",
    "cp .env.example .env\n",
    "# ç·¨è¼¯ .env æª”æ¡ˆï¼ŒåŠ å…¥ä½ çš„ API é‡‘é‘°\n",
    "```\n",
    "\n",
    "2. **Docker éƒ¨ç½²**\n",
    "```bash\n",
    "# å»ºæ§‹æ˜ åƒæª”\n",
    "docker-compose build\n",
    "\n",
    "# å•Ÿå‹•æœå‹™\n",
    "docker-compose up -d\n",
    "\n",
    "# æª¢æŸ¥æœå‹™ç‹€æ…‹\n",
    "docker-compose ps\n",
    "```\n",
    "\n",
    "3. **å¥åº·æª¢æŸ¥**\n",
    "```bash\n",
    "# æ¸¬è©¦ API é€£ç·š\n",
    "curl http://localhost:8000/health\n",
    "\n",
    "# æŸ¥çœ‹ API æ–‡æª”\n",
    "# é–‹å•Ÿç€è¦½å™¨è¨ªå•ï¼šhttp://localhost:8000/docs\n",
    "```\n",
    "\n",
    "## ğŸ”§ é…ç½®ç®¡ç†\n",
    "\n",
    "### ç’°å¢ƒè®Šæ•¸è¨­å®š\n",
    "```env\n",
    "# API é‡‘é‘°\n",
    "OPENAI_API_KEY=your-openai-api-key\n",
    "ANTHROPIC_API_KEY=your-anthropic-api-key\n",
    "\n",
    "# ç³»çµ±è¨­å®š\n",
    "ENVIRONMENT=production\n",
    "LOG_LEVEL=INFO\n",
    "MAX_CONCURRENT_TASKS=10\n",
    "\n",
    "# è³‡æ–™åº«è¨­å®šï¼ˆå¦‚æœä½¿ç”¨ï¼‰\n",
    "DATABASE_URL=postgresql://user:pass@localhost:5432/storm\n",
    "REDIS_URL=redis://localhost:6379/0\n",
    "```\n",
    "\n",
    "### ç³»çµ±ç›£æ§\n",
    "```bash\n",
    "# æŸ¥çœ‹æ—¥èªŒ\n",
    "docker-compose logs -f storm-api\n",
    "\n",
    "# ç›£æ§ç³»çµ±è³‡æº\n",
    "docker stats\n",
    "\n",
    "# æŸ¥çœ‹å¥åº·ç‹€æ…‹\n",
    "curl http://localhost:8000/health\n",
    "```\n",
    "\n",
    "## ğŸ“Š API ä½¿ç”¨ç¯„ä¾‹\n",
    "\n",
    "### ç”Ÿæˆæ–‡ç« è«‹æ±‚\n",
    "```python\n",
    "import requests\n",
    "\n",
    "# æäº¤ç”Ÿæˆè«‹æ±‚\n",
    "response = requests.post('http://localhost:8000/generate', json={\n",
    "    \"topic\": \"äººå·¥æ™ºæ…§åœ¨é‡‘èæ¥­çš„å‰µæ–°æ‡‰ç”¨\",\n",
    "    \"project_type\": \"business\",\n",
    "    \"target_length\": 2000,\n",
    "    \"style_preferences\": {\n",
    "        \"tone\": \"professional\",\n",
    "        \"include_examples\": True\n",
    "    }\n",
    "})\n",
    "\n",
    "task_info = response.json()\n",
    "task_id = task_info['task_id']\n",
    "\n",
    "# æŸ¥è©¢ç‹€æ…‹\n",
    "status_response = requests.get(f'http://localhost:8000/status/{task_id}')\n",
    "print(status_response.json())\n",
    "\n",
    "# ç²å–çµæœ\n",
    "result_response = requests.get(f'http://localhost:8000/result/{task_id}')\n",
    "article = result_response.json()\n",
    "```\n",
    "\n",
    "## ğŸ”’ å®‰å…¨æ€§è€ƒé‡\n",
    "\n",
    "### ç”Ÿç”¢ç’°å¢ƒå®‰å…¨\n",
    "- ä½¿ç”¨ HTTPS åŠ å¯†é€šè¨Š\n",
    "- å¯¦æ–½ API é‡‘é‘°é©—è­‰\n",
    "- è¨­å®šé€Ÿç‡é™åˆ¶\n",
    "- å®šæœŸæ›´æ–°ä¾è³´å¥—ä»¶\n",
    "- ç›£æ§ç•°å¸¸æ´»å‹•\n",
    "\n",
    "### è³‡æ–™ä¿è­·\n",
    "- åŠ å¯†æ•æ„Ÿè³‡æ–™å„²å­˜\n",
    "- å®šæœŸå‚™ä»½é‡è¦è³‡æ–™\n",
    "- å¯¦æ–½å­˜å–æ§åˆ¶\n",
    "- éµå¾ªè³‡æ–™éš±ç§æ³•è¦\n",
    "\n",
    "## ğŸ“ˆ æ“´å±•èˆ‡å„ªåŒ–\n",
    "\n",
    "### æ°´å¹³æ“´å±•\n",
    "```yaml\n",
    "# docker-compose.scale.yml\n",
    "version: '3.8'\n",
    "services:\n",
    "  storm-api:\n",
    "    deploy:\n",
    "      replicas: 3\n",
    "  \n",
    "  nginx:\n",
    "    image: nginx:alpine\n",
    "    ports:\n",
    "      - \"80:80\"\n",
    "    depends_on:\n",
    "      - storm-api\n",
    "```\n",
    "\n",
    "### ç›£æ§èˆ‡æ—¥èªŒ\n",
    "```bash\n",
    "# ä½¿ç”¨ ELK Stack é€²è¡Œæ—¥èªŒåˆ†æ\n",
    "docker run -d --name elasticsearch elasticsearch:7.15.0\n",
    "docker run -d --name kibana kibana:7.15.0\n",
    "docker run -d --name logstash logstash:7.15.0\n",
    "```\n",
    "\n",
    "## ğŸ› ï¸ æ•…éšœæ’é™¤\n",
    "\n",
    "### å¸¸è¦‹å•é¡Œ\n",
    "1. **API é‡‘é‘°å•é¡Œ**ï¼šæª¢æŸ¥ç’°å¢ƒè®Šæ•¸è¨­å®š\n",
    "2. **è¨˜æ†¶é«”ä¸è¶³**ï¼šå¢åŠ  Docker è¨˜æ†¶é«”é…é¡\n",
    "3. **ç¶²è·¯é€£ç·šå•é¡Œ**ï¼šæª¢æŸ¥é˜²ç«ç‰†è¨­å®š\n",
    "4. **ä¾è³´å¥—ä»¶è¡çª**ï¼šä½¿ç”¨å›ºå®šç‰ˆæœ¬è™Ÿ\n",
    "\n",
    "### é™¤éŒ¯æ­¥é©Ÿ\n",
    "```bash\n",
    "# æŸ¥çœ‹è©³ç´°æ—¥èªŒ\n",
    "docker-compose logs --tail=100 storm-api\n",
    "\n",
    "# é€²å…¥å®¹å™¨é™¤éŒ¯\n",
    "docker-compose exec storm-api bash\n",
    "\n",
    "# é‡æ–°å•Ÿå‹•æœå‹™\n",
    "docker-compose restart storm-api\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# ä¿å­˜éƒ¨ç½²æŒ‡å—\n",
    "with open('DEPLOYMENT_GUIDE.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(deployment_guide)\n",
    "\n",
    "print(\"âœ… éƒ¨ç½²æŒ‡å—å·²å»ºç«‹ï¼šDEPLOYMENT_GUIDE.md\")\n",
    "print(\"   ğŸš€ åŒ…å«å®Œæ•´çš„éƒ¨ç½²ã€é…ç½®ã€ç›£æ§æŒ‡å—\")\n",
    "print(\"   ğŸ”§ æä¾›æ•…éšœæ’é™¤å’Œæœ€ä½³å¯¦è¸å»ºè­°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14. æœ€çµ‚å°ˆæ¡ˆé©—è­‰\n",
    "\n",
    "### å®Œæ•´ç³»çµ±åŠŸèƒ½æª¢æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_system_validation():\n",
    "    \"\"\"æœ€çµ‚ç³»çµ±é©—è­‰\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” é€²è¡Œæœ€çµ‚ç³»çµ±åŠŸèƒ½é©—è­‰...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    validation_results = []\n",
    "    \n",
    "    # æª¢æŸ¥æª”æ¡ˆå®Œæ•´æ€§\n",
    "    required_files = [\n",
    "        '01_æ¦‚å¿µç†è§£/01_å¤šæ™ºèƒ½é«”ç³»çµ±æ¦‚å¿µ.md',\n",
    "        '01_æ¦‚å¿µç†è§£/02_STORM_é•·æ–‡å¯«ä½œåŸç†.md',\n",
    "        '02_æ‰‹å‹•å¯¦ä½œ/01_ç’°å¢ƒè¨­å®š.ipynb',\n",
    "        '02_æ‰‹å‹•å¯¦ä½œ/02_æ‰‹å‹•ç‰ˆ_ç ”ç©¶æ™ºèƒ½é«”.ipynb',\n",
    "        '02_æ‰‹å‹•å¯¦ä½œ/03_æ‰‹å‹•ç‰ˆ_å¯«ä½œæ™ºèƒ½é«”.ipynb',\n",
    "        '02_æ‰‹å‹•å¯¦ä½œ/04_æ‰‹å‹•ç‰ˆ_å®Œæ•´æµç¨‹.ipynb',\n",
    "        '03_æ¡†æ¶å¯¦ä½œ/01_CrewAI_åŸºç¤.ipynb',\n",
    "        '03_æ¡†æ¶å¯¦ä½œ/02_CrewAI_é•·æ–‡å¯«ä½œç³»çµ±.ipynb',\n",
    "        '03_æ¡†æ¶å¯¦ä½œ/03_å¯¦æˆ°å°ˆæ¡ˆ.ipynb'\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ“ æª”æ¡ˆå®Œæ•´æ€§æª¢æŸ¥ï¼š\")\n",
    "    for file_path in required_files:\n",
    "        if os.path.exists(file_path):\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            print(f\"   âœ… {file_path} ({file_size} bytes)\")\n",
    "            validation_results.append(True)\n",
    "        else:\n",
    "            print(f\"   âŒ {file_path} - æª”æ¡ˆéºå¤±\")\n",
    "            validation_results.append(False)\n",
    "    \n",
    "    # æª¢æŸ¥éƒ¨ç½²æª”æ¡ˆ\n",
    "    deployment_files = [\n",
    "        'app.py',\n",
    "        'requirements.txt', \n",
    "        'Dockerfile',\n",
    "        'docker-compose.yml',\n",
    "        'production_config.json',\n",
    "        'DEPLOYMENT_GUIDE.md'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nğŸ³ éƒ¨ç½²æª”æ¡ˆæª¢æŸ¥ï¼š\")\n",
    "    for file_path in deployment_files:\n",
    "        if os.path.exists(file_path):\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            print(f\"   âœ… {file_path} ({file_size} bytes)\")\n",
    "            validation_results.append(True)\n",
    "        else:\n",
    "            print(f\"   âŒ {file_path} - æª”æ¡ˆéºå¤±\")\n",
    "            validation_results.append(False)\n",
    "    \n",
    "    # ç’°å¢ƒæª¢æŸ¥\n",
    "    print(\"\\nğŸ”§ ç’°å¢ƒé…ç½®æª¢æŸ¥ï¼š\")\n",
    "    \n",
    "    # æª¢æŸ¥ API é‡‘é‘°\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if api_key and api_key != 'your-openai-api-key-here':\n",
    "        print(\"   âœ… OpenAI API é‡‘é‘°å·²è¨­å®š\")\n",
    "        validation_results.append(True)\n",
    "    else:\n",
    "        print(\"   âš ï¸ OpenAI API é‡‘é‘°æœªè¨­å®š\")\n",
    "        validation_results.append(False)\n",
    "    \n",
    "    # æª¢æŸ¥å¥—ä»¶\n",
    "    try:\n",
    "        import crewai\n",
    "        print(f\"   âœ… CrewAI ç‰ˆæœ¬ï¼š{crewai.__version__}\")\n",
    "        validation_results.append(True)\n",
    "    except ImportError:\n",
    "        print(\"   âŒ CrewAI å¥—ä»¶æœªå®‰è£\")\n",
    "        validation_results.append(False)\n",
    "    \n",
    "    # è¨ˆç®—å®Œæ•´æ€§åˆ†æ•¸\n",
    "    success_rate = sum(validation_results) / len(validation_results)\n",
    "    \n",
    "    print(\"\\nğŸ“Š ç³»çµ±é©—è­‰çµæœï¼š\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"å®Œæ•´æ€§åˆ†æ•¸ï¼š{success_rate:.1%}\")\n",
    "    print(f\"é€šéæª¢æŸ¥ï¼š{sum(validation_results)}/{len(validation_results)}\")\n",
    "    \n",
    "    if success_rate >= 0.9:\n",
    "        print(\"ğŸ‰ ç³»çµ±é©—è­‰é€šéï¼å¯ä»¥é€²è¡Œéƒ¨ç½²\")\n",
    "        return True\n",
    "    elif success_rate >= 0.7:\n",
    "        print(\"âš ï¸ ç³»çµ±åŸºæœ¬å®Œæ•´ï¼Œå»ºè­°ä¿®å¾©éºå¤±é …ç›®å¾Œéƒ¨ç½²\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"âŒ ç³»çµ±ä¸å®Œæ•´ï¼Œè«‹æª¢æŸ¥éºå¤±çš„æª”æ¡ˆå’Œé…ç½®\")\n",
    "        return False\n",
    "\n",
    "# åŸ·è¡Œæœ€çµ‚é©—è­‰\n",
    "system_ready = final_system_validation()\n",
    "\n",
    "if system_ready:\n",
    "    print(\"\\nğŸš€ ç³»çµ±æº–å‚™å°±ç·’ï¼Œå¯ä»¥é–‹å§‹éƒ¨ç½²ï¼\")\n",
    "else:\n",
    "    print(\"\\nğŸ”§ è«‹å…ˆè§£æ±ºä¸Šè¿°å•é¡Œå¾Œå†é€²è¡Œéƒ¨ç½²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ èª²ç¨‹å®Œæˆç¸½çµ\n",
    "\n",
    "### ğŸ† å®Œæ•´å­¸ç¿’æˆæœ\n",
    "\n",
    "ğŸ“ **ç†è«–æŒæ¡**ï¼š\n",
    "- âœ… å¤šæ™ºèƒ½é«”ç³»çµ±æ ¸å¿ƒæ¦‚å¿µ\n",
    "- âœ… STORM æ–¹æ³•è«–æ·±åº¦ç†è§£\n",
    "- âœ… ç³»çµ±è¨­è¨ˆåŸå‰‡èˆ‡æœ€ä½³å¯¦è¸\n",
    "\n",
    "ğŸ› ï¸ **å¯¦ä½œèƒ½åŠ›**ï¼š\n",
    "- âœ… æ‰‹å‹•ç‰ˆå¤šæ™ºèƒ½é«”ç³»çµ±é–‹ç™¼\n",
    "- âœ… CrewAI æ¡†æ¶ç†Ÿç·´ä½¿ç”¨\n",
    "- âœ… ç”Ÿç”¢ç´šç³»çµ±æ¶æ§‹è¨­è¨ˆ\n",
    "\n",
    "ğŸš€ **å°ˆæ¡ˆç¶“é©—**ï¼š\n",
    "- âœ… LinkedIn å°ˆæ¥­æ–‡ç« ç”Ÿæˆå™¨\n",
    "- âœ… API æœå‹™é–‹ç™¼èˆ‡éƒ¨ç½²\n",
    "- âœ… Docker å®¹å™¨åŒ–éƒ¨ç½²\n",
    "\n",
    "### ğŸ“Š æŠ€è¡“æŠ€èƒ½æ¨¹\n",
    "\n",
    "**æ ¸å¿ƒæŠ€è¡“**ï¼š\n",
    "- Python é€²éšç¨‹å¼è¨­è¨ˆ â­â­â­â­â­\n",
    "- LangChain æ¡†æ¶æ‡‰ç”¨ â­â­â­â­â­\n",
    "- CrewAI å¤šæ™ºèƒ½é«”é–‹ç™¼ â­â­â­â­â­\n",
    "- OpenAI API æ•´åˆ â­â­â­â­â­\n",
    "\n",
    "**æ¶æ§‹è¨­è¨ˆ**ï¼š\n",
    "- å¾®æœå‹™æ¶æ§‹è¨­è¨ˆ â­â­â­â­\n",
    "- äº‹ä»¶é©…å‹•æ¶æ§‹ â­â­â­\n",
    "- API è¨­è¨ˆèˆ‡é–‹ç™¼ â­â­â­â­\n",
    "- ç³»çµ±æ•´åˆ â­â­â­â­\n",
    "\n",
    "**éƒ¨ç½²ç¶­é‹**ï¼š\n",
    "- Docker å®¹å™¨åŒ– â­â­â­â­\n",
    "- CI/CD æµç¨‹ â­â­â­\n",
    "- ç›£æ§èˆ‡æ—¥èªŒ â­â­â­\n",
    "- ç”Ÿç”¢ç’°å¢ƒç®¡ç† â­â­â­â­\n",
    "\n",
    "### ğŸ¯ å¯¦éš›æ‡‰ç”¨åƒ¹å€¼\n",
    "\n",
    "**å€‹äººèƒ½åŠ›æå‡**ï¼š\n",
    "- å…·å‚™è¨­è¨ˆå’Œå¯¦ä½œä¼æ¥­ç´š AI ç³»çµ±çš„èƒ½åŠ›\n",
    "- æŒæ¡ç¾ä»£å¤šæ™ºèƒ½é«”é–‹ç™¼çš„æœ€ä½³å¯¦è¸\n",
    "- ç†è§£å¾åŸå‹åˆ°ç”Ÿç”¢çš„å®Œæ•´é–‹ç™¼é€±æœŸ\n",
    "\n",
    "**è·æ¥­ç™¼å±•æ©Ÿæœƒ**ï¼š\n",
    "- AI ç³»çµ±æ¶æ§‹å¸«\n",
    "- å¤šæ™ºèƒ½é«”ç³»çµ±é–‹ç™¼å·¥ç¨‹å¸«\n",
    "- AI ç”¢å“æŠ€è¡“è² è²¬äºº\n",
    "- æ™ºèƒ½å…§å®¹å¹³å°é–‹ç™¼è€…\n",
    "\n",
    "**å•†æ¥­æ‡‰ç”¨æ½›åŠ›**ï¼š\n",
    "- å…§å®¹è¡ŒéŠ·è‡ªå‹•åŒ–å¹³å°\n",
    "- ä¼æ¥­çŸ¥è­˜ç®¡ç†ç³»çµ±\n",
    "- æ•™è‚²ç§‘æŠ€ç”¢å“é–‹ç™¼\n",
    "- å°ˆæ¥­æœå‹™æ™ºèƒ½åŒ–\n",
    "\n",
    "### ğŸš€ æœªä¾†ç™¼å±•æ–¹å‘\n",
    "\n",
    "**æŠ€è¡“æ¼”é€²**ï¼š\n",
    "- å¤šæ¨¡æ…‹æ™ºèƒ½é«”ï¼ˆæ–‡å­—+åœ–åƒ+èªéŸ³ï¼‰\n",
    "- æ›´å¤§è¦æ¨¡çš„æ™ºèƒ½é«”å”ä½œ\n",
    "- å¯¦æ™‚å”ä½œèˆ‡å³æ™‚åé¥‹\n",
    "- è‡ªé©æ‡‰å­¸ç¿’èˆ‡å„ªåŒ–\n",
    "\n",
    "**æ‡‰ç”¨æ“´å±•**ï¼š\n",
    "- è·¨èªè¨€å…§å®¹ç”Ÿæˆ\n",
    "- å‚ç›´é ˜åŸŸæ·±åº¦å®¢è£½åŒ–\n",
    "- ä¼æ¥­ç´šå·¥ä½œæµç¨‹æ•´åˆ\n",
    "- å€‹äººåŠ©ç†æ™ºèƒ½åŒ–\n",
    "\n",
    "### ğŸ’¡ æŒçºŒå­¸ç¿’å»ºè­°\n",
    "\n",
    "**æŠ€è¡“æ·±åŒ–**ï¼š\n",
    "- æ¢ç´¢å…¶ä»–å¤šæ™ºèƒ½é«”æ¡†æ¶ï¼ˆAutoGen, LangGraphï¼‰\n",
    "- å­¸ç¿’é€²éš prompt engineering æŠ€è¡“\n",
    "- ç ”ç©¶ RAG å’Œå‘é‡è³‡æ–™åº«å„ªåŒ–\n",
    "- æŒæ¡ LLM å¾®èª¿èˆ‡å®¢è£½åŒ–\n",
    "\n",
    "**å¯¦å‹™æ‡‰ç”¨**ï¼š\n",
    "- åƒèˆ‡é–‹æºå°ˆæ¡ˆè²¢ç»\n",
    "- å»ºç«‹å€‹äºº AI å·¥å…·é›†\n",
    "- é–‹ç™¼å‚ç›´é ˜åŸŸæ‡‰ç”¨\n",
    "- åˆ†äº«ç¶“é©—èˆ‡æœ€ä½³å¯¦è¸\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŠ æ­å–œå®Œæˆæ•´å€‹èª²ç¨‹ï¼\n",
    "\n",
    "**æ‚¨å·²ç¶“æˆåŠŸæŒæ¡äº†å¤šæ™ºèƒ½é«”é•·æ–‡å¯«ä½œç³»çµ±çš„å®Œæ•´æŠ€è¡“æ£§ï¼**\n",
    "\n",
    "å¾åŸºç¤æ¦‚å¿µåˆ°æ‰‹å‹•å¯¦ä½œï¼Œå†åˆ°æ¡†æ¶æ‡‰ç”¨å’Œç”Ÿç”¢éƒ¨ç½²ï¼Œæ‚¨å·²ç¶“å…·å‚™äº†:\n",
    "\n",
    "âœ¨ **ç†è«–æ·±åº¦**ï¼šæ·±å…¥ç†è§£å¤šæ™ºèƒ½é«”å”ä½œåŸç†  \n",
    "ğŸ› ï¸ **å¯¦ä½œèƒ½åŠ›**ï¼šèƒ½å¤ å¾é›¶é–‹å§‹å»ºæ§‹å®Œæ•´ç³»çµ±  \n",
    "âš¡ **æ¡†æ¶ç†Ÿç·´**ï¼šç†Ÿç·´ä½¿ç”¨ç¾ä»£é–‹ç™¼æ¡†æ¶  \n",
    "ğŸš€ **éƒ¨ç½²ç¶“é©—**ï¼šå…·å‚™ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²èƒ½åŠ›  \n",
    "ğŸ¯ **å•†æ¥­è¦–é‡**ï¼šç†è§£æŠ€è¡“çš„å•†æ¥­æ‡‰ç”¨åƒ¹å€¼  \n",
    "\n",
    "**ç¾åœ¨ï¼Œæ‚¨å·²ç¶“æº–å‚™å¥½å°‡é€™äº›æŠ€èƒ½æ‡‰ç”¨åˆ°å¯¦éš›å°ˆæ¡ˆä¸­ï¼Œå‰µé€ çœŸæ­£çš„åƒ¹å€¼ï¼**\n",
    "\n",
    "### ğŸ“ å¾ŒçºŒæ”¯æ´\n",
    "\n",
    "å¦‚æœæ‚¨åœ¨å¯¦éš›æ‡‰ç”¨ä¸­é‡åˆ°å•é¡Œï¼Œæ­¡è¿ï¼š\n",
    "- ğŸ“§ è¯ç¹«è¬›å¸«ç²å¾—æŠ€è¡“æ”¯æ´\n",
    "- ğŸ’¬ åŠ å…¥é–‹ç™¼è€…ç¤¾ç¾¤è¨è«–\n",
    "- ğŸ“š æŸ¥é–±èª²ç¨‹è³‡æ–™èˆ‡ç¯„ä¾‹ä»£ç¢¼\n",
    "- ğŸ”— é—œæ³¨æŠ€è¡“æ›´æ–°èˆ‡æœ€æ–°ç™¼å±•\n",
    "\n",
    "**ç¥æ‚¨åœ¨ AI é–‹ç™¼çš„é“è·¯ä¸ŠæŒçºŒç²¾é€²ï¼Œå‰µé€ æ›´å¤šå‰µæ–°æ‡‰ç”¨ï¼** ğŸŒŸ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}