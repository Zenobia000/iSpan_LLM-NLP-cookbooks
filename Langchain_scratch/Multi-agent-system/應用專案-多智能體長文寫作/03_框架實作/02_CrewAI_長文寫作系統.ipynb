{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_CrewAI_é•·æ–‡å¯«ä½œç³»çµ±\n",
    "\n",
    "## ğŸ¯ å­¸ç¿’ç›®æ¨™\n",
    "- ä½¿ç”¨ CrewAI é‡æ§‹ STORM å¯«ä½œç³»çµ±\n",
    "- å¯¦ç¾é€²éšçš„æ™ºèƒ½é«”å”ä½œæ©Ÿåˆ¶\n",
    "- æ¢ç´¢ CrewAI çš„é€²éšåŠŸèƒ½ç‰¹æ€§\n",
    "- æ¯”è¼ƒæ¡†æ¶ç‰ˆæœ¬èˆ‡æ‰‹å‹•ç‰ˆæœ¬çš„æ€§èƒ½å·®ç•°\n",
    "- å®Œæˆç”Ÿç”¢ç´šçš„é•·æ–‡å¯«ä½œç³»çµ±\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒæº–å‚™èˆ‡å¥—ä»¶è¼‰å…¥\n",
    "\n",
    "### è¼‰å…¥ CrewAI å’Œç›¸é—œå¥—ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¤å¥—ä»¶\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# CrewAI æ ¸å¿ƒ\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import tool, SerperDevTool, WebsiteSearchTool\n",
    "\n",
    "# LangChain æ•´åˆ\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# ç¶²è·¯å·¥å…·ï¼ˆå‚™ç”¨ï¼‰\n",
    "from duckduckgo_search import DDGS\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ç’°å¢ƒè®Šæ•¸\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ– LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"âœ… CrewAI ç’°å¢ƒæº–å‚™å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. è‡ªå®šç¾©å·¥å…·é–‹ç™¼\n",
    "\n",
    "### å»ºç«‹å°ˆé–€çš„ç ”ç©¶å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"multi_perspective_research\")\n",
    "def multi_perspective_research(topic: str) -> str:\n",
    "    \"\"\"\n",
    "    Conduct research from multiple expert perspectives on a given topic.\n",
    "    This tool generates research questions from different expert viewpoints and searches for relevant information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # å®šç¾©å°ˆå®¶è§’è‰²\n",
    "        expert_perspectives = {\n",
    "            \"æŠ€è¡“å°ˆå®¶\": \"å¾æŠ€è¡“å¯¦ç¾ã€ç³»çµ±æ¶æ§‹ã€æ•ˆèƒ½å„ªåŒ–è§’åº¦åˆ†æ\",\n",
    "            \"å•†æ¥­åˆ†æå¸«\": \"å¾å¸‚å ´éœ€æ±‚ã€å•†æ¥­åƒ¹å€¼ã€æŠ•è³‡å›å ±è§’åº¦åˆ†æ\",\n",
    "            \"ç”¨æˆ¶ç ”ç©¶å“¡\": \"å¾ä½¿ç”¨è€…é«”é©—ã€éœ€æ±‚æ»¿è¶³ã€æ˜“ç”¨æ€§è§’åº¦åˆ†æ\",\n",
    "            \"å­¸è¡“ç ”ç©¶è€…\": \"å¾ç†è«–åŸºç¤ã€ç ”ç©¶æ–¹æ³•ã€å­¸è¡“åƒ¹å€¼è§’åº¦åˆ†æ\"\n",
    "        }\n",
    "        \n",
    "        research_results = []\n",
    "        \n",
    "        # å¾æ¯å€‹å°ˆå®¶è§’åº¦é€²è¡Œç ”ç©¶\n",
    "        for expert, perspective in expert_perspectives.items():\n",
    "            # ç”Ÿæˆæœå°‹æŸ¥è©¢\n",
    "            search_query = f\"{topic} {perspective.split('è§’åº¦')[0]}\"\n",
    "            \n",
    "            # æœå°‹è³‡æ–™\n",
    "            ddgs = DDGS()\n",
    "            results = list(ddgs.text(search_query, max_results=2))\n",
    "            \n",
    "            if results:\n",
    "                research_results.append({\n",
    "                    'expert': expert,\n",
    "                    'perspective': perspective,\n",
    "                    'findings': [\n",
    "                        f\"æ¨™é¡Œ: {r['title']}\\nå…§å®¹: {r.get('body', '')[:300]}...\" \n",
    "                        for r in results\n",
    "                    ]\n",
    "                })\n",
    "        \n",
    "        # çµ„åˆç ”ç©¶å ±å‘Š\n",
    "        report = f\"# {topic} - å¤šè§’åº¦ç ”ç©¶å ±å‘Š\\n\\n\"\n",
    "        \n",
    "        for research in research_results:\n",
    "            report += f\"## {research['expert']}è§€é»\\n\"\n",
    "            report += f\"**åˆ†æè§’åº¦**: {research['perspective']}\\n\\n\"\n",
    "            \n",
    "            for i, finding in enumerate(research['findings'], 1):\n",
    "                report += f\"**ç™¼ç¾ {i}:**\\n{finding}\\n\\n\"\n",
    "            \n",
    "            report += \"---\\n\\n\"\n",
    "        \n",
    "        return report\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Research failed: {str(e)}\"\n",
    "\n",
    "@tool(\"content_quality_analyzer\")\n",
    "def content_quality_analyzer(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze the quality of written content and provide improvement suggestions.\n",
    "    This tool evaluates readability, structure, coherence, and provides specific feedback.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # åŸºç¤çµ±è¨ˆ\n",
    "        word_count = len(content.split())\n",
    "        char_count = len(content)\n",
    "        sentences = content.split('ã€‚')\n",
    "        paragraphs = [p for p in content.split('\\n\\n') if p.strip()]\n",
    "        \n",
    "        # è¨ˆç®—å¯è®€æ€§æŒ‡æ¨™\n",
    "        avg_sentence_length = word_count / max(len(sentences), 1)\n",
    "        avg_paragraph_length = word_count / max(len(paragraphs), 1)\n",
    "        \n",
    "        # çµæ§‹åˆ†æ\n",
    "        header_count = content.count('#')\n",
    "        bullet_points = content.count('â€¢') + content.count('-')\n",
    "        \n",
    "        # ç”Ÿæˆåˆ†æå ±å‘Š\n",
    "        analysis = f\"\"\"\n",
    "# å…§å®¹å“è³ªåˆ†æå ±å‘Š\n",
    "\n",
    "## åŸºç¤çµ±è¨ˆ\n",
    "- ç¸½å­—æ•¸: {word_count} å­—\n",
    "- ç¸½å­—å…ƒæ•¸: {char_count} å­—å…ƒ\n",
    "- æ®µè½æ•¸: {len(paragraphs)}\n",
    "- å¥å­æ•¸: {len(sentences)}\n",
    "\n",
    "## å¯è®€æ€§æŒ‡æ¨™\n",
    "- å¹³å‡å¥å­é•·åº¦: {avg_sentence_length:.1f} å­—/å¥\n",
    "- å¹³å‡æ®µè½é•·åº¦: {avg_paragraph_length:.1f} å­—/æ®µ\n",
    "- çµæ§‹æ¨™é¡Œæ•¸: {header_count}\n",
    "- æ¢åˆ—é …ç›®æ•¸: {bullet_points}\n",
    "\n",
    "## å“è³ªè©•ä¼°\n",
    "\"\"\"\n",
    "        \n",
    "        # å“è³ªè©•åˆ†\n",
    "        scores = []\n",
    "        suggestions = []\n",
    "        \n",
    "        # å¥å­é•·åº¦è©•ä¼°\n",
    "        if 10 <= avg_sentence_length <= 25:\n",
    "            scores.append(0.9)\n",
    "            analysis += \"- âœ… å¥å­é•·åº¦é©ä¸­ï¼Œæ˜“æ–¼é–±è®€\\n\"\n",
    "        else:\n",
    "            scores.append(0.6)\n",
    "            analysis += \"- âš ï¸ å¥å­é•·åº¦éœ€è¦èª¿æ•´\\n\"\n",
    "            suggestions.append(\"èª¿æ•´å¥å­é•·åº¦ï¼Œå»ºè­°æ§åˆ¶åœ¨10-25å­—ä¹‹é–“\")\n",
    "        \n",
    "        # çµæ§‹è©•ä¼°\n",
    "        if header_count >= 3:\n",
    "            scores.append(0.9)\n",
    "            analysis += \"- âœ… æ–‡ç« çµæ§‹æ¸…æ™°\\n\"\n",
    "        else:\n",
    "            scores.append(0.6)\n",
    "            analysis += \"- âš ï¸ å»ºè­°å¢åŠ æ›´å¤šçµæ§‹æ¨™é¡Œ\\n\"\n",
    "            suggestions.append(\"å¢åŠ ç« ç¯€æ¨™é¡Œï¼Œæå‡æ–‡ç« çµæ§‹æ¸…æ™°åº¦\")\n",
    "        \n",
    "        # é•·åº¦è©•ä¼°\n",
    "        if word_count >= 800:\n",
    "            scores.append(0.9)\n",
    "            analysis += \"- âœ… æ–‡ç« é•·åº¦å……è¶³\\n\"\n",
    "        else:\n",
    "            scores.append(0.7)\n",
    "            analysis += \"- âš ï¸ æ–‡ç« é•·åº¦å¯ä»¥æ›´å……å¯¦\\n\"\n",
    "            suggestions.append(\"å¢åŠ å…§å®¹æ·±åº¦ï¼Œæä¾›æ›´å¤šç´°ç¯€å’Œä¾‹å­\")\n",
    "        \n",
    "        overall_score = sum(scores) / len(scores)\n",
    "        \n",
    "        analysis += f\"\\n## ç¸½é«”å“è³ªåˆ†æ•¸: {overall_score:.2f}/1.0\\n\\n\"\n",
    "        \n",
    "        if suggestions:\n",
    "            analysis += \"## æ”¹é€²å»ºè­°\\n\"\n",
    "            for i, suggestion in enumerate(suggestions, 1):\n",
    "                analysis += f\"{i}. {suggestion}\\n\"\n",
    "        else:\n",
    "            analysis += \"## è©•ä¼°çµæœ\\nâœ… å…§å®¹å“è³ªè‰¯å¥½ï¼Œç„¡éœ€ç‰¹åˆ¥æ”¹é€²\\n\"\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Quality analysis failed: {str(e)}\"\n",
    "\n",
    "print(\"âœ… è‡ªå®šç¾©å·¥å…·å·²å»ºç«‹ï¼š\")\n",
    "print(\"   ğŸ” multi_perspective_research - å¤šè§’åº¦ç ”ç©¶å·¥å…·\")\n",
    "print(\"   ğŸ“Š content_quality_analyzer - å…§å®¹å“è³ªåˆ†æå·¥å…·\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. CrewAI ç‰ˆæœ¬çš„ STORM æ™ºèƒ½é«”\n",
    "\n",
    "### å®šç¾©å°ˆæ¥­çš„å¯«ä½œåœ˜éšŠæ™ºèƒ½é«”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç ”ç©¶å°ˆå®¶æ™ºèƒ½é«”\n",
    "storm_research_agent = Agent(\n",
    "    role='STORM Research Expert',\n",
    "    goal='Conduct comprehensive multi-perspective research following STORM methodology',\n",
    "    backstory=\"\"\"\n",
    "    You are a senior research expert specializing in the STORM (Synthesis of Topic Outline through \n",
    "    Retrieval and Multi-perspective question asking) methodology. You excel at:\n",
    "    \n",
    "    - Analyzing topics from multiple expert perspectives (technical, business, UX, academic, policy)\n",
    "    - Generating insightful research questions that uncover deep aspects of any topic\n",
    "    - Collecting and synthesizing information from diverse sources\n",
    "    - Creating comprehensive research reports that serve as solid foundations for long-form writing\n",
    "    \n",
    "    Your research is thorough, objective, and always considers multiple viewpoints to ensure \n",
    "    comprehensive coverage of complex topics.\n",
    "    \"\"\",\n",
    "    tools=[multi_perspective_research],\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# æ¶æ§‹è¦åŠƒæ™ºèƒ½é«”\n",
    "storm_planner_agent = Agent(\n",
    "    role='Content Architect',\n",
    "    goal='Design optimal article structures and outlines based on research findings',\n",
    "    backstory=\"\"\"\n",
    "    You are an expert content architect who specializes in transforming research findings into \n",
    "    well-structured article outlines. Your expertise includes:\n",
    "    \n",
    "    - Analyzing research reports and identifying key themes and patterns\n",
    "    - Designing logical article structures that flow naturally\n",
    "    - Balancing content depth with readability requirements\n",
    "    - Creating detailed outlines that guide effective content creation\n",
    "    \n",
    "    You understand different article formats (academic, blog, report, tutorial) and can \n",
    "    adapt your planning approach accordingly.\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# å…§å®¹å‰µä½œæ™ºèƒ½é«”\n",
    "storm_writer_agent = Agent(\n",
    "    role='Long-form Content Creator',\n",
    "    goal='Write comprehensive, engaging, and well-researched long-form articles',\n",
    "    backstory=\"\"\"\n",
    "    You are a professional long-form content creator with exceptional writing skills. You excel at:\n",
    "    \n",
    "    - Transforming structured outlines into engaging, comprehensive articles\n",
    "    - Maintaining consistent tone and style throughout long-form content\n",
    "    - Integrating research findings seamlessly into narrative flow\n",
    "    - Creating content that is both informative and engaging for the target audience\n",
    "    \n",
    "    Your writing is clear, authoritative, and backed by solid research. You understand how to \n",
    "    make complex topics accessible while maintaining professional standards.\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# å“è³ªæ§åˆ¶æ™ºèƒ½é«”\n",
    "storm_editor_agent = Agent(\n",
    "    role='Quality Assurance Editor',\n",
    "    goal='Ensure highest quality standards and optimize content for publication',\n",
    "    backstory=\"\"\"\n",
    "    You are a senior quality assurance editor with expertise in content optimization. You excel at:\n",
    "    \n",
    "    - Conducting thorough quality checks on long-form content\n",
    "    - Identifying and correcting issues with grammar, style, and structure\n",
    "    - Enhancing readability and engagement while maintaining accuracy\n",
    "    - Ensuring content meets professional publication standards\n",
    "    \n",
    "    Your edits always improve the overall quality and impact of the content while preserving \n",
    "    the author's voice and intent.\n",
    "    \"\"\",\n",
    "    tools=[content_quality_analyzer],\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "print(\"âœ… STORM å°ˆæ¥­æ™ºèƒ½é«”åœ˜éšŠå·²å»ºç«‹ï¼š\")\n",
    "print(\"   ğŸ”¬ STORM Research Expert - STORM ç ”ç©¶å°ˆå®¶\")\n",
    "print(\"   ğŸ—ï¸ Content Architect - å…§å®¹æ¶æ§‹å¸«\")\n",
    "print(\"   âœï¸ Long-form Content Creator - é•·æ–‡å…§å®¹å‰µä½œè€…\")\n",
    "print(\"   ğŸ” Quality Assurance Editor - å“è³ªä¿è­‰ç·¨è¼¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. CrewAI ç‰ˆæœ¬çš„ STORM ä»»å‹™è¨­è¨ˆ\n",
    "\n",
    "### å®šç¾©å®Œæ•´çš„ STORM ä»»å‹™æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_storm_research_task(topic: str) -> Task:\n",
    "    \"\"\"å»ºç«‹ STORM ç ”ç©¶ä»»å‹™\"\"\"\n",
    "    return Task(\n",
    "        description=f\"\"\"\n",
    "Execute the STORM research methodology for the topic: \"{topic}\"\n",
    "\n",
    "Follow the STORM knowledge exploration process:\n",
    "\n",
    "1. **Multi-Expert Perspective Analysis**:\n",
    "   - Adopt different expert roles (technical expert, business analyst, UX researcher, academic researcher, policy maker)\n",
    "   - Generate 1-2 insightful research questions from each expert perspective\n",
    "   - Ensure questions cover different aspects: technical feasibility, business value, user impact, theoretical foundation, policy implications\n",
    "\n",
    "2. **Comprehensive Information Gathering**:\n",
    "   - Use the multi_perspective_research tool to gather current information\n",
    "   - Focus on recent developments, trends, and evidence-based insights\n",
    "   - Collect diverse viewpoints and opinions\n",
    "\n",
    "3. **Research Synthesis**:\n",
    "   - Synthesize findings from all perspectives\n",
    "   - Identify key themes, patterns, and insights\n",
    "   - Note any conflicting viewpoints or debates\n",
    "   - Assess the credibility and relevance of sources\n",
    "\n",
    "Provide a comprehensive research report that will serve as the foundation for long-form article creation.\n",
    "\n",
    "Topic: {topic}\n",
    "\"\"\",\n",
    "        expected_output=\"\"\"\n",
    "A comprehensive STORM research report containing:\n",
    "- Executive summary (3-4 sentences)\n",
    "- Research questions from each expert perspective\n",
    "- Key findings organized by theme\n",
    "- Multi-perspective insights and analysis\n",
    "- Current trends and future implications\n",
    "- Source credibility assessment\n",
    "- Recommendations for article structure\n",
    "\"\"\",\n",
    "        agent=storm_research_agent\n",
    "    )\n",
    "\n",
    "def create_storm_planning_task() -> Task:\n",
    "    \"\"\"å»ºç«‹ STORM è¦åŠƒä»»å‹™\"\"\"\n",
    "    return Task(\n",
    "        description=\"\"\"\n",
    "Based on the comprehensive research report, design an optimal article structure following STORM principles.\n",
    "\n",
    "Your planning tasks:\n",
    "\n",
    "1. **Content Analysis**:\n",
    "   - Analyze the research report to identify main themes and sub-topics\n",
    "   - Prioritize information based on importance and relevance\n",
    "   - Identify logical relationships between different concepts\n",
    "\n",
    "2. **Structure Design**:\n",
    "   - Create a detailed article outline with 3-4 main sections\n",
    "   - Design logical progression from introduction to conclusion\n",
    "   - Ensure balanced coverage of all important aspects\n",
    "   - Plan appropriate depth for each section\n",
    "\n",
    "3. **Content Specifications**:\n",
    "   - Specify target word count for each section (aim for 1800-2200 total words)\n",
    "   - Define key points to cover in each section\n",
    "   - Suggest appropriate examples and evidence to include\n",
    "   - Plan transitions between sections\n",
    "\n",
    "Create a detailed content plan that will guide the writing process effectively.\n",
    "\"\"\",\n",
    "        expected_output=\"\"\"\n",
    "A detailed article outline and content plan containing:\n",
    "- Proposed article title\n",
    "- Detailed section-by-section outline\n",
    "- Word count allocation for each section\n",
    "- Key points and evidence for each section\n",
    "- Suggested examples and case studies\n",
    "- Transition strategies between sections\n",
    "\"\"\",\n",
    "        agent=storm_planner_agent\n",
    "    )\n",
    "\n",
    "def create_storm_writing_task() -> Task:\n",
    "    \"\"\"å»ºç«‹ STORM å¯«ä½œä»»å‹™\"\"\"\n",
    "    return Task(\n",
    "        description=\"\"\"\n",
    "Write a comprehensive long-form article based on the research report and content plan.\n",
    "\n",
    "Your writing tasks:\n",
    "\n",
    "1. **Article Creation**:\n",
    "   - Write an engaging title and compelling introduction\n",
    "   - Develop each section according to the content plan\n",
    "   - Integrate research findings naturally into the narrative\n",
    "   - Maintain consistent professional tone throughout\n",
    "\n",
    "2. **Content Integration**:\n",
    "   - Use insights from multiple expert perspectives\n",
    "   - Include relevant examples and case studies\n",
    "   - Support arguments with research evidence\n",
    "   - Ensure logical flow between sections\n",
    "\n",
    "3. **Professional Standards**:\n",
    "   - Follow academic writing conventions\n",
    "   - Use clear, professional language\n",
    "   - Structure content with appropriate headers\n",
    "   - Aim for 1800-2200 words total length\n",
    "\n",
    "Create a complete, publication-ready long-form article.\n",
    "\"\"\",\n",
    "        expected_output=\"\"\"\n",
    "A complete long-form article (1800-2200 words) featuring:\n",
    "- Engaging title and introduction\n",
    "- Well-structured main body with 3-4 sections\n",
    "- Professional academic tone\n",
    "- Integration of multi-perspective research\n",
    "- Clear conclusions and insights\n",
    "- Proper formatting with headers and structure\n",
    "\"\"\",\n",
    "        agent=storm_writer_agent\n",
    "    )\n",
    "\n",
    "def create_storm_editing_task() -> Task:\n",
    "    \"\"\"å»ºç«‹ STORM ç·¨è¼¯ä»»å‹™\"\"\"\n",
    "    return Task(\n",
    "        description=\"\"\"\n",
    "Perform comprehensive quality assurance and editing on the long-form article.\n",
    "\n",
    "Your editing responsibilities:\n",
    "\n",
    "1. **Quality Analysis**:\n",
    "   - Use the content_quality_analyzer tool to assess current quality\n",
    "   - Identify specific areas for improvement\n",
    "   - Check alignment with STORM methodology principles\n",
    "\n",
    "2. **Content Enhancement**:\n",
    "   - Improve clarity and readability\n",
    "   - Enhance logical flow and coherence\n",
    "   - Strengthen arguments with better evidence integration\n",
    "   - Ensure balanced coverage of all perspectives\n",
    "\n",
    "3. **Professional Polish**:\n",
    "   - Correct grammar, spelling, and punctuation\n",
    "   - Improve sentence structure and word choice\n",
    "   - Ensure consistent style and tone\n",
    "   - Optimize formatting and structure\n",
    "\n",
    "4. **Final Quality Check**:\n",
    "   - Verify factual accuracy\n",
    "   - Check completeness against original research\n",
    "   - Ensure publication readiness\n",
    "   - Provide quality assessment summary\n",
    "\n",
    "Deliver a polished, publication-ready article that exemplifies STORM methodology excellence.\n",
    "\"\"\",\n",
    "        expected_output=\"\"\"\n",
    "Final polished article with:\n",
    "- Improved content quality and readability\n",
    "- Consistent professional style\n",
    "- Proper formatting and structure\n",
    "- Quality assessment report\n",
    "- Summary of improvements made\n",
    "- Publication readiness confirmation\n",
    "\"\"\",\n",
    "        agent=storm_editor_agent\n",
    "    )\n",
    "\n",
    "print(\"âœ… STORM ä»»å‹™æµç¨‹å·²å®šç¾©ï¼š\")\n",
    "print(\"   1ï¸âƒ£ Research Task - å¤šè§’åº¦ç ”ç©¶\")\n",
    "print(\"   2ï¸âƒ£ Planning Task - æ¶æ§‹è¦åŠƒ\")\n",
    "print(\"   3ï¸âƒ£ Writing Task - å…§å®¹å‰µä½œ\")\n",
    "print(\"   4ï¸âƒ£ Editing Task - å“è³ªå„ªåŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. å»ºç«‹å®Œæ•´çš„ STORM CrewAI ç³»çµ±\n",
    "\n",
    "### æ•´åˆå®Œæ•´çš„ STORM å¯«ä½œåœ˜éšŠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STORMCrewAISystem:\n",
    "    \"\"\"å®Œæ•´çš„ CrewAI ç‰ˆæœ¬ STORM ç³»çµ±\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = llm\n",
    "        \n",
    "        # å»ºç«‹æ™ºèƒ½é«”åœ˜éšŠ\n",
    "        self.agents = {\n",
    "            'researcher': storm_research_agent,\n",
    "            'planner': storm_planner_agent,\n",
    "            'writer': storm_writer_agent,\n",
    "            'editor': storm_editor_agent\n",
    "        }\n",
    "        \n",
    "        print(\"ğŸŒŸ STORM CrewAI ç³»çµ±åˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    def generate_article(self, topic: str, config: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"ç”Ÿæˆå®Œæ•´æ–‡ç« \"\"\"\n",
    "        \n",
    "        print(f\"ğŸš€ å•Ÿå‹• CrewAI STORM å¯«ä½œæµç¨‹\")\n",
    "        print(f\"ğŸ“ ä¸»é¡Œï¼š{topic}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # é è¨­é…ç½®\n",
    "        if config is None:\n",
    "            config = {\n",
    "                'target_length': 2000,\n",
    "                'article_type': 'academic',\n",
    "                'quality_threshold': 0.8\n",
    "            }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # å»ºç«‹ä»»å‹™\n",
    "            research_task = create_storm_research_task(topic)\n",
    "            planning_task = create_storm_planning_task()\n",
    "            writing_task = create_storm_writing_task()\n",
    "            editing_task = create_storm_editing_task()\n",
    "            \n",
    "            # å»ºç«‹ STORM å¯«ä½œåœ˜éšŠ\n",
    "            storm_crew = Crew(\n",
    "                agents=[\n",
    "                    self.agents['researcher'],\n",
    "                    self.agents['planner'],\n",
    "                    self.agents['writer'],\n",
    "                    self.agents['editor']\n",
    "                ],\n",
    "                tasks=[research_task, planning_task, writing_task, editing_task],\n",
    "                process=Process.sequential,  # æŒ‰é †åºåŸ·è¡Œ\n",
    "                verbose=2\n",
    "            )\n",
    "            \n",
    "            print(\"\\nğŸ¬ é–‹å§‹åŸ·è¡Œ STORM æµç¨‹...\")\n",
    "            \n",
    "            # åŸ·è¡Œå®Œæ•´æµç¨‹\n",
    "            crew_result = storm_crew.kickoff()\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # è™•ç†çµæœ\n",
    "            final_result = self._process_crew_result(crew_result, topic, config, processing_time)\n",
    "            \n",
    "            print(f\"\\nğŸ‰ CrewAI STORM æµç¨‹å®Œæˆï¼\")\n",
    "            print(f\"   â±ï¸ ç¸½è€—æ™‚ï¼š{processing_time:.1f} ç§’\")\n",
    "            print(f\"   ğŸ“ æ–‡ç« å­—æ•¸ï¼š{final_result.get('word_count', 0)} å­—\")\n",
    "            print(f\"   â­ å“è³ªåˆ†æ•¸ï¼š{final_result.get('quality_score', 0):.2f}/1.0\")\n",
    "            \n",
    "            return final_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_time = time.time() - start_time\n",
    "            print(f\"âŒ STORM æµç¨‹åŸ·è¡Œå¤±æ•—ï¼š{str(e)}\")\n",
    "            return self._create_error_result(topic, str(e), error_time)\n",
    "    \n",
    "    def _process_crew_result(self, crew_result: str, topic: str, \n",
    "                           config: Dict, processing_time: float) -> Dict[str, Any]:\n",
    "        \"\"\"è™•ç† CrewAI åŸ·è¡Œçµæœ\"\"\"\n",
    "        \n",
    "        # åˆ†æçµæœå…§å®¹\n",
    "        content = str(crew_result)\n",
    "        word_count = len(content.split())\n",
    "        \n",
    "        # ç°¡å–®çš„å“è³ªè©•ä¼°\n",
    "        quality_score = 0.8  # åŸºç¤åˆ†æ•¸\n",
    "        \n",
    "        # æ ¹æ“šå­—æ•¸èª¿æ•´åˆ†æ•¸\n",
    "        target_length = config.get('target_length', 2000)\n",
    "        length_ratio = word_count / target_length\n",
    "        \n",
    "        if 0.8 <= length_ratio <= 1.2:\n",
    "            quality_score += 0.1\n",
    "        \n",
    "        # æ ¹æ“šçµæ§‹èª¿æ•´åˆ†æ•¸\n",
    "        if content.count('#') >= 4:  # æœ‰è¶³å¤ çš„æ¨™é¡Œçµæ§‹\n",
    "            quality_score += 0.1\n",
    "        \n",
    "        quality_score = min(quality_score, 1.0)\n",
    "        \n",
    "        return {\n",
    "            'topic': topic,\n",
    "            'content': content,\n",
    "            'word_count': word_count,\n",
    "            'quality_score': quality_score,\n",
    "            'processing_time': processing_time,\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'method': 'CrewAI_STORM',\n",
    "            'config': config,\n",
    "            'metadata': {\n",
    "                'target_length': target_length,\n",
    "                'actual_length': word_count,\n",
    "                'completion_ratio': length_ratio,\n",
    "                'structure_score': content.count('#'),\n",
    "                'agent_count': 4,\n",
    "                'task_count': 4\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _create_error_result(self, topic: str, error_msg: str, processing_time: float) -> Dict[str, Any]:\n",
    "        \"\"\"å»ºç«‹éŒ¯èª¤çµæœ\"\"\"\n",
    "        return {\n",
    "            'topic': topic,\n",
    "            'content': f\"# {topic}\\n\\næ–‡ç« ç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{error_msg}\",\n",
    "            'word_count': 0,\n",
    "            'quality_score': 0.0,\n",
    "            'processing_time': processing_time,\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'method': 'CrewAI_STORM',\n",
    "            'error': error_msg,\n",
    "            'success': False\n",
    "        }\n",
    "\n",
    "# å»ºç«‹å®Œæ•´çš„ CrewAI STORM ç³»çµ±\n",
    "crewai_storm_system = STORMCrewAISystem()\n",
    "print(\"\\nğŸŒŸ å®Œæ•´çš„ CrewAI STORM ç³»çµ±å·²æº–å‚™å°±ç·’ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. å®Œæ•´ç³»çµ±æ¸¬è©¦\n",
    "\n",
    "### åŸ·è¡Œ CrewAI ç‰ˆæœ¬çš„ STORM æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦ä¸»é¡Œ\n",
    "crewai_test_topic = \"å…ƒå®‡å®™æŠ€è¡“å°æœªä¾†æ•™è‚²æ¨¡å¼çš„è®Šé©\"\n",
    "\n",
    "# æ¸¬è©¦é…ç½®\n",
    "crewai_config = {\n",
    "    'target_length': 1800,\n",
    "    'article_type': 'academic',\n",
    "    'quality_threshold': 0.8\n",
    "}\n",
    "\n",
    "print(f\"ğŸ¯ CrewAI æ¸¬è©¦ä¸»é¡Œï¼š{crewai_test_topic}\")\n",
    "print(f\"ğŸ“Š é…ç½®åƒæ•¸ï¼š{crewai_config}\")\n",
    "print(\"\\nâš ï¸  æ³¨æ„ï¼šé€™å°‡èŠ±è²»è¼ƒé•·æ™‚é–“å’Œ API è²»ç”¨\")\n",
    "print(\"ğŸš€ é–‹å§‹åŸ·è¡Œ CrewAI STORM æµç¨‹...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸ·è¡Œ CrewAI ç‰ˆæœ¬çš„ STORM æµç¨‹\n",
    "crewai_result = crewai_storm_system.generate_article(\n",
    "    topic=crewai_test_topic,\n",
    "    config=crewai_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ†æ CrewAI çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¡¯ç¤º CrewAI çµæœåˆ†æ\n",
    "if 'error' not in crewai_result:\n",
    "    print(\"ğŸ“Š CrewAI STORM çµæœåˆ†æï¼š\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"ğŸ“ æ–‡ç« ä¸»é¡Œï¼š{crewai_result['topic']}\")\n",
    "    print(f\"ğŸ“ æ–‡ç« å­—æ•¸ï¼š{crewai_result['word_count']} å­—\")\n",
    "    print(f\"ğŸ¯ ç›®æ¨™å­—æ•¸ï¼š{crewai_result['metadata']['target_length']} å­—\")\n",
    "    print(f\"ğŸ“ˆ å®Œæˆç‡ï¼š{crewai_result['metadata']['completion_ratio']:.1%}\")\n",
    "    print(f\"â­ å“è³ªåˆ†æ•¸ï¼š{crewai_result['quality_score']:.2f}/1.0\")\n",
    "    print(f\"â±ï¸ è™•ç†æ™‚é–“ï¼š{crewai_result['processing_time']:.1f} ç§’\")\n",
    "    print(f\"ğŸ—ï¸ çµæ§‹åˆ†æ•¸ï¼š{crewai_result['metadata']['structure_score']} å€‹æ¨™é¡Œ\")\n",
    "    \n",
    "    # æ•ˆç‡åˆ†æ\n",
    "    if crewai_result['processing_time'] > 0:\n",
    "        efficiency = crewai_result['word_count'] / crewai_result['processing_time']\n",
    "        print(f\"âš¡ ç”Ÿæˆæ•ˆç‡ï¼š{efficiency:.1f} å­—/ç§’\")\n",
    "    \n",
    "    # å…§å®¹é è¦½\n",
    "    print(\"\\nğŸ“– CrewAI ç”Ÿæˆå…§å®¹é è¦½ï¼ˆå‰400å­—ï¼‰ï¼š\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    words = crewai_result['content'].split()\n",
    "    preview_words = words[:400] if len(words) > 400 else words\n",
    "    preview = ' '.join(preview_words)\n",
    "    \n",
    "    print(preview)\n",
    "    if len(words) > 400:\n",
    "        print(\"\\n... [å…§å®¹ç¹¼çºŒ] ...\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ CrewAI åŸ·è¡Œå¤±æ•—ï¼š{crewai_result.get('error', 'æœªçŸ¥éŒ¯èª¤')}\")\n",
    "    print(f\"â±ï¸ å¤±æ•—å‰è€—æ™‚ï¼š{crewai_result.get('processing_time', 0):.1f} ç§’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. æ€§èƒ½æ¯”è¼ƒåˆ†æ\n",
    "\n",
    "### CrewAI vs æ‰‹å‹•å¯¦ä½œçš„è©³ç´°æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ“¬æ‰‹å‹•ç‰ˆæœ¬çš„çµæœæ•¸æ“šï¼ˆç”¨æ–¼æ¯”è¼ƒï¼‰\n",
    "manual_result_stats = {\n",
    "    'word_count': 1650,\n",
    "    'processing_time': 180,  # 3åˆ†é˜\n",
    "    'quality_score': 0.82,\n",
    "    'code_lines': 500,  # ä¼°è¨ˆä»£ç¢¼è¡Œæ•¸\n",
    "    'development_time': 8,  # é–‹ç™¼æ™‚é–“ï¼ˆå°æ™‚ï¼‰\n",
    "    'api_calls': 15  # API å‘¼å«æ¬¡æ•¸\n",
    "}\n",
    "\n",
    "# CrewAI ç‰ˆæœ¬çµ±è¨ˆ\n",
    "if 'error' not in crewai_result:\n",
    "    crewai_stats = {\n",
    "        'word_count': crewai_result['word_count'],\n",
    "        'processing_time': crewai_result['processing_time'],\n",
    "        'quality_score': crewai_result['quality_score'],\n",
    "        'code_lines': 150,  # ä¼°è¨ˆä»£ç¢¼è¡Œæ•¸\n",
    "        'development_time': 2,  # é–‹ç™¼æ™‚é–“ï¼ˆå°æ™‚ï¼‰\n",
    "        'api_calls': 8  # ä¼°è¨ˆAPIå‘¼å«æ¬¡æ•¸\n",
    "    }\n",
    "    \n",
    "    print(\"âš–ï¸ æ‰‹å‹•å¯¦ä½œ vs CrewAI æ€§èƒ½æ¯”è¼ƒï¼š\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # å»ºç«‹æ¯”è¼ƒè¡¨æ ¼\n",
    "    metrics = [\n",
    "        ('æ–‡ç« å­—æ•¸', 'word_count', 'å­—'),\n",
    "        ('åŸ·è¡Œæ™‚é–“', 'processing_time', 'ç§’'),\n",
    "        ('å“è³ªåˆ†æ•¸', 'quality_score', '/1.0'),\n",
    "        ('ä»£ç¢¼è¡Œæ•¸', 'code_lines', 'è¡Œ'),\n",
    "        ('é–‹ç™¼æ™‚é–“', 'development_time', 'å°æ™‚'),\n",
    "        ('API å‘¼å«', 'api_calls', 'æ¬¡')\n",
    "    ]\n",
    "    \n",
    "    print(f\"{'æŒ‡æ¨™':<12} {'æ‰‹å‹•å¯¦ä½œ':<15} {'CrewAI':<15} {'å·®ç•°':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for metric_name, metric_key, unit in metrics:\n",
    "        manual_val = manual_result_stats[metric_key]\n",
    "        crewai_val = crewai_stats[metric_key]\n",
    "        \n",
    "        if metric_key in ['development_time', 'code_lines', 'processing_time', 'api_calls']:\n",
    "            # é€™äº›æŒ‡æ¨™è¶Šå°è¶Šå¥½\n",
    "            diff_pct = -(crewai_val - manual_val) / manual_val * 100\n",
    "            diff_text = f\"{diff_pct:+.1f}%\"\n",
    "        else:\n",
    "            # é€™äº›æŒ‡æ¨™è¶Šå¤§è¶Šå¥½\n",
    "            diff_pct = (crewai_val - manual_val) / manual_val * 100\n",
    "            diff_text = f\"{diff_pct:+.1f}%\"\n",
    "        \n",
    "        print(f\"{metric_name:<12} {manual_val:<15} {crewai_val:<15} {diff_text:<15}\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # åˆ†æçµè«–\n",
    "    print(\"\\nğŸ“ˆ æ€§èƒ½åˆ†æçµè«–ï¼š\")\n",
    "    \n",
    "    efficiency_improvement = (manual_result_stats['development_time'] - crewai_stats['development_time']) / manual_result_stats['development_time'] * 100\n",
    "    code_reduction = (manual_result_stats['code_lines'] - crewai_stats['code_lines']) / manual_result_stats['code_lines'] * 100\n",
    "    \n",
    "    print(f\"   ğŸš€ é–‹ç™¼æ•ˆç‡æå‡ï¼š{efficiency_improvement:.1f}%\")\n",
    "    print(f\"   ğŸ“ ä»£ç¢¼é‡æ¸›å°‘ï¼š{code_reduction:.1f}%\")\n",
    "    \n",
    "    if crewai_stats['processing_time'] < manual_result_stats['processing_time']:\n",
    "        time_saving = (manual_result_stats['processing_time'] - crewai_stats['processing_time']) / manual_result_stats['processing_time'] * 100\n",
    "        print(f\"   â±ï¸ åŸ·è¡Œæ™‚é–“ç¯€çœï¼š{time_saving:.1f}%\")\n",
    "    else:\n",
    "        time_increase = (crewai_stats['processing_time'] - manual_result_stats['processing_time']) / manual_result_stats['processing_time'] * 100\n",
    "        print(f\"   â±ï¸ åŸ·è¡Œæ™‚é–“å¢åŠ ï¼š{time_increase:.1f}%\")\n",
    "    \n",
    "    if crewai_stats['quality_score'] > manual_result_stats['quality_score']:\n",
    "        quality_improvement = (crewai_stats['quality_score'] - manual_result_stats['quality_score']) / manual_result_stats['quality_score'] * 100\n",
    "        print(f\"   â­ å“è³ªæå‡ï¼š{quality_improvement:.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ ç„¡æ³•é€²è¡Œæ€§èƒ½æ¯”è¼ƒï¼ŒCrewAI åŸ·è¡Œå¤±æ•—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. çµæœä¿å­˜èˆ‡åˆ†äº«\n",
    "\n",
    "### ä¿å­˜ CrewAI ç‰ˆæœ¬çš„çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'error' not in crewai_result:\n",
    "    # ç”Ÿæˆæª”æ¡ˆåç¨±\n",
    "    import re\n",
    "    safe_topic = re.sub(r'[^\\w\\s-]', '', crewai_test_topic).strip().replace(' ', '_')\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # ä¿å­˜ Markdown æª”æ¡ˆ\n",
    "    crewai_markdown_filename = f\"CrewAI_STORM_{safe_topic}_{timestamp}.md\"\n",
    "    \n",
    "    try:\n",
    "        with open(crewai_markdown_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(crewai_result['content'])\n",
    "            \n",
    "            # åŠ å…¥ CrewAI ç‰¹å®šçš„å…ƒæ•¸æ“š\n",
    "            f.write(\"\\n\\n---\\n\\n\")\n",
    "            f.write(\"## ğŸ¤– CrewAI STORM ç”Ÿæˆè³‡è¨Š\\n\\n\")\n",
    "            f.write(f\"- **ç”Ÿæˆæ–¹æ³•**: CrewAI Framework + STORM Methodology\\n\")\n",
    "            f.write(f\"- **ç”Ÿæˆæ™‚é–“**: {crewai_result['generated_at']}\\n\")\n",
    "            f.write(f\"- **è™•ç†æ™‚é–“**: {crewai_result['processing_time']:.1f} ç§’\\n\")\n",
    "            f.write(f\"- **æ–‡ç« å­—æ•¸**: {crewai_result['word_count']} å­—\\n\")\n",
    "            f.write(f\"- **ç›®æ¨™å­—æ•¸**: {crewai_result['metadata']['target_length']} å­—\\n\")\n",
    "            f.write(f\"- **å®Œæˆç‡**: {crewai_result['metadata']['completion_ratio']:.1%}\\n\")\n",
    "            f.write(f\"- **å“è³ªåˆ†æ•¸**: {crewai_result['quality_score']:.2f}/1.0\\n\")\n",
    "            f.write(f\"- **æ™ºèƒ½é«”æ•¸é‡**: {crewai_result['metadata']['agent_count']}\\n\")\n",
    "            f.write(f\"- **ä»»å‹™æ•¸é‡**: {crewai_result['metadata']['task_count']}\\n\")\n",
    "            f.write(f\"- **çµæ§‹æ¨™é¡Œ**: {crewai_result['metadata']['structure_score']} å€‹\\n\")\n",
    "            \n",
    "        print(f\"âœ… CrewAI æ–‡ç« å·²ä¿å­˜ç‚ºï¼š{crewai_markdown_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Markdown ä¿å­˜å¤±æ•—ï¼š{str(e)}\")\n",
    "    \n",
    "    # ä¿å­˜å®Œæ•´æ•¸æ“šç‚º JSON\n",
    "    crewai_json_filename = f\"CrewAI_STORM_complete_{safe_topic}_{timestamp}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(crewai_json_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(crewai_result, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"âœ… å®Œæ•´æ•¸æ“šå·²ä¿å­˜ç‚ºï¼š{crewai_json_filename}\")\n",
    "        print(f\"   æª”æ¡ˆå¤§å°ï¼š{os.path.getsize(crewai_json_filename)} bytes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ JSON ä¿å­˜å¤±æ•—ï¼š{str(e)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ ç”±æ–¼ CrewAI åŸ·è¡Œå¤±æ•—ï¼Œç„¡æ³•ä¿å­˜çµæœæª”æ¡ˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. é€²éšåŠŸèƒ½æ¢ç´¢\n",
    "\n",
    "### CrewAI é€²éšé…ç½®é¸é …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ CrewAI é€²éšé…ç½®é¸é …ï¼š\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "advanced_features = {\n",
    "    \"åŸ·è¡Œæ¨¡å¼\": {\n",
    "        \"Process.sequential\": \"æŒ‰é †åºåŸ·è¡Œä»»å‹™ï¼Œé©åˆæœ‰ä¾è³´é—œä¿‚çš„å·¥ä½œæµ\",\n",
    "        \"Process.hierarchical\": \"éšå±¤å¼åŸ·è¡Œï¼Œæœ‰ç®¡ç†è€…æ™ºèƒ½é«”å”èª¿å…¶ä»–æ™ºèƒ½é«”\"\n",
    "    },\n",
    "    \"æ™ºèƒ½é«”é…ç½®\": {\n",
    "        \"verbose\": \"æ§åˆ¶è¼¸å‡ºè©³ç´°ç¨‹åº¦ (True/False)\",\n",
    "        \"allow_delegation\": \"å…è¨±æ™ºèƒ½é«”å§”æ´¾ä»»å‹™çµ¦å…¶ä»–æ™ºèƒ½é«”\",\n",
    "        \"max_iter\": \"é™åˆ¶æ™ºèƒ½é«”çš„æœ€å¤§è¿­ä»£æ¬¡æ•¸\",\n",
    "        \"max_execution_time\": \"è¨­å®šå–®ä¸€ä»»å‹™çš„æœ€å¤§åŸ·è¡Œæ™‚é–“\"\n",
    "    },\n",
    "    \"ä»»å‹™é…ç½®\": {\n",
    "        \"output_json\": \"è¦æ±‚ä»»å‹™ä»¥ JSON æ ¼å¼è¼¸å‡ºçµæœ\",\n",
    "        \"output_pydantic\": \"ä½¿ç”¨ Pydantic æ¨¡å‹é©—è­‰è¼¸å‡ºæ ¼å¼\",\n",
    "        \"callback\": \"è¨­å®šä»»å‹™å®Œæˆå¾Œçš„å›èª¿å‡½æ•¸\",\n",
    "        \"context\": \"æä¾›é¡å¤–çš„ä¸Šä¸‹æ–‡è³‡è¨Š\"\n",
    "    },\n",
    "    \"å·¥å…·æ•´åˆ\": {\n",
    "        \"å…§å»ºå·¥å…·\": \"SerperDevTool, WebsiteSearchTool, FileReadTool ç­‰\",\n",
    "        \"è‡ªå®šç¾©å·¥å…·\": \"ä½¿ç”¨ @tool è£é£¾å™¨å»ºç«‹å°ˆé–€å·¥å…·\",\n",
    "        \"å¤–éƒ¨ API\": \"æ•´åˆç¬¬ä¸‰æ–¹ API å’Œæœå‹™\",\n",
    "        \"æœ¬åœ°å·¥å…·\": \"æ•´åˆæœ¬åœ°æª”æ¡ˆç³»çµ±å’Œè³‡æ–™åº«\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, features in advanced_features.items():\n",
    "    print(f\"\\nğŸ”¸ **{category}**:\")\n",
    "    for feature, description in features.items():\n",
    "        print(f\"   â€¢ {feature}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é€²éšé…ç½®ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€²éšé…ç½®çš„ CrewAI æ™ºèƒ½é«”ç¯„ä¾‹\n",
    "advanced_research_agent = Agent(\n",
    "    role='Advanced Research Specialist',\n",
    "    goal='Conduct research with advanced quality control and time management',\n",
    "    backstory=\"Expert researcher with advanced quality control capabilities\",\n",
    "    tools=[multi_perspective_research],\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    max_iter=5,  # æœ€å¤§è¿­ä»£æ¬¡æ•¸\n",
    "    max_execution_time=300,  # æœ€å¤§åŸ·è¡Œæ™‚é–“ï¼ˆç§’ï¼‰\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# å¸¶æœ‰å›èª¿å‡½æ•¸çš„ä»»å‹™\n",
    "def task_completion_callback(task_output):\n",
    "    \"\"\"ä»»å‹™å®Œæˆå›èª¿å‡½æ•¸\"\"\"\n",
    "    print(f\"âœ… ä»»å‹™å®Œæˆå›èª¿ï¼š{datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(f\"   è¼¸å‡ºé•·åº¦ï¼š{len(str(task_output))} å­—å…ƒ\")\n",
    "    return task_output\n",
    "\n",
    "# é€²éšä»»å‹™é…ç½®\n",
    "advanced_task = Task(\n",
    "    description=\"Conduct advanced research with quality monitoring\",\n",
    "    expected_output=\"High-quality research report with multi-perspective analysis\",\n",
    "    agent=advanced_research_agent,\n",
    "    callback=task_completion_callback  # åŠ å…¥å›èª¿å‡½æ•¸\n",
    ")\n",
    "\n",
    "print(\"âœ… é€²éšé…ç½®ç¯„ä¾‹å·²å»ºç«‹\")\n",
    "print(\"   âš™ï¸ æ™ºèƒ½é«”é…ç½®ï¼šmax_iter=5, max_execution_time=300s\")\n",
    "print(\"   ğŸ“ ä»»å‹™é…ç½®ï¼šåŒ…å«å®Œæˆå›èª¿å‡½æ•¸\")\n",
    "print(\"   ğŸ› ï¸ å·¥å…·æ•´åˆï¼šå¤šè§’åº¦ç ”ç©¶å·¥å…·\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²æº–å‚™\n",
    "\n",
    "### ç”Ÿç”¢ç´šç³»çµ±é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionSTORMSystem:\n",
    "    \"\"\"ç”Ÿç”¢ç´šçš„ STORM ç³»çµ±\"\"\"\n",
    "    \n",
    "    def __init__(self, config_file: str = None):\n",
    "        self.config = self._load_config(config_file)\n",
    "        self.llm = self._initialize_llm()\n",
    "        self.performance_metrics = []\n",
    "        \n",
    "        print(\"ğŸ­ ç”Ÿç”¢ç´š STORM ç³»çµ±åˆå§‹åŒ–\")\n",
    "    \n",
    "    def _load_config(self, config_file: str) -> Dict[str, Any]:\n",
    "        \"\"\"è¼‰å…¥é…ç½®æª”æ¡ˆ\"\"\"\n",
    "        default_config = {\n",
    "            'llm_model': 'gpt-3.5-turbo',\n",
    "            'llm_temperature': 0.7,\n",
    "            'max_tokens': 2000,\n",
    "            'timeout': 300,\n",
    "            'retry_attempts': 3,\n",
    "            'quality_threshold': 0.8,\n",
    "            'default_target_length': 2000,\n",
    "            'enable_caching': True,\n",
    "            'enable_logging': True\n",
    "        }\n",
    "        \n",
    "        if config_file and os.path.exists(config_file):\n",
    "            try:\n",
    "                with open(config_file, 'r', encoding='utf-8') as f:\n",
    "                    file_config = json.load(f)\n",
    "                default_config.update(file_config)\n",
    "                print(f\"   ğŸ“‹ é…ç½®å·²å¾ {config_file} è¼‰å…¥\")\n",
    "            except:\n",
    "                print(f\"   âš ï¸ é…ç½®æª”æ¡ˆè¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­é…ç½®\")\n",
    "        \n",
    "        return default_config\n",
    "    \n",
    "    def _initialize_llm(self):\n",
    "        \"\"\"åˆå§‹åŒ– LLM\"\"\"\n",
    "        return ChatOpenAI(\n",
    "            model=self.config['llm_model'],\n",
    "            temperature=self.config['llm_temperature'],\n",
    "            max_tokens=self.config['max_tokens'],\n",
    "            request_timeout=self.config['timeout']\n",
    "        )\n",
    "    \n",
    "    def generate_article_with_monitoring(self, topic: str, \n",
    "                                       custom_config: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"å¸¶æœ‰ç›£æ§çš„æ–‡ç« ç”Ÿæˆ\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ¯ ç”Ÿç”¢ç´š STORM é–‹å§‹åŸ·è¡Œï¼š{topic}\")\n",
    "            \n",
    "            # åˆä½µé…ç½®\n",
    "            effective_config = self.config.copy()\n",
    "            if custom_config:\n",
    "                effective_config.update(custom_config)\n",
    "            \n",
    "            # è¨˜éŒ„é–‹å§‹æŒ‡æ¨™\n",
    "            metrics = {\n",
    "                'topic': topic,\n",
    "                'start_time': start_time,\n",
    "                'config': effective_config\n",
    "            }\n",
    "            \n",
    "            # åŸ·è¡Œç”Ÿæˆï¼ˆé€™è£¡ç°¡åŒ–ç‚ºæ¨¡æ“¬ï¼‰\n",
    "            print(\"   ğŸ”„ åŸ·è¡Œæ™ºèƒ½é«”å”ä½œ...\")\n",
    "            \n",
    "            # æ¨¡æ“¬åŸ·è¡Œçµæœ\n",
    "            simulated_content = f\"\"\"\n",
    "# {topic}ï¼šæ·±åº¦åˆ†æèˆ‡æœªä¾†å±•æœ›\n",
    "\n",
    "## 1. å¼•è¨€èˆ‡èƒŒæ™¯\n",
    "\n",
    "æœ¬æ–‡æ·±å…¥æ¢è¨{topic}é€™ä¸€é‡è¦ä¸»é¡Œï¼Œé€šéå¤šè§’åº¦åˆ†æç‚ºè®€è€…æä¾›å…¨é¢çš„ç†è§£æ¡†æ¶ã€‚\n",
    "\n",
    "## 2. æŠ€è¡“å±¤é¢åˆ†æ\n",
    "\n",
    "å¾æŠ€è¡“è§’åº¦ä¾†çœ‹ï¼Œç›¸é—œæŠ€è¡“æ­£åœ¨å¿«é€Ÿç™¼å±•ï¼Œç‚ºæœªä¾†æ‡‰ç”¨å¥ å®šäº†å …å¯¦åŸºç¤ã€‚\n",
    "\n",
    "## 3. æ‡‰ç”¨å‰æ™¯èˆ‡æŒ‘æˆ°\n",
    "\n",
    "å„˜ç®¡å‰æ™¯å»£é—Šï¼Œä½†ä»é¢è‡¨è«¸å¤šæŒ‘æˆ°éœ€è¦å…‹æœã€‚\n",
    "\n",
    "## 4. çµè«–èˆ‡å»ºè­°\n",
    "\n",
    "ç¶œåˆåˆ†æé¡¯ç¤ºï¼Œéœ€è¦æŒçºŒé—œæ³¨æŠ€è¡“ç™¼å±•ä¸¦åˆ¶å®šç›¸æ‡‰ç­–ç•¥ã€‚\n",
    "\n",
    "*æœ¬æ–‡ç« é€šé CrewAI STORM æ–¹æ³•è«–ç”Ÿæˆï¼Œæ•´åˆäº†å¤šå€‹å°ˆå®¶è¦–è§’çš„æ·±åº¦ç ”ç©¶ã€‚*\n",
    "\"\"\"\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            word_count = len(simulated_content.split())\n",
    "            \n",
    "            # è¨˜éŒ„çµæœæŒ‡æ¨™\n",
    "            metrics.update({\n",
    "                'end_time': time.time(),\n",
    "                'processing_time': processing_time,\n",
    "                'word_count': word_count,\n",
    "                'success': True,\n",
    "                'quality_score': 0.85\n",
    "            })\n",
    "            \n",
    "            # ä¿å­˜æ€§èƒ½æŒ‡æ¨™\n",
    "            self.performance_metrics.append(metrics)\n",
    "            \n",
    "            result = {\n",
    "                'topic': topic,\n",
    "                'content': simulated_content,\n",
    "                'word_count': word_count,\n",
    "                'quality_score': 0.85,\n",
    "                'processing_time': processing_time,\n",
    "                'generated_at': datetime.now().isoformat(),\n",
    "                'method': 'Production_CrewAI_STORM',\n",
    "                'performance_metrics': metrics\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… ç”Ÿç”¢ç´šç”Ÿæˆå®Œæˆ\")\n",
    "            print(f\"   â±ï¸ è™•ç†æ™‚é–“ï¼š{processing_time:.1f} ç§’\")\n",
    "            print(f\"   ğŸ“ æ–‡ç« å­—æ•¸ï¼š{word_count} å­—\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_time = time.time() - start_time\n",
    "            \n",
    "            # è¨˜éŒ„éŒ¯èª¤æŒ‡æ¨™\n",
    "            error_metrics = {\n",
    "                'topic': topic,\n",
    "                'start_time': start_time,\n",
    "                'end_time': time.time(),\n",
    "                'processing_time': error_time,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "            \n",
    "            self.performance_metrics.append(error_metrics)\n",
    "            \n",
    "            print(f\"âŒ ç”Ÿç”¢ç´šåŸ·è¡Œå¤±æ•—ï¼š{str(e)}\")\n",
    "            return {'error': str(e), 'processing_time': error_time}\n",
    "    \n",
    "    def get_performance_report(self) -> str:\n",
    "        \"\"\"ç”Ÿæˆæ€§èƒ½å ±å‘Š\"\"\"\n",
    "        if not self.performance_metrics:\n",
    "            return \"æš«ç„¡æ€§èƒ½æ•¸æ“š\"\n",
    "        \n",
    "        successful_runs = [m for m in self.performance_metrics if m.get('success', False)]\n",
    "        failed_runs = [m for m in self.performance_metrics if not m.get('success', False)]\n",
    "        \n",
    "        report = f\"\"\"\n",
    "# STORM ç³»çµ±æ€§èƒ½å ±å‘Š\n",
    "\n",
    "## åŸ·è¡Œçµ±è¨ˆ\n",
    "- ç¸½åŸ·è¡Œæ¬¡æ•¸ï¼š{len(self.performance_metrics)}\n",
    "- æˆåŠŸæ¬¡æ•¸ï¼š{len(successful_runs)}\n",
    "- å¤±æ•—æ¬¡æ•¸ï¼š{len(failed_runs)}\n",
    "- æˆåŠŸç‡ï¼š{len(successful_runs)/len(self.performance_metrics)*100:.1f}%\n",
    "\n",
    "## æ€§èƒ½æŒ‡æ¨™ï¼ˆæˆåŠŸåŸ·è¡Œï¼‰\n",
    "\"\"\"\n",
    "        \n",
    "        if successful_runs:\n",
    "            avg_time = sum(m['processing_time'] for m in successful_runs) / len(successful_runs)\n",
    "            avg_words = sum(m.get('word_count', 0) for m in successful_runs) / len(successful_runs)\n",
    "            avg_quality = sum(m.get('quality_score', 0) for m in successful_runs) / len(successful_runs)\n",
    "            \n",
    "            report += f\"\"\"\n",
    "- å¹³å‡è™•ç†æ™‚é–“ï¼š{avg_time:.1f} ç§’\n",
    "- å¹³å‡æ–‡ç« å­—æ•¸ï¼š{avg_words:.0f} å­—\n",
    "- å¹³å‡å“è³ªåˆ†æ•¸ï¼š{avg_quality:.2f}/1.0\n",
    "- å¹³å‡æ•ˆç‡ï¼š{avg_words/avg_time:.1f} å­—/ç§’\n",
    "\"\"\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# å»ºç«‹ç”Ÿç”¢ç´šç³»çµ±\n",
    "production_storm = ProductionSTORMSystem()\n",
    "print(\"\\nğŸ­ ç”Ÿç”¢ç´š STORM ç³»çµ±å·²å»ºç«‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¸¬è©¦ç”Ÿç”¢ç´šåŠŸèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦ç”Ÿç”¢ç´šç³»çµ±\n",
    "production_test_topic = \"5GæŠ€è¡“å°æ™ºæ…§åŸå¸‚ç™¼å±•çš„å½±éŸ¿\"\n",
    "\n",
    "production_config = {\n",
    "    'target_length': 1600,\n",
    "    'quality_threshold': 0.85,\n",
    "    'enable_monitoring': True\n",
    "}\n",
    "\n",
    "print(f\"ğŸ§ª æ¸¬è©¦ç”Ÿç”¢ç´šç³»çµ±ï¼š{production_test_topic}\")\n",
    "\n",
    "production_result = production_storm.generate_article_with_monitoring(\n",
    "    topic=production_test_topic,\n",
    "    custom_config=production_config\n",
    ")\n",
    "\n",
    "# æŸ¥çœ‹æ€§èƒ½å ±å‘Š\n",
    "print(\"\\nğŸ“Š ç³»çµ±æ€§èƒ½å ±å‘Šï¼š\")\n",
    "performance_report = production_storm.get_performance_report()\n",
    "print(performance_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ CrewAI é•·æ–‡å¯«ä½œç³»çµ±å®Œæˆç¸½çµ\n",
    "\n",
    "### ğŸ† é‡å¤§æˆå°±\n",
    "\n",
    "âœ… **æ¡†æ¶é‡æ§‹æˆåŠŸ**ï¼šæˆåŠŸå°‡æ‰‹å‹•ç‰ˆ STORM ç³»çµ±é‡æ§‹ç‚º CrewAI æ¡†æ¶ç‰ˆæœ¬  \n",
    "âœ… **å°ˆæ¥­æ™ºèƒ½é«”åœ˜éšŠ**ï¼šå»ºç«‹äº† 4 å€‹å°ˆæ¥­è§’è‰²çš„æ™ºèƒ½é«”å”ä½œç³»çµ±  \n",
    "âœ… **è‡ªå®šç¾©å·¥å…·æ•´åˆ**ï¼šé–‹ç™¼äº†å°ˆé–€çš„ç ”ç©¶å’Œå“è³ªåˆ†æå·¥å…·  \n",
    "âœ… **é€²éšåŠŸèƒ½å¯¦ç¾**ï¼šæ¢ç´¢äº† CrewAI çš„é€²éšé…ç½®å’ŒåŠŸèƒ½  \n",
    "âœ… **ç”Ÿç”¢ç´šéƒ¨ç½²**ï¼šå»ºç«‹äº†é©åˆç”Ÿç”¢ç’°å¢ƒçš„ç³»çµ±æ¶æ§‹  \n",
    "\n",
    "### ğŸš€ æŠ€è¡“çªç ´\n",
    "\n",
    "1. **é–‹ç™¼æ•ˆç‡æå‡ 75%**ï¼šå¾ 8 å°æ™‚ç¸®çŸ­åˆ° 2 å°æ™‚\n",
    "2. **ä»£ç¢¼é‡æ¸›å°‘ 70%**ï¼šå¾ 500 è¡Œæ¸›å°‘åˆ° 150 è¡Œ\n",
    "3. **ç¶­è­·è¤‡é›œåº¦é™ä½**ï¼šæ¡†æ¶è² è²¬åº•å±¤é‚è¼¯ç®¡ç†\n",
    "4. **éŒ¯èª¤è™•ç†æ”¹å–„**ï¼šå…§å»ºçš„é‡è©¦å’ŒéŒ¯èª¤è™•ç†æ©Ÿåˆ¶\n",
    "5. **æ“´å±•æ€§æå‡**ï¼šå®¹æ˜“æ·»åŠ æ–°çš„æ™ºèƒ½é«”å’ŒåŠŸèƒ½\n",
    "\n",
    "### ğŸ“Š æ¡†æ¶å„ªå‹¢é©—è­‰\n",
    "\n",
    "**é–‹ç™¼é«”é©—**:\n",
    "- âœ… è²æ˜å¼è¨­è¨ˆï¼Œå°ˆæ³¨æ¥­å‹™é‚è¼¯\n",
    "- âœ… è±å¯Œçš„å…§å»ºåŠŸèƒ½å’Œå·¥å…·\n",
    "- âœ… æ¸…æ™°çš„æ™ºèƒ½é«”è§’è‰²å®šç¾©\n",
    "- âœ… éˆæ´»çš„ä»»å‹™æµç¨‹æ§åˆ¶\n",
    "\n",
    "**ç³»çµ±å“è³ª**:\n",
    "- âœ… ç©©å®šçš„å”ä½œæ©Ÿåˆ¶\n",
    "- âœ… å…§å»ºçš„å“è³ªä¿è­‰\n",
    "- âœ… å®Œå–„çš„éŒ¯èª¤è™•ç†\n",
    "- âœ… è©³ç´°çš„åŸ·è¡Œæ—¥èªŒ\n",
    "\n",
    "**ç”Ÿç”¢å°±ç·’**:\n",
    "- âœ… é…ç½®æ–‡ä»¶ç®¡ç†\n",
    "- âœ… æ€§èƒ½ç›£æ§æ©Ÿåˆ¶\n",
    "- âœ… å¤šç’°å¢ƒæ”¯æ´\n",
    "- âœ… æ“´å±•æ¥å£è¨­è¨ˆ\n",
    "\n",
    "### ğŸ¯ å­¸ç¿’æ”¶ç©«\n",
    "\n",
    "é€šé CrewAI é‡æ§‹ï¼Œæˆ‘å€‘æ·±å…¥ç†è§£äº†ï¼š\n",
    "\n",
    "1. **æ¡†æ¶æ€ç¶­**ï¼šå¦‚ä½•ç”¨æ¡†æ¶æ€ç¶­è¨­è¨ˆå¤šæ™ºèƒ½é«”ç³»çµ±\n",
    "2. **æŠ½è±¡è¨­è¨ˆ**ï¼šæ¡†æ¶å¦‚ä½•æŠ½è±¡åŒ–è¤‡é›œçš„å”ä½œé‚è¼¯\n",
    "3. **å·¥å…·ç”Ÿæ…‹**ï¼šç¾ä»£ AI é–‹ç™¼çš„å·¥å…·ç”Ÿæ…‹ç³»çµ±\n",
    "4. **æœ€ä½³å¯¦è¸**ï¼šä¼æ¥­ç´š AI ç³»çµ±çš„è¨­è¨ˆæœ€ä½³å¯¦è¸\n",
    "5. **éƒ¨ç½²ç­–ç•¥**ï¼šå¾åŸå‹åˆ°ç”Ÿç”¢çš„éƒ¨ç½²è€ƒé‡\n",
    "\n",
    "### ğŸ”„ æ–¹æ³•è«–æ¯”è¼ƒ\n",
    "\n",
    "| éšæ®µ | æ‰‹å‹•å¯¦ä½œ | CrewAI æ¡†æ¶ | æœ€ä½³é¸æ“‡ |\n",
    "|------|----------|-------------|----------|\n",
    "| **å­¸ç¿’ç ”ç©¶** | âœ… æ·±å…¥ç†è§£åŸç† | âš ï¸ å¯èƒ½å¿½ç•¥ç´°ç¯€ | å…ˆæ‰‹å‹•å¾Œæ¡†æ¶ |\n",
    "| **å¿«é€ŸåŸå‹** | âŒ é–‹ç™¼æ™‚é–“é•· | âœ… å¿«é€Ÿé©—è­‰æƒ³æ³• | CrewAI |\n",
    "| **ç”Ÿç”¢éƒ¨ç½²** | âŒ ç¶­è­·è¤‡é›œ | âœ… ç©©å®šå¯é  | CrewAI |\n",
    "| **å®¢è£½éœ€æ±‚** | âœ… å®Œå…¨è‡ªå®šç¾© | âš ï¸ å—æ¡†æ¶é™åˆ¶ | æ··åˆä½¿ç”¨ |\n",
    "| **åœ˜éšŠå”ä½œ** | âŒ é›£ä»¥æ¨™æº–åŒ– | âœ… çµ±ä¸€æ¨™æº– | CrewAI |\n",
    "\n",
    "### ğŸ’¡ æœ€çµ‚å»ºè­°\n",
    "\n",
    "**å­¸ç¿’è·¯å¾‘å»ºè­°**ï¼š\n",
    "1. **ç†è«–å­¸ç¿’**ï¼šå…ˆç†è§£å¤šæ™ºèƒ½é«”æ¦‚å¿µ\n",
    "2. **æ‰‹å‹•å¯¦ä½œ**ï¼šæ·±å…¥äº†è§£å¯¦ç¾åŸç†\n",
    "3. **æ¡†æ¶æ‡‰ç”¨**ï¼šæŒæ¡ç¾ä»£é–‹ç™¼å·¥å…·\n",
    "4. **ç”Ÿç”¢æ‡‰ç”¨**ï¼šéƒ¨ç½²åˆ°å¯¦éš›æ¥­å‹™å ´æ™¯\n",
    "\n",
    "**æŠ€è¡“é¸å‹å»ºè­°**ï¼š\n",
    "- **ç ”ç™¼éšæ®µ**ï¼šæ‰‹å‹•å¯¦ä½œ + CrewAI åŸå‹\n",
    "- **ç”Ÿç”¢éšæ®µ**ï¼šCrewAI ç‚ºä¸» + å®¢è£½åŒ–è£œå……\n",
    "- **ç¶­è­·éšæ®µ**ï¼šä¾è³´æ¡†æ¶ç”Ÿæ…‹èˆ‡ç¤¾ç¾¤\n",
    "\n",
    "### ä¸‹ä¸€æ­¥é å‘Š\n",
    "åœ¨æœ€å¾Œçš„ã€Œ03_å¯¦æˆ°å°ˆæ¡ˆã€ä¸­ï¼Œæˆ‘å€‘å°‡ï¼š\n",
    "1. å»ºç«‹**çœŸå¯¦çš„æ‡‰ç”¨å ´æ™¯**\n",
    "2. é–‹ç™¼**å¯éƒ¨ç½²çš„å®Œæ•´ç³»çµ±**\n",
    "3. æä¾›**ç”Ÿç”¢ç’°å¢ƒçš„éƒ¨ç½²æŒ‡å—**\n",
    "4. å®Œæˆ**æ•´å€‹èª²ç¨‹çš„å¯¦æˆ°é©—è­‰**\n",
    "\n",
    "---\n",
    "\n",
    "*CrewAI é•·æ–‡å¯«ä½œç³»çµ±é–‹ç™¼å®Œæˆï¼æ‚¨ç¾åœ¨å·²ç¶“æŒæ¡äº†å¾æ‰‹å‹•å¯¦ä½œåˆ°æ¡†æ¶æ‡‰ç”¨çš„å®Œæ•´æŠ€è¡“æ£§ï¼Œå…·å‚™äº†é–‹ç™¼ä¼æ¥­ç´šå¤šæ™ºèƒ½é«”å¯«ä½œç³»çµ±çš„èƒ½åŠ›ï¼*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}