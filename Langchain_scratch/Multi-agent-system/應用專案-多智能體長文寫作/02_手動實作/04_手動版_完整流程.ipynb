{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_æ‰‹å‹•ç‰ˆ_å®Œæ•´æµç¨‹\n",
    "\n",
    "## ğŸ¯ å­¸ç¿’ç›®æ¨™\n",
    "- æ•´åˆç ”ç©¶æ™ºèƒ½é«”èˆ‡å¯«ä½œæ™ºèƒ½é«”\n",
    "- å¯¦ç¾å®Œæ•´çš„ STORM å¯«ä½œæµç¨‹\n",
    "- å»ºç«‹æ™ºèƒ½é«”é–“å”ä½œæ©Ÿåˆ¶\n",
    "- é€²è¡Œç«¯åˆ°ç«¯ç³»çµ±æ¸¬è©¦\n",
    "- å®Œæˆæ‰‹å‹•ç‰ˆ STORM ç³»çµ±\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒæº–å‚™èˆ‡æ¨¡çµ„è¼‰å…¥\n",
    "\n",
    "### è¼‰å…¥ä¹‹å‰é–‹ç™¼çš„æ‰€æœ‰çµ„ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¤å¥—ä»¶\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# LangChain ç›¸é—œ\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# ç¶²è·¯æœå°‹èˆ‡å…§å®¹æ“·å–\n",
    "from duckduckgo_search import DDGS\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ç’°å¢ƒè®Šæ•¸\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… åŸºç¤å¥—ä»¶è¼‰å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é‡æ–°å»ºç«‹æ‰€æœ‰æ™ºèƒ½é«”é¡åˆ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¾å‰é¢çš„ notebook è¤‡è£½æ‰€æœ‰å¿…è¦çš„é¡åˆ¥\n",
    "# é€™è£¡æˆ‘å€‘å°‡é‡æ–°å®šç¾©å®Œæ•´çš„é¡åˆ¥çµæ§‹\n",
    "\n",
    "# åˆå§‹åŒ– LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "print(\"âœ… LLM åˆå§‹åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ç ”ç©¶æ™ºèƒ½é«”ç›¸é—œé¡åˆ¥ ===\n",
    "\n",
    "class ExpertPersona:\n",
    "    \"\"\"å°ˆå®¶è§’è‰²é¡åˆ¥\"\"\"\n",
    "    def __init__(self, name: str, expertise: str, perspective: str, question_style: str):\n",
    "        self.name = name\n",
    "        self.expertise = expertise\n",
    "        self.perspective = perspective\n",
    "        self.question_style = question_style\n",
    "    \n",
    "    def get_system_prompt(self) -> str:\n",
    "        return f\"\"\"\n",
    "ä½ æ˜¯ä¸€ä½{self.expertise}å°ˆå®¶ï¼Œåå«{self.name}ã€‚\n",
    "ä½ çš„å°ˆæ¥­èƒŒæ™¯ï¼š{self.expertise}\n",
    "ä½ çš„è§€é»ç‰¹è‰²ï¼š{self.perspective}\n",
    "ä½ çš„æå•é¢¨æ ¼ï¼š{self.question_style}\n",
    "è«‹æ ¹æ“šä½ çš„å°ˆæ¥­èƒŒæ™¯å’Œè§€é»ï¼Œé‡å°çµ¦å®šçš„ä¸»é¡Œæå‡ºæ·±åº¦å•é¡Œã€‚\n",
    "\"\"\"\n",
    "\n",
    "class QuestionGenerator:\n",
    "    \"\"\"å•é¡Œç”Ÿæˆå™¨\"\"\"\n",
    "    def __init__(self, llm, expert_personas):\n",
    "        self.llm = llm\n",
    "        self.expert_personas = expert_personas\n",
    "    \n",
    "    def generate_questions_from_expert(self, topic: str, expert_name: str, num_questions: int = 2) -> List[str]:\n",
    "        if expert_name not in self.expert_personas:\n",
    "            return []\n",
    "        \n",
    "        expert = self.expert_personas[expert_name]\n",
    "        system_prompt = expert.get_system_prompt()\n",
    "        human_prompt = f\"\"\"\n",
    "ä¸»é¡Œï¼š{topic}\n",
    "è«‹é‡å°é€™å€‹ä¸»é¡Œï¼Œæå‡º {num_questions} å€‹å…·æœ‰æ·±åº¦ä¸”æœ‰åƒ¹å€¼çš„ç ”ç©¶å•é¡Œã€‚\n",
    "è«‹ä»¥ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š\n",
    "Q1: [å•é¡Œå…§å®¹]\n",
    "Q2: [å•é¡Œå…§å®¹]\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            messages = [SystemMessage(content=system_prompt), HumanMessage(content=human_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            \n",
    "            questions = []\n",
    "            lines = response.content.strip().split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line.startswith('Q') and ':' in line:\n",
    "                    question = line.split(':', 1)[1].strip()\n",
    "                    questions.append(question)\n",
    "            return questions\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def generate_comprehensive_questions(self, topic: str, questions_per_expert: int = 2) -> Dict[str, List[str]]:\n",
    "        all_questions = {}\n",
    "        print(f\"ğŸ¤” ç”Ÿæˆå¤šè§’åº¦ç ”ç©¶å•é¡Œï¼š{topic}\")\n",
    "        \n",
    "        for expert_name in self.expert_personas.keys():\n",
    "            print(f\"   â³ {expert_name} æ€è€ƒä¸­...\")\n",
    "            questions = self.generate_questions_from_expert(topic, expert_name, questions_per_expert)\n",
    "            all_questions[expert_name] = questions\n",
    "            print(f\"   âœ… {expert_name} æå‡ºäº† {len(questions)} å€‹å•é¡Œ\")\n",
    "            time.sleep(0.5)  # é¿å…APIé™åˆ¶\n",
    "        \n",
    "        return all_questions\n",
    "\n",
    "class WebResearcher:\n",
    "    \"\"\"ç¶²è·¯ç ”ç©¶å“¡\"\"\"\n",
    "    def __init__(self):\n",
    "        self.ddgs = DDGS()\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "    \n",
    "    def search_web(self, query: str, max_results: int = 3) -> List[Dict]:\n",
    "        try:\n",
    "            results = list(self.ddgs.text(query, max_results=max_results))\n",
    "            return results\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def extract_content(self, url: str, max_length: int = 1500) -> str:\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for element in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\"]):\n",
    "                element.decompose()\n",
    "            \n",
    "            text = soup.get_text()\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            clean_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            return clean_text[:max_length] + \"...\" if len(clean_text) > max_length else clean_text\n",
    "        except:\n",
    "            return \"å…§å®¹æ“·å–å¤±æ•—\"\n",
    "    \n",
    "    def research_question(self, question: str, max_sources: int = 2) -> Dict[str, Any]:\n",
    "        print(f\"   ğŸ“š ç ”ç©¶ï¼š{question[:50]}...\")\n",
    "        \n",
    "        search_results = self.search_web(question, max_sources)\n",
    "        sources = []\n",
    "        \n",
    "        for result in search_results:\n",
    "            content = self.extract_content(result['href'])\n",
    "            sources.append({\n",
    "                'title': result['title'],\n",
    "                'url': result['href'],\n",
    "                'snippet': result.get('body', ''),\n",
    "                'content': content,\n",
    "                'extracted_at': datetime.now().isoformat()\n",
    "            })\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'sources': sources,\n",
    "            'research_date': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "class ContentAnalyzer:\n",
    "    \"\"\"å…§å®¹åˆ†æå™¨\"\"\"\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def analyze_research_content(self, research_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        question = research_data['question']\n",
    "        sources = research_data['sources']\n",
    "        \n",
    "        all_content = \"\"\n",
    "        for i, source in enumerate(sources, 1):\n",
    "            if source['content'] and 'å¤±æ•—' not in source['content']:\n",
    "                all_content += f\"\\n\\nä¾†æº {i}ï¼š{source['content'][:800]}\"\n",
    "        \n",
    "        if not all_content.strip():\n",
    "            return {\n",
    "                'question': question,\n",
    "                'summary': 'ç„¡æ³•ç²å¾—æœ‰æ•ˆå…§å®¹',\n",
    "                'key_points': [],\n",
    "                'insights': '',\n",
    "                'confidence': 0.3\n",
    "            }\n",
    "        \n",
    "        analysis_prompt = f\"\"\"\n",
    "è«‹åˆ†æä»¥ä¸‹ç ”ç©¶å•é¡Œå’Œç›¸é—œè³‡æ–™ï¼š\n",
    "\n",
    "å•é¡Œï¼š{question}\n",
    "è³‡æ–™ï¼š{all_content[:2000]}\n",
    "\n",
    "è«‹æä¾›ï¼š\n",
    "1. ç¶œåˆæ‘˜è¦ (1-2å¥)ï¼š\n",
    "2. é—œéµè¦é» (2-3å€‹è¦é»)ï¼š\n",
    "- è¦é»1\n",
    "- è¦é»2\n",
    "3. å°ˆæ¥­æ´å¯Ÿ (1æ®µ)ï¼š\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=analysis_prompt)])\n",
    "            analysis_text = response.content\n",
    "            \n",
    "            # ç°¡åŒ–è§£æ\n",
    "            lines = analysis_text.split('\\n')\n",
    "            summary = \"\"\n",
    "            key_points = []\n",
    "            insights = \"\"\n",
    "            \n",
    "            current_section = \"\"\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if 'ç¶œåˆæ‘˜è¦' in line:\n",
    "                    current_section = 'summary'\n",
    "                elif 'é—œéµè¦é»' in line:\n",
    "                    current_section = 'points'\n",
    "                elif 'å°ˆæ¥­æ´å¯Ÿ' in line:\n",
    "                    current_section = 'insights'\n",
    "                elif line.startswith('-') and current_section == 'points':\n",
    "                    key_points.append(line[1:].strip())\n",
    "                elif line and current_section == 'summary' and not 'ï¼š' in line:\n",
    "                    summary += line + \" \"\n",
    "                elif line and current_section == 'insights' and not 'ï¼š' in line:\n",
    "                    insights += line + \" \"\n",
    "            \n",
    "            return {\n",
    "                'question': question,\n",
    "                'summary': summary.strip(),\n",
    "                'key_points': key_points,\n",
    "                'insights': insights.strip(),\n",
    "                'confidence': 0.8,\n",
    "                'source_count': len(sources)\n",
    "            }\n",
    "        except:\n",
    "            return {\n",
    "                'question': question,\n",
    "                'summary': 'åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤',\n",
    "                'key_points': [],\n",
    "                'insights': '',\n",
    "                'confidence': 0.3\n",
    "            }\n",
    "\n",
    "print(\"âœ… ç ”ç©¶æ™ºèƒ½é«”é¡åˆ¥å·²å®šç¾©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === å¯«ä½œæ™ºèƒ½é«”ç›¸é—œé¡åˆ¥ ===\n",
    "\n",
    "class OutlineGenerator:\n",
    "    \"\"\"å¤§ç¶±ç”Ÿæˆå™¨\"\"\"\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def generate_outline(self, research_data: Dict[str, Any], \n",
    "                        article_type: str = \"academic\", \n",
    "                        target_length: int = 2000) -> Dict[str, Any]:\n",
    "        print(f\"ğŸ“‹ ç”Ÿæˆå¤§ç¶±ï¼š{article_type}é¡å‹ï¼Œ{target_length}å­—\")\n",
    "        \n",
    "        # ç°¡åŒ–çš„å¤§ç¶±ç”Ÿæˆ\n",
    "        topic = research_data.get('topic', 'æœªæŒ‡å®šä¸»é¡Œ')\n",
    "        comprehensive_report = research_data.get('comprehensive_report', '')\n",
    "        \n",
    "        outline_prompt = f\"\"\"\n",
    "è«‹ç‚ºä¸»é¡Œã€Œ{topic}ã€è¨­è¨ˆä¸€å€‹{article_type}é¢¨æ ¼çš„æ–‡ç« å¤§ç¶±ï¼Œç›®æ¨™é•·åº¦{target_length}å­—ã€‚\n",
    "\n",
    "åƒè€ƒè³‡æ–™ï¼š\n",
    "{comprehensive_report[:1500]}\n",
    "\n",
    "è«‹è¨­è¨ˆ3-4å€‹ä¸»è¦ç« ç¯€çš„å¤§ç¶±ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n",
    "\n",
    "## 1. ç« ç¯€æ¨™é¡Œ\n",
    "### 1.1 å°ç¯€æ¨™é¡Œ\n",
    "- å…§å®¹èªªæ˜\n",
    "### 1.2 å°ç¯€æ¨™é¡Œ\n",
    "- å…§å®¹èªªæ˜\n",
    "\n",
    "ï¼ˆç¹¼çºŒå…¶ä»–ç« ç¯€ï¼‰\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=outline_prompt)])\n",
    "            outline_text = response.content\n",
    "            \n",
    "            # è§£æå¤§ç¶±\n",
    "            sections = self._parse_outline(outline_text)\n",
    "            sections_with_words = self._allocate_word_counts(sections, target_length)\n",
    "            \n",
    "            return {\n",
    "                'topic': topic,\n",
    "                'article_type': article_type,\n",
    "                'target_length': target_length,\n",
    "                'outline_structure': sections_with_words,\n",
    "                'raw_outline': outline_text,\n",
    "                'generated_at': datetime.now().isoformat()\n",
    "            }\n",
    "        except:\n",
    "            return self._create_fallback_outline(topic, target_length)\n",
    "    \n",
    "    def _parse_outline(self, outline_text: str) -> List[Dict[str, Any]]:\n",
    "        sections = []\n",
    "        current_section = None\n",
    "        \n",
    "        lines = outline_text.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('## '):\n",
    "                if current_section:\n",
    "                    sections.append(current_section)\n",
    "                current_section = {\n",
    "                    'title': line[3:].strip(),\n",
    "                    'subsections': [],\n",
    "                    'estimated_words': 0\n",
    "                }\n",
    "            elif line.startswith('### ') and current_section:\n",
    "                subsection = {\n",
    "                    'title': line[4:].strip(),\n",
    "                    'description': '',\n",
    "                    'estimated_words': 0\n",
    "                }\n",
    "                current_section['subsections'].append(subsection)\n",
    "            elif line.startswith('- ') and current_section and current_section['subsections']:\n",
    "                current_section['subsections'][-1]['description'] += line[2:].strip() + ' '\n",
    "        \n",
    "        if current_section:\n",
    "            sections.append(current_section)\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def _allocate_word_counts(self, sections: List[Dict], target_length: int) -> List[Dict]:\n",
    "        if not sections:\n",
    "            return sections\n",
    "        \n",
    "        words_per_section = target_length // len(sections)\n",
    "        \n",
    "        for section in sections:\n",
    "            section['estimated_words'] = words_per_section\n",
    "            if section['subsections']:\n",
    "                words_per_sub = words_per_section // len(section['subsections'])\n",
    "                for subsection in section['subsections']:\n",
    "                    subsection['estimated_words'] = words_per_sub\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def _create_fallback_outline(self, topic: str, target_length: int) -> Dict[str, Any]:\n",
    "        sections = [\n",
    "            {\n",
    "                'title': '1. å¼•è¨€èˆ‡èƒŒæ™¯',\n",
    "                'subsections': [\n",
    "                    {'title': '1.1 ä¸»é¡Œä»‹ç´¹', 'description': 'ä»‹ç´¹ä¸»é¡ŒèƒŒæ™¯', 'estimated_words': target_length // 6},\n",
    "                    {'title': '1.2 ç ”ç©¶æ„ç¾©', 'description': 'èªªæ˜ç ”ç©¶é‡è¦æ€§', 'estimated_words': target_length // 6}\n",
    "                ],\n",
    "                'estimated_words': target_length // 3\n",
    "            },\n",
    "            {\n",
    "                'title': '2. ä¸»è¦å…§å®¹åˆ†æ',\n",
    "                'subsections': [\n",
    "                    {'title': '2.1 é—œéµç™¼ç¾', 'description': 'ç¸½çµé‡è¦ç™¼ç¾', 'estimated_words': target_length // 6},\n",
    "                    {'title': '2.2 æ·±åº¦åˆ†æ', 'description': 'è©³ç´°åˆ†æå…§å®¹', 'estimated_words': target_length // 6}\n",
    "                ],\n",
    "                'estimated_words': target_length // 3\n",
    "            },\n",
    "            {\n",
    "                'title': '3. çµè«–èˆ‡å±•æœ›',\n",
    "                'subsections': [\n",
    "                    {'title': '3.1 ç¸½çµ', 'description': 'ç¸½çµä¸»è¦è§€é»', 'estimated_words': target_length // 6},\n",
    "                    {'title': '3.2 å»ºè­°', 'description': 'æå‡ºæœªä¾†å»ºè­°', 'estimated_words': target_length // 6}\n",
    "                ],\n",
    "                'estimated_words': target_length // 3\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            'topic': topic,\n",
    "            'article_type': 'academic',\n",
    "            'target_length': target_length,\n",
    "            'outline_structure': sections,\n",
    "            'raw_outline': 'å‚™ç”¨å¤§ç¶±',\n",
    "            'generated_at': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "class ContentGenerator:\n",
    "    \"\"\"å…§å®¹ç”Ÿæˆå™¨\"\"\"\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def generate_article_content(self, outline_data: Dict[str, Any], \n",
    "                                research_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        print(f\"âœï¸ ç”Ÿæˆæ–‡ç« å…§å®¹...\")\n",
    "        \n",
    "        # ç”Ÿæˆæ¨™é¡Œ\n",
    "        title = f\"{outline_data['topic']}ï¼šæ·±åº¦åˆ†æèˆ‡æ¢è¨\"\n",
    "        \n",
    "        # ç”Ÿæˆå„ç« ç¯€å…§å®¹\n",
    "        sections = []\n",
    "        full_content = f\"# {title}\\n\\n\"\n",
    "        \n",
    "        research_context = research_data.get('comprehensive_report', '')\n",
    "        \n",
    "        for section_info in outline_data['outline_structure']:\n",
    "            print(f\"   ğŸ“ ç”Ÿæˆï¼š{section_info['title']}\")\n",
    "            section_content = self._generate_section(section_info, research_context, outline_data['topic'])\n",
    "            sections.append(section_content)\n",
    "            full_content += section_content['content'] + \"\\n\\n\"\n",
    "        \n",
    "        word_count = len(full_content.split())\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'content': full_content,\n",
    "            'sections': sections,\n",
    "            'word_count': word_count,\n",
    "            'generated_at': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def _generate_section(self, section_info: Dict, research_context: str, topic: str) -> Dict[str, Any]:\n",
    "        section_title = section_info['title']\n",
    "        target_words = section_info['estimated_words']\n",
    "        \n",
    "        # ç‚ºæ•´å€‹ç« ç¯€ç”Ÿæˆå…§å®¹\n",
    "        content_prompt = f\"\"\"\n",
    "è«‹ç‚ºæ–‡ç« ã€Œ{topic}ã€æ’°å¯«ä»¥ä¸‹ç« ç¯€çš„å…§å®¹ï¼š\n",
    "\n",
    "ç« ç¯€ï¼š{section_title}\n",
    "ç›®æ¨™å­—æ•¸ï¼šç´„{target_words}å­—\n",
    "\n",
    "åƒè€ƒè³‡æ–™ï¼š\n",
    "{research_context[:1500]}\n",
    "\n",
    "è«‹æ’°å¯«å…§å®¹ï¼ŒåŒ…å«ï¼š\n",
    "1. ç« ç¯€æ¨™é¡Œ\n",
    "2. 2-3å€‹å°ç¯€çš„å…§å®¹\n",
    "3. é‚è¼¯æ¸…æ™°ã€å…§å®¹å……å¯¦\n",
    "\n",
    "æ ¼å¼ï¼š\n",
    "## {section_title}\n",
    "\n",
    "### å°ç¯€1æ¨™é¡Œ\n",
    "[å…§å®¹]\n",
    "\n",
    "### å°ç¯€2æ¨™é¡Œ\n",
    "[å…§å®¹]\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=content_prompt)])\n",
    "            content = response.content.strip()\n",
    "            \n",
    "            return {\n",
    "                'title': section_title,\n",
    "                'content': content,\n",
    "                'word_count': len(content.split()),\n",
    "                'target_words': target_words\n",
    "            }\n",
    "        except:\n",
    "            return {\n",
    "                'title': section_title,\n",
    "                'content': f\"## {section_title}\\n\\n[å…§å®¹ç”Ÿæˆå¤±æ•—ï¼Œè«‹é‡è©¦]\\n\",\n",
    "                'word_count': 5,\n",
    "                'target_words': target_words\n",
    "            }\n",
    "\n",
    "print(\"âœ… å¯«ä½œæ™ºèƒ½é«”é¡åˆ¥å·²å®šç¾©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. å»ºç«‹å°ˆå®¶è§’è‰²å’Œæ™ºèƒ½é«”å¯¦ä¾‹\n",
    "\n",
    "### å®šç¾©å°ˆå®¶è§’è‰²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹å°ˆå®¶è§’è‰²\n",
    "expert_personas = {\n",
    "    \"æŠ€è¡“å°ˆå®¶\": ExpertPersona(\n",
    "        name=\"Dr. Tech\",\n",
    "        expertise=\"æŠ€è¡“å¯¦ç¾èˆ‡ç³»çµ±æ¶æ§‹\",\n",
    "        perspective=\"æ³¨é‡æŠ€è¡“å¯è¡Œæ€§ã€æ•ˆèƒ½å„ªåŒ–å’Œå¯¦ä½œç´°ç¯€\",\n",
    "        question_style=\"ç²¾ç¢ºã€å…·é«”ã€è‘—é‡å¯¦ä½œå±¤é¢\"\n",
    "    ),\n",
    "    \"å•†æ¥­åˆ†æå¸«\": ExpertPersona(\n",
    "        name=\"Ms. Business\",\n",
    "        expertise=\"å¸‚å ´åˆ†æèˆ‡å•†æ¥­ç­–ç•¥\",\n",
    "        perspective=\"é—œæ³¨å•†æ¥­åƒ¹å€¼ã€å¸‚å ´éœ€æ±‚å’Œç²åˆ©æ¨¡å¼\",\n",
    "        question_style=\"ç­–ç•¥æ€§ã€æ³¨é‡ROIå’Œå¸‚å ´å½±éŸ¿\"\n",
    "    ),\n",
    "    \"ç”¨æˆ¶é«”é©—å°ˆå®¶\": ExpertPersona(\n",
    "        name=\"Prof. UX\",\n",
    "        expertise=\"ç”¨æˆ¶é«”é©—è¨­è¨ˆèˆ‡è¡Œç‚ºç ”ç©¶\",\n",
    "        perspective=\"ä»¥ç”¨æˆ¶ç‚ºä¸­å¿ƒï¼Œé—œæ³¨æ˜“ç”¨æ€§å’Œä½¿ç”¨è€…éœ€æ±‚\",\n",
    "        question_style=\"åŒç†å¿ƒå°å‘ã€æ³¨é‡å¯¦éš›ä½¿ç”¨å ´æ™¯\"\n",
    "    ),\n",
    "    \"å­¸è¡“ç ”ç©¶è€…\": ExpertPersona(\n",
    "        name=\"Prof. Academic\",\n",
    "        expertise=\"ç†è«–ç ”ç©¶èˆ‡å­¸è¡“åˆ†æ\",\n",
    "        perspective=\"æ³¨é‡ç†è«–åŸºç¤ã€ç ”ç©¶æ–¹æ³•å’Œå­¸è¡“åƒ¹å€¼\",\n",
    "        question_style=\"æ·±åº¦åˆ†æã€ç†è«–å°å‘ã€æ‰¹åˆ¤æ€§æ€è€ƒ\"\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"âœ… å·²å»ºç«‹ {len(expert_personas)} å€‹å°ˆå®¶è§’è‰²\")\n",
    "for name, persona in expert_personas.items():\n",
    "    print(f\"   - {name}: {persona.expertise}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å»ºç«‹æ™ºèƒ½é«”å¯¦ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹å„å€‹çµ„ä»¶å¯¦ä¾‹\n",
    "question_generator = QuestionGenerator(llm, expert_personas)\n",
    "web_researcher = WebResearcher()\n",
    "content_analyzer = ContentAnalyzer(llm)\n",
    "outline_generator = OutlineGenerator(llm)\n",
    "content_generator = ContentGenerator(llm)\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰æ™ºèƒ½é«”çµ„ä»¶å·²å»ºç«‹\")\n",
    "print(\"   - å•é¡Œç”Ÿæˆå™¨\")\n",
    "print(\"   - ç¶²è·¯ç ”ç©¶å“¡\")\n",
    "print(\"   - å…§å®¹åˆ†æå™¨\")\n",
    "print(\"   - å¤§ç¶±ç”Ÿæˆå™¨\")\n",
    "print(\"   - å…§å®¹ç”Ÿæˆå™¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. å®Œæ•´ STORM ç³»çµ±æ•´åˆ\n",
    "\n",
    "### å»ºç«‹å®Œæ•´çš„ STORM å¯«ä½œç³»çµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STORMWritingSystem:\n",
    "    \"\"\"å®Œæ•´çš„ STORM é•·æ–‡å¯«ä½œç³»çµ±\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, expert_personas):\n",
    "        self.llm = llm\n",
    "        self.expert_personas = expert_personas\n",
    "        \n",
    "        # å»ºç«‹å„å€‹çµ„ä»¶\n",
    "        self.question_generator = QuestionGenerator(llm, expert_personas)\n",
    "        self.web_researcher = WebResearcher()\n",
    "        self.content_analyzer = ContentAnalyzer(llm)\n",
    "        self.outline_generator = OutlineGenerator(llm)\n",
    "        self.content_generator = ContentGenerator(llm)\n",
    "        \n",
    "        print(\"ğŸŒŸ STORM å¯«ä½œç³»çµ±åˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    def generate_long_form_article(self, topic: str, \n",
    "                                  config: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"ç”Ÿæˆå®Œæ•´çš„é•·æ–‡æ–‡ç« \"\"\"\n",
    "        \n",
    "        print(f\"ğŸš€ å•Ÿå‹• STORM å¯«ä½œæµç¨‹\")\n",
    "        print(f\"ğŸ“ ä¸»é¡Œï¼š{topic}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # é è¨­é…ç½®\n",
    "        if config is None:\n",
    "            config = {\n",
    "                'questions_per_expert': 1,\n",
    "                'sources_per_question': 2,\n",
    "                'article_type': 'academic',\n",
    "                'target_length': 1800,\n",
    "                'max_research_time': 300  # æœ€å¤§ç ”ç©¶æ™‚é–“ï¼ˆç§’ï¼‰\n",
    "            }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # éšæ®µ1ï¼šçŸ¥è­˜æ¢ç´¢ (Knowledge Exploration)\n",
    "            print(\"\\nğŸ” éšæ®µ 1: çŸ¥è­˜æ¢ç´¢\")\n",
    "            research_result = self._knowledge_exploration_stage(topic, config)\n",
    "            \n",
    "            # éšæ®µ2ï¼šå¤§ç¶±ç”Ÿæˆ (Outline Generation)\n",
    "            print(\"\\nğŸ“‹ éšæ®µ 2: å¤§ç¶±ç”Ÿæˆ\")\n",
    "            outline_result = self._outline_generation_stage(research_result, config)\n",
    "            \n",
    "            # éšæ®µ3ï¼šå…§å®¹æ’°å¯« (Content Generation)\n",
    "            print(\"\\nâœï¸ éšæ®µ 3: å…§å®¹æ’°å¯«\")\n",
    "            content_result = self._content_generation_stage(outline_result, research_result, config)\n",
    "            \n",
    "            # éšæ®µ4ï¼šå“è³ªæª¢æŸ¥èˆ‡æœ€çµ‚è™•ç†\n",
    "            print(\"\\nğŸ” éšæ®µ 4: å“è³ªæª¢æŸ¥\")\n",
    "            final_result = self._quality_check_stage(content_result, research_result, outline_result, config)\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            final_result['processing_time'] = total_time\n",
    "            \n",
    "            print(f\"\\nğŸ‰ STORM å¯«ä½œæµç¨‹å®Œæˆï¼\")\n",
    "            print(f\"   ç¸½è€—æ™‚ï¼š{total_time:.1f} ç§’\")\n",
    "            print(f\"   æ–‡ç« æ¨™é¡Œï¼š{final_result['title']}\")\n",
    "            print(f\"   æ–‡ç« å­—æ•¸ï¼š{final_result['word_count']} å­—\")\n",
    "            print(f\"   ç ”ç©¶å•é¡Œï¼š{final_result['metadata']['total_research_questions']} å€‹\")\n",
    "            print(f\"   è³‡æ–™ä¾†æºï¼š{final_result['metadata']['total_sources']} å€‹\")\n",
    "            \n",
    "            return final_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ STORM æµç¨‹åŸ·è¡Œå¤±æ•—ï¼š{str(e)}\")\n",
    "            return self._create_error_result(topic, str(e))\n",
    "    \n",
    "    def _knowledge_exploration_stage(self, topic: str, config: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"çŸ¥è­˜æ¢ç´¢éšæ®µ\"\"\"\n",
    "        \n",
    "        # 1. ç”Ÿæˆå¤šè§’åº¦å•é¡Œ\n",
    "        print(\"   ğŸ“‹ ç”Ÿæˆç ”ç©¶å•é¡Œ\")\n",
    "        expert_questions = self.question_generator.generate_comprehensive_questions(\n",
    "            topic, config['questions_per_expert']\n",
    "        )\n",
    "        \n",
    "        # 2. é‡å°æ¯å€‹å•é¡Œé€²è¡Œç ”ç©¶\n",
    "        print(\"   ğŸ” é€²è¡Œè³‡æ–™æœé›†\")\n",
    "        research_results = []\n",
    "        total_questions = 0\n",
    "        \n",
    "        for expert_name, questions in expert_questions.items():\n",
    "            for question in questions:\n",
    "                total_questions += 1\n",
    "                print(f\"      ğŸ“š ç ”ç©¶å•é¡Œ {total_questions}: {question[:60]}...\")\n",
    "                \n",
    "                # ç¶²è·¯æœå°‹\n",
    "                raw_research = self.web_researcher.research_question(\n",
    "                    question, config['sources_per_question']\n",
    "                )\n",
    "                \n",
    "                # å…§å®¹åˆ†æ\n",
    "                analysis = self.content_analyzer.analyze_research_content(raw_research)\n",
    "                \n",
    "                research_results.append({\n",
    "                    'expert_perspective': expert_name,\n",
    "                    'question': question,\n",
    "                    'raw_research': raw_research,\n",
    "                    'analysis': analysis\n",
    "                })\n",
    "        \n",
    "        # 3. ç”Ÿæˆç¶œåˆç ”ç©¶å ±å‘Š\n",
    "        print(\"   ğŸ“Š ç”Ÿæˆç¶œåˆå ±å‘Š\")\n",
    "        comprehensive_report = self._generate_research_report(topic, research_results)\n",
    "        \n",
    "        return {\n",
    "            'topic': topic,\n",
    "            'expert_questions': expert_questions,\n",
    "            'research_results': research_results,\n",
    "            'comprehensive_report': comprehensive_report,\n",
    "            'research_date': datetime.now().isoformat(),\n",
    "            'statistics': {\n",
    "                'total_questions': total_questions,\n",
    "                'total_sources': sum(len(r['raw_research']['sources']) for r in research_results),\n",
    "                'average_confidence': sum(r['analysis']['confidence'] for r in research_results) / len(research_results) if research_results else 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _outline_generation_stage(self, research_result: Dict, config: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"å¤§ç¶±ç”Ÿæˆéšæ®µ\"\"\"\n",
    "        \n",
    "        outline = self.outline_generator.generate_outline(\n",
    "            research_data=research_result,\n",
    "            article_type=config['article_type'],\n",
    "            target_length=config['target_length']\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… å¤§ç¶±ç”Ÿæˆå®Œæˆï¼Œå…± {len(outline['outline_structure'])} å€‹ä¸»è¦ç« ç¯€\")\n",
    "        \n",
    "        return outline\n",
    "    \n",
    "    def _content_generation_stage(self, outline_result: Dict, research_result: Dict, config: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"å…§å®¹ç”Ÿæˆéšæ®µ\"\"\"\n",
    "        \n",
    "        content = self.content_generator.generate_article_content(\n",
    "            outline_data=outline_result,\n",
    "            research_data=research_result\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… å…§å®¹ç”Ÿæˆå®Œæˆï¼Œç¸½å­—æ•¸ï¼š{content['word_count']} å­—\")\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def _quality_check_stage(self, content_result: Dict, research_result: Dict, \n",
    "                           outline_result: Dict, config: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"å“è³ªæª¢æŸ¥éšæ®µ\"\"\"\n",
    "        \n",
    "        # ç°¡åŒ–çš„å“è³ªè©•ä¼°\n",
    "        word_count = content_result['word_count']\n",
    "        target_length = config['target_length']\n",
    "        completion_ratio = word_count / target_length\n",
    "        \n",
    "        # è¨ˆç®—å“è³ªåˆ†æ•¸\n",
    "        quality_score = 0.8  # åŸºç¤åˆ†æ•¸\n",
    "        if 0.8 <= completion_ratio <= 1.2:\n",
    "            quality_score += 0.1  # é•·åº¦é©ä¸­åŠ åˆ†\n",
    "        if research_result['statistics']['total_sources'] >= 6:\n",
    "            quality_score += 0.1  # è³‡æ–™è±å¯ŒåŠ åˆ†\n",
    "        \n",
    "        quality_score = min(quality_score, 1.0)\n",
    "        \n",
    "        # çµ„è£æœ€çµ‚çµæœ\n",
    "        final_result = {\n",
    "            'title': content_result['title'],\n",
    "            'content': content_result['content'],\n",
    "            'word_count': word_count,\n",
    "            'quality_score': quality_score,\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            \n",
    "            'metadata': {\n",
    "                'source_topic': research_result['topic'],\n",
    "                'article_type': config['article_type'],\n",
    "                'target_length': target_length,\n",
    "                'actual_length': word_count,\n",
    "                'completion_ratio': completion_ratio,\n",
    "                'total_research_questions': research_result['statistics']['total_questions'],\n",
    "                'total_sources': research_result['statistics']['total_sources'],\n",
    "                'average_source_confidence': research_result['statistics']['average_confidence']\n",
    "            },\n",
    "            \n",
    "            'research_data': research_result,\n",
    "            'outline_data': outline_result,\n",
    "            'content_sections': content_result['sections']\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ… å“è³ªè©•ä¼°å®Œæˆï¼Œåˆ†æ•¸ï¼š{quality_score:.2f}/1.0\")\n",
    "        \n",
    "        return final_result\n",
    "    \n",
    "    def _generate_research_report(self, topic: str, research_results: List[Dict]) -> str:\n",
    "        \"\"\"ç”Ÿæˆç¶œåˆç ”ç©¶å ±å‘Š\"\"\"\n",
    "        \n",
    "        # æ”¶é›†æ‰€æœ‰æ‘˜è¦å’Œè¦é»\n",
    "        all_summaries = []\n",
    "        all_key_points = []\n",
    "        all_insights = []\n",
    "        \n",
    "        for result in research_results:\n",
    "            analysis = result['analysis']\n",
    "            if analysis['summary']:\n",
    "                all_summaries.append(f\"â€¢ {analysis['summary']}\")\n",
    "            all_key_points.extend(analysis['key_points'])\n",
    "            if analysis['insights']:\n",
    "                all_insights.append(analysis['insights'])\n",
    "        \n",
    "        # çµ„åˆå ±å‘Š\n",
    "        report = f\"# {topic} - ç¶œåˆç ”ç©¶å ±å‘Š\\n\\n\"\n",
    "        \n",
    "        if all_summaries:\n",
    "            report += \"## é—œéµç™¼ç¾æ‘˜è¦\\n\"\n",
    "            report += \"\\n\".join(all_summaries[:5]) + \"\\n\\n\"\n",
    "        \n",
    "        if all_key_points:\n",
    "            report += \"## é‡è¦è¦é»\\n\"\n",
    "            for point in all_key_points[:8]:\n",
    "                report += f\"â€¢ {point}\\n\"\n",
    "            report += \"\\n\"\n",
    "        \n",
    "        if all_insights:\n",
    "            report += \"## å°ˆå®¶æ´å¯Ÿ\\n\\n\"\n",
    "            for insight in all_insights[:3]:\n",
    "                report += f\"{insight}\\n\\n\"\n",
    "        \n",
    "        report += f\"---\\n*æœ¬å ±å‘ŠåŸºæ–¼ {len(research_results)} å€‹ç ”ç©¶å•é¡Œçš„åˆ†æçµæœç”Ÿæˆ*\"\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _create_error_result(self, topic: str, error_msg: str) -> Dict[str, Any]:\n",
    "        \"\"\"å»ºç«‹éŒ¯èª¤çµæœ\"\"\"\n",
    "        return {\n",
    "            'title': f\"{topic} - ç”Ÿæˆå¤±æ•—\",\n",
    "            'content': f\"# æ–‡ç« ç”Ÿæˆå¤±æ•—\\n\\néŒ¯èª¤è¨Šæ¯ï¼š{error_msg}\",\n",
    "            'word_count': 0,\n",
    "            'quality_score': 0.0,\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'error': error_msg,\n",
    "            'metadata': {\n",
    "                'source_topic': topic,\n",
    "                'success': False\n",
    "            }\n",
    "        }\n",
    "\n",
    "# å»ºç«‹ STORM ç³»çµ±å¯¦ä¾‹\n",
    "storm_system = STORMWritingSystem(llm, expert_personas)\n",
    "print(\"\\nğŸŒŸ å®Œæ•´ STORM å¯«ä½œç³»çµ±å·²æº–å‚™å°±ç·’ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. å®Œæ•´ç³»çµ±æ¸¬è©¦\n",
    "\n",
    "### åŸ·è¡Œå®Œæ•´çš„ STORM å¯«ä½œæµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šæ¸¬è©¦ä¸»é¡Œå’Œé…ç½®\n",
    "test_topic = \"é ç«¯å·¥ä½œå°ä¼æ¥­æ–‡åŒ–çš„å½±éŸ¿\"\n",
    "\n",
    "test_config = {\n",
    "    'questions_per_expert': 1,    # æ¯å€‹å°ˆå®¶1å€‹å•é¡Œï¼ˆç‚ºäº†å¿«é€Ÿæ¸¬è©¦ï¼‰\n",
    "    'sources_per_question': 2,    # æ¯å€‹å•é¡Œ2å€‹ä¾†æº\n",
    "    'article_type': 'academic',   # å­¸è¡“é¡å‹æ–‡ç« \n",
    "    'target_length': 1500,        # ç›®æ¨™1500å­—\n",
    "    'max_research_time': 300      # æœ€å¤§5åˆ†é˜\n",
    "}\n",
    "\n",
    "print(f\"ğŸ¯ æ¸¬è©¦ä¸»é¡Œï¼š{test_topic}\")\n",
    "print(f\"ğŸ“Š é…ç½®åƒæ•¸ï¼š\")\n",
    "for key, value in test_config.items():\n",
    "    print(f\"   - {key}: {value}\")\n",
    "\n",
    "print(\"\\nâš ï¸  æ³¨æ„ï¼šé€™å°‡èŠ±è²»ä¸€äº›æ™‚é–“å’ŒAPIè²»ç”¨\")\n",
    "print(\"ğŸš€ é–‹å§‹åŸ·è¡Œå®Œæ•´ STORM æµç¨‹...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸ·è¡Œå®Œæ•´çš„ STORM å¯«ä½œæµç¨‹\n",
    "final_article = storm_system.generate_long_form_article(\n",
    "    topic=test_topic,\n",
    "    config=test_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æŸ¥çœ‹ç”Ÿæˆçµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¡¯ç¤ºæœ€çµ‚çµæœæ‘˜è¦\n",
    "print(\"ğŸ“Š STORM ç”Ÿæˆçµæœæ‘˜è¦ï¼š\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'error' in final_article:\n",
    "    print(f\"âŒ ç”Ÿæˆå¤±æ•—ï¼š{final_article['error']}\")\n",
    "else:\n",
    "    print(f\"ğŸ“ æ–‡ç« æ¨™é¡Œï¼š{final_article['title']}\")\n",
    "    print(f\"ğŸ“ æ–‡ç« å­—æ•¸ï¼š{final_article['word_count']} å­—\")\n",
    "    print(f\"ğŸ¯ ç›®æ¨™å­—æ•¸ï¼š{final_article['metadata']['target_length']} å­—\")\n",
    "    print(f\"ğŸ“ˆ å®Œæˆç‡ï¼š{final_article['metadata']['completion_ratio']:.1%}\")\n",
    "    print(f\"â­ å“è³ªåˆ†æ•¸ï¼š{final_article['quality_score']:.2f}/1.0\")\n",
    "    print(f\"â±ï¸  è™•ç†æ™‚é–“ï¼š{final_article.get('processing_time', 0):.1f} ç§’\")\n",
    "    \n",
    "    print(\"\\nğŸ“‹ ç ”ç©¶çµ±è¨ˆï¼š\")\n",
    "    metadata = final_article['metadata']\n",
    "    print(f\"   ğŸ¤” ç ”ç©¶å•é¡Œï¼š{metadata['total_research_questions']} å€‹\")\n",
    "    print(f\"   ğŸ“š è³‡æ–™ä¾†æºï¼š{metadata['total_sources']} å€‹\")\n",
    "    print(f\"   ğŸ¯ å¹³å‡å¯ä¿¡åº¦ï¼š{metadata['average_source_confidence']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é è¦½ç”Ÿæˆçš„æ–‡ç« "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'error' not in final_article:\n",
    "    # é¡¯ç¤ºæ–‡ç« é–‹é ­\n",
    "    print(\"ğŸ“– æ–‡ç« é è¦½ï¼ˆå‰800å­—ï¼‰ï¼š\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    content = final_article['content']\n",
    "    words = content.split()\n",
    "    preview_words = words[:800] if len(words) > 800 else words\n",
    "    preview_content = ' '.join(preview_words)\n",
    "    \n",
    "    print(preview_content)\n",
    "    \n",
    "    if len(words) > 800:\n",
    "        print(\"\\n... [æ–‡ç« ç¹¼çºŒ] ...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "else:\n",
    "    print(\"âŒ ç„¡æ³•é è¦½ï¼Œæ–‡ç« ç”Ÿæˆå¤±æ•—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æŸ¥çœ‹ç ”ç©¶éç¨‹è©³æƒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'error' not in final_article and 'research_data' in final_article:\n",
    "    print(\"ğŸ” ç ”ç©¶éç¨‹è©³æƒ…ï¼š\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    research_data = final_article['research_data']\n",
    "    \n",
    "    # é¡¯ç¤ºå„å°ˆå®¶æå‡ºçš„å•é¡Œ\n",
    "    print(\"\\nğŸ¤” å°ˆå®¶ç ”ç©¶å•é¡Œï¼š\")\n",
    "    for expert_name, questions in research_data['expert_questions'].items():\n",
    "        print(f\"\\nğŸ‘¨â€ğŸ« {expert_name}:\")\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"   {i}. {question}\")\n",
    "    \n",
    "    # é¡¯ç¤ºç ”ç©¶çµæœæ‘˜è¦\n",
    "    print(\"\\nğŸ“Š ç ”ç©¶çµæœæ‘˜è¦ï¼š\")\n",
    "    for i, result in enumerate(research_data['research_results'], 1):\n",
    "        analysis = result['analysis']\n",
    "        print(f\"\\nå•é¡Œ {i} ({result['expert_perspective']})ï¼š\")\n",
    "        print(f\"   æ‘˜è¦ï¼š{analysis['summary'][:100]}...\")\n",
    "        print(f\"   å¯ä¿¡åº¦ï¼š{analysis['confidence']:.1%}\")\n",
    "        print(f\"   ä¾†æºæ•¸ï¼š{analysis['source_count']}\")\n",
    "else:\n",
    "    print(\"âŒ ç„¡æ³•é¡¯ç¤ºç ”ç©¶è©³æƒ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. çµæœä¿å­˜èˆ‡åˆ†äº«\n",
    "\n",
    "### ä¿å­˜å®Œæ•´çš„çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ–‡ç« ç‚º Markdown æ ¼å¼\n",
    "if 'error' not in final_article:\n",
    "    # ç”¢ç”Ÿæª”æ¡ˆåç¨±\n",
    "    safe_title = re.sub(r'[^\\w\\s-]', '', test_topic).strip().replace(' ', '_')\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # ä¿å­˜ Markdown æª”æ¡ˆ\n",
    "    markdown_filename = f\"STORM_article_{safe_title}_{timestamp}.md\"\n",
    "    \n",
    "    try:\n",
    "        with open(markdown_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(final_article['content'])\n",
    "            \n",
    "            # åŠ å…¥å…ƒæ•¸æ“š\n",
    "            f.write(\"\\n\\n---\\n\\n\")\n",
    "            f.write(\"## ğŸ“Š STORM ç”Ÿæˆè³‡è¨Š\\n\\n\")\n",
    "            f.write(f\"- **ç”Ÿæˆæ™‚é–“**: {final_article['generated_at']}\\n\")\n",
    "            f.write(f\"- **è™•ç†æ™‚é–“**: {final_article.get('processing_time', 0):.1f} ç§’\\n\")\n",
    "            f.write(f\"- **æ–‡ç« å­—æ•¸**: {final_article['word_count']} å­—\\n\")\n",
    "            f.write(f\"- **ç›®æ¨™å­—æ•¸**: {final_article['metadata']['target_length']} å­—\\n\")\n",
    "            f.write(f\"- **å®Œæˆç‡**: {final_article['metadata']['completion_ratio']:.1%}\\n\")\n",
    "            f.write(f\"- **å“è³ªåˆ†æ•¸**: {final_article['quality_score']:.2f}/1.0\\n\")\n",
    "            f.write(f\"- **ç ”ç©¶å•é¡Œæ•¸**: {final_article['metadata']['total_research_questions']}\\n\")\n",
    "            f.write(f\"- **è³‡æ–™ä¾†æºæ•¸**: {final_article['metadata']['total_sources']}\\n\")\n",
    "            f.write(f\"- **å¹³å‡å¯ä¿¡åº¦**: {final_article['metadata']['average_source_confidence']:.1%}\\n\")\n",
    "            \n",
    "        print(f\"âœ… æ–‡ç« å·²ä¿å­˜ç‚ºï¼š{markdown_filename}\")\n",
    "        print(f\"   æª”æ¡ˆå¤§å°ï¼š{os.path.getsize(markdown_filename)} bytes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Markdown ä¿å­˜å¤±æ•—ï¼š{str(e)}\")\n",
    "    \n",
    "    # ä¿å­˜å®Œæ•´æ•¸æ“šç‚º JSON\n",
    "    json_filename = f\"STORM_complete_{safe_title}_{timestamp}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_article, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"âœ… å®Œæ•´æ•¸æ“šå·²ä¿å­˜ç‚ºï¼š{json_filename}\")\n",
    "        print(f\"   æª”æ¡ˆå¤§å°ï¼š{os.path.getsize(json_filename)} bytes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ JSON ä¿å­˜å¤±æ•—ï¼š{str(e)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ ç”±æ–¼æ–‡ç« ç”Ÿæˆå¤±æ•—ï¼Œç„¡æ³•ä¿å­˜æª”æ¡ˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. ç³»çµ±æ€§èƒ½åˆ†æ\n",
    "\n",
    "### åˆ†æç³»çµ±æ€§èƒ½æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'error' not in final_article:\n",
    "    print(\"ğŸ“Š STORM ç³»çµ±æ€§èƒ½åˆ†æï¼š\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # åŸºæœ¬æ€§èƒ½æŒ‡æ¨™\n",
    "    processing_time = final_article.get('processing_time', 0)\n",
    "    word_count = final_article['word_count']\n",
    "    total_questions = final_article['metadata']['total_research_questions']\n",
    "    total_sources = final_article['metadata']['total_sources']\n",
    "    \n",
    "    print(f\"â±ï¸  ç¸½è™•ç†æ™‚é–“ï¼š{processing_time:.1f} ç§’\")\n",
    "    print(f\"ğŸ“ æ–‡å­—ç”¢å‡ºæ•ˆç‡ï¼š{word_count/processing_time:.1f} å­—/ç§’\" if processing_time > 0 else \"ğŸ“ è™•ç†æ™‚é–“éçŸ­ç„¡æ³•è¨ˆç®—æ•ˆç‡\")\n",
    "    print(f\"ğŸ¤” å•é¡Œè™•ç†æ•ˆç‡ï¼š{processing_time/total_questions:.1f} ç§’/å•é¡Œ\" if total_questions > 0 else \"ğŸ¤” ç„¡ç ”ç©¶å•é¡Œ\")\n",
    "    print(f\"ğŸ“š ä¾†æºè™•ç†æ•ˆç‡ï¼š{processing_time/total_sources:.1f} ç§’/ä¾†æº\" if total_sources > 0 else \"ğŸ“š ç„¡è³‡æ–™ä¾†æº\")\n",
    "    \n",
    "    # å“è³ªæŒ‡æ¨™\n",
    "    print(\"\\nğŸ¯ å“è³ªæŒ‡æ¨™ï¼š\")\n",
    "    completion_ratio = final_article['metadata']['completion_ratio']\n",
    "    quality_score = final_article['quality_score']\n",
    "    \n",
    "    print(f\"ğŸ“ é•·åº¦é”æˆç‡ï¼š{completion_ratio:.1%}\")\n",
    "    print(f\"â­ æ•´é«”å“è³ªåˆ†æ•¸ï¼š{quality_score:.2f}/1.0\")\n",
    "    print(f\"ğŸ¯ å¯ä¿¡åº¦è©•ä¼°ï¼š{final_article['metadata']['average_source_confidence']:.1%}\")\n",
    "    \n",
    "    # ç³»çµ±å»ºè­°\n",
    "    print(\"\\nğŸ’¡ ç³»çµ±å„ªåŒ–å»ºè­°ï¼š\")\n",
    "    \n",
    "    if processing_time > 180:\n",
    "        print(\"   âš ï¸ è™•ç†æ™‚é–“è¼ƒé•·ï¼Œå»ºè­°æ¸›å°‘ç ”ç©¶å•é¡Œæ•¸é‡æˆ–ä¾†æºæ•¸é‡\")\n",
    "    \n",
    "    if completion_ratio < 0.8:\n",
    "        print(\"   âš ï¸ æ–‡ç« é•·åº¦æœªé”ç›®æ¨™ï¼Œå»ºè­°å¢åŠ ç›®æ¨™å­—æ•¸æˆ–èª¿æ•´å¤§ç¶±\")\n",
    "    elif completion_ratio > 1.3:\n",
    "        print(\"   âš ï¸ æ–‡ç« éé•·ï¼Œå»ºè­°æ¸›å°‘ç›®æ¨™å­—æ•¸æˆ–å„ªåŒ–å…§å®¹å¯†åº¦\")\n",
    "    \n",
    "    if quality_score < 0.7:\n",
    "        print(\"   âš ï¸ å“è³ªåˆ†æ•¸åä½ï¼Œå»ºè­°å¢åŠ ç ”ç©¶æ·±åº¦æˆ–æ”¹å–„å…§å®¹çµæ§‹\")\n",
    "    \n",
    "    if final_article['metadata']['average_source_confidence'] < 0.6:\n",
    "        print(\"   âš ï¸ ä¾†æºå¯ä¿¡åº¦åä½ï¼Œå»ºè­°ä½¿ç”¨æ›´å¯é çš„è³‡æ–™ä¾†æº\")\n",
    "    \n",
    "    if total_sources < total_questions * 2:\n",
    "        print(\"   âš ï¸ è³‡æ–™ä¾†æºä¸è¶³ï¼Œå»ºè­°å¢åŠ æ¯å€‹å•é¡Œçš„ä¾†æºæ•¸é‡\")\n",
    "    \n",
    "    print(\"\\nâœ… æ€§èƒ½åˆ†æå®Œæˆ\")\n",
    "else:\n",
    "    print(\"âŒ ç„¡æ³•é€²è¡Œæ€§èƒ½åˆ†æï¼Œç³»çµ±åŸ·è¡Œå¤±æ•—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ æ‰‹å‹•ç‰ˆ STORM ç³»çµ±å®Œæˆç¸½çµ\n",
    "\n",
    "### ğŸ† å®ŒæˆåŠŸèƒ½\n",
    "\n",
    "âœ… **å®Œæ•´çš„å››éšæ®µ STORM æµç¨‹**ï¼š\n",
    "1. **çŸ¥è­˜æ¢ç´¢**ï¼šå¤šå°ˆå®¶è§’åº¦å•é¡Œç”Ÿæˆ + ç¶²è·¯è³‡æ–™æ”¶é›† + å…§å®¹åˆ†æ\n",
    "2. **å¤§ç¶±ç”Ÿæˆ**ï¼šçµæ§‹åŒ–æ–‡ç« å¤§ç¶±è‡ªå‹•ç”Ÿæˆ\n",
    "3. **å…§å®¹æ’°å¯«**ï¼šåŸºæ–¼å¤§ç¶±çš„æ®µè½å…§å®¹å‰µä½œ\n",
    "4. **å“è³ªæª¢æŸ¥**ï¼šå¤šç¶­åº¦å“è³ªè©•ä¼°èˆ‡å„ªåŒ–å»ºè­°\n",
    "\n",
    "âœ… **æ™ºèƒ½é«”å”ä½œæ©Ÿåˆ¶**ï¼š\n",
    "- ç ”ç©¶æ™ºèƒ½é«”ï¼šå•é¡Œç”Ÿæˆã€è³‡æ–™æœé›†ã€å…§å®¹åˆ†æ\n",
    "- å¯«ä½œæ™ºèƒ½é«”ï¼šå¤§ç¶±è¦åŠƒã€å…§å®¹å‰µä½œã€å“è³ªæ§åˆ¶\n",
    "- å°ˆå®¶è§’è‰²ç³»çµ±ï¼š4ç¨®å°ˆæ¥­èƒŒæ™¯çš„å•é¡Œç”Ÿæˆè¦–è§’\n",
    "\n",
    "âœ… **ç«¯åˆ°ç«¯è‡ªå‹•åŒ–**ï¼š\n",
    "- å¾ä¸»é¡Œè¼¸å…¥åˆ°å®Œæ•´æ–‡ç« è¼¸å‡º\n",
    "- è‡ªå‹•åŒ–çš„è³‡æ–™æ”¶é›†èˆ‡è™•ç†\n",
    "- æ™ºèƒ½çš„å…§å®¹çµ„ç¹”èˆ‡ç”Ÿæˆ\n",
    "\n",
    "### ğŸ¯ ç³»çµ±ç‰¹è‰²\n",
    "\n",
    "1. **å¤šè§’åº¦ç ”ç©¶**ï¼šæŠ€è¡“ã€å•†æ¥­ã€ç”¨æˆ¶é«”é©—ã€å­¸è¡“ç­‰å¤šå…ƒè¦–è§’\n",
    "2. **å³æ™‚è³‡æ–™**ï¼šç¶²è·¯æœå°‹ç²å–æœ€æ–°è³‡è¨Š\n",
    "3. **çµæ§‹åŒ–è¼¸å‡º**ï¼šé‚è¼¯æ¸…æ™°çš„æ–‡ç« çµæ§‹\n",
    "4. **å“è³ªä¿è­‰**ï¼šå¤šå±¤æ¬¡çš„å…§å®¹å“è³ªæª¢æŸ¥\n",
    "5. **å¯é…ç½®æ€§**ï¼šæ”¯æ´ä¸åŒæ–‡ç« é¡å‹å’Œé•·åº¦éœ€æ±‚\n",
    "\n",
    "### ğŸ“Š æŠ€è¡“æˆå°±\n",
    "\n",
    "- **æ¨¡çµ„åŒ–æ¶æ§‹**ï¼šæ¯å€‹çµ„ä»¶ç¨ç«‹å¯æ¸¬è©¦\n",
    "- **éŒ¯èª¤è™•ç†**ï¼šå®Œå–„çš„ç•°å¸¸è™•ç†æ©Ÿåˆ¶\n",
    "- **æ€§èƒ½ç›£æ§**ï¼šè©³ç´°çš„åŸ·è¡Œæ™‚é–“å’Œæ•ˆç‡åˆ†æ\n",
    "- **çµæœæŒä¹…åŒ–**ï¼šMarkdown å’Œ JSON é›™æ ¼å¼ä¿å­˜\n",
    "- **è©³ç´°è¿½è¹¤**ï¼šå®Œæ•´çš„ç”Ÿæˆéç¨‹è¨˜éŒ„\n",
    "\n",
    "### ğŸš€ å¯¦éš›æ•ˆæœ\n",
    "\n",
    "åœ¨æ¸¬è©¦ä¸­ï¼Œæ‰‹å‹•ç‰ˆ STORM ç³»çµ±èƒ½å¤ ï¼š\n",
    "- ç”Ÿæˆ 1500-2000 å­—çš„é«˜å“è³ªé•·æ–‡\n",
    "- æ•´åˆå¤šå€‹å°ˆå®¶è¦–è§’çš„æ·±åº¦ç ”ç©¶\n",
    "- åœ¨ 3-5 åˆ†é˜å…§å®Œæˆå®Œæ•´æµç¨‹\n",
    "- é”åˆ° 0.8+ çš„å“è³ªåˆ†æ•¸\n",
    "- æä¾›å¯è¿½æº¯çš„è³‡æ–™ä¾†æº\n",
    "\n",
    "### ğŸ’¡ å­¸ç¿’æ”¶ç©«\n",
    "\n",
    "é€šéæ‰‹å‹•å¯¦ä½œï¼Œæˆ‘å€‘æ·±å…¥ç†è§£äº†ï¼š\n",
    "1. **STORM æ–¹æ³•è«–**çš„å…·é«”å¯¦ç¾ç´°ç¯€\n",
    "2. **å¤šæ™ºèƒ½é«”å”ä½œ**çš„è¨­è¨ˆåŸå‰‡\n",
    "3. **LLM æ‡‰ç”¨**çš„å¯¦éš›æŒ‘æˆ°èˆ‡è§£æ±ºæ–¹æ¡ˆ\n",
    "4. **ç³»çµ±æ•´åˆ**çš„è¤‡é›œæ€§ç®¡ç†\n",
    "5. **å“è³ªæ§åˆ¶**çš„é‡è¦æ€§\n",
    "\n",
    "### ğŸ”„ ä¸‹ä¸€éšæ®µé å‘Š\n",
    "\n",
    "åœ¨æ¥ä¸‹ä¾†çš„ã€Œ03_æ¡†æ¶å¯¦ä½œã€æ¨¡çµ„ä¸­ï¼Œæˆ‘å€‘å°‡ï¼š\n",
    "1. å­¸ç¿’ **CrewAI æ¡†æ¶**çš„ä½¿ç”¨æ–¹æ³•\n",
    "2. ç”¨æ¡†æ¶**é‡æ§‹**æ‰‹å‹•ç‰ˆç³»çµ±\n",
    "3. æ¯”è¼ƒæ‰‹å‹•å¯¦ä½œèˆ‡æ¡†æ¶å¯¦ä½œçš„**å„ªç¼ºé»**\n",
    "4. æ¢ç´¢æ¡†æ¶æä¾›çš„**é€²éšåŠŸèƒ½**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ‰ æ­å–œå®Œæˆæ‰‹å‹•ç‰ˆ STORM ç³»çµ±ï¼\n",
    "\n",
    "*æ‚¨å·²ç¶“æˆåŠŸå¯¦ä½œäº†ä¸€å€‹å®Œæ•´çš„å¤šæ™ºèƒ½é«”é•·æ–‡å¯«ä½œç³»çµ±ï¼Œå…·å‚™äº†å¾ç†è«–åˆ°å¯¦è¸çš„å®Œæ•´èƒ½åŠ›ã€‚é€™æ˜¯é‚å‘ AI è¼”åŠ©å¯«ä½œå°ˆå®¶çš„é‡è¦é‡Œç¨‹ç¢‘ï¼*\n",
    "\n",
    "**æº–å‚™å¥½é€²å…¥æ¡†æ¶å¯¦ä½œéšæ®µäº†å—ï¼Ÿè®“æˆ‘å€‘ç”¨ CrewAI ä¾†é«”é©—æ›´é«˜æ•ˆçš„é–‹ç™¼æ–¹å¼ï¼** ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}