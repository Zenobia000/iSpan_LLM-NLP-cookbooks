{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03_æ‰‹å‹•ç‰ˆ_å¯«ä½œæ™ºèƒ½é«”\n",
    "\n",
    "## ğŸ¯ å­¸ç¿’ç›®æ¨™\n",
    "- å¯¦ä½œ STORM ç¬¬äºŒã€ä¸‰éšæ®µï¼šå¤§ç¶±ç”Ÿæˆèˆ‡å…§å®¹æ’°å¯«\n",
    "- é–‹ç™¼çµæ§‹åŒ–å¤§ç¶±ç”ŸæˆåŠŸèƒ½\n",
    "- å»ºç«‹æ®µè½å…§å®¹å‰µä½œæ©Ÿåˆ¶\n",
    "- å¯¦ç¾é¢¨æ ¼ä¸€è‡´æ€§æ§åˆ¶\n",
    "- æ•´åˆå¼•ç”¨èˆ‡ä¾†æºæ¨™è¨»\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒæº–å‚™\n",
    "\n",
    "### è¼‰å…¥å¿…è¦å¥—ä»¶èˆ‡å‰ç½®è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¤å¥—ä»¶\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# LangChain ç›¸é—œ\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# ç’°å¢ƒè®Šæ•¸\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# åˆå§‹åŒ– LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒæº–å‚™å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¼‰å…¥ç ”ç©¶çµæœ (æ¨¡æ“¬æ•¸æ“š)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ“¬ç ”ç©¶çµæœæ•¸æ“šçµæ§‹\n",
    "sample_research_data = {\n",
    "    \"topic\": \"ChatGPTå°å‚³çµ±æ•™è‚²æ¨¡å¼çš„å½±éŸ¿\",\n",
    "    \"research_date\": datetime.now().isoformat(),\n",
    "    \"expert_questions\": {\n",
    "        \"æŠ€è¡“å°ˆå®¶\": [\"ChatGPTçš„æŠ€è¡“æ¶æ§‹å¦‚ä½•æ”¯æ´æ•™è‚²æ‡‰ç”¨ï¼Ÿ\"],\n",
    "        \"å•†æ¥­åˆ†æå¸«\": [\"ChatGPTåœ¨æ•™è‚²å¸‚å ´çš„å•†æ¥­æ¨¡å¼æœ‰å“ªäº›ï¼Ÿ\"],\n",
    "        \"ç”¨æˆ¶é«”é©—å°ˆå®¶\": [\"å­¸ç”Ÿä½¿ç”¨ChatGPTå­¸ç¿’çš„é«”é©—å¦‚ä½•ï¼Ÿ\"],\n",
    "        \"å­¸è¡“ç ”ç©¶è€…\": [\"ChatGPTå°å­¸ç¿’æˆæ•ˆçš„å½±éŸ¿æœ‰å“ªäº›ç ”ç©¶è­‰æ“šï¼Ÿ\"],\n",
    "        \"æ”¿ç­–åˆ¶å®šè€…\": [\"ä½¿ç”¨ChatGPTåœ¨æ•™è‚²ä¸­éœ€è¦å“ªäº›æ”¿ç­–è¦ç¯„ï¼Ÿ\"]\n",
    "    },\n",
    "    \"comprehensive_report\": \"\"\"\n",
    "# ChatGPTå°å‚³çµ±æ•™è‚²æ¨¡å¼çš„å½±éŸ¿ - ç¶œåˆç ”ç©¶å ±å‘Š\n",
    "\n",
    "## é—œéµç™¼ç¾æ‘˜è¦\n",
    "â€¢ ChatGPTç‚ºå€‹æ€§åŒ–å­¸ç¿’æä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼Œèƒ½å¤ æ ¹æ“šå­¸ç”Ÿéœ€æ±‚èª¿æ•´æ•™å­¸å…§å®¹\n",
    "â€¢ æŠ€è¡“ä¸ŠChatGPTå…·å‚™24/7å¯ç”¨æ€§ï¼Œèƒ½å¤ å³æ™‚å›ç­”å­¸ç”Ÿå•é¡Œä¸¦æä¾›å­¸ç¿’æ”¯æ´\n",
    "â€¢ å•†æ¥­æ¨¡å¼æ­£åœ¨å¿«é€Ÿç™¼å±•ï¼ŒåŒ…æ‹¬è¨‚é–±åˆ¶æ•™è‚²å¹³å°å’Œå®¢è£½åŒ–æ•™å­¸è§£æ±ºæ–¹æ¡ˆ\n",
    "â€¢ å­¸ç”Ÿæ™®éå°AIè¼”åŠ©å­¸ç¿’è¡¨ç¾å‡ºç©æ¥µæ…‹åº¦ï¼Œä½†ä¹Ÿå­˜åœ¨å°éåº¦ä¾è³´çš„æ“”æ†‚\n",
    "\n",
    "## é‡è¦è¦é»\n",
    "â€¢ æå‡å­¸ç¿’æ•ˆç‡ï¼šå³æ™‚å›ç­”å•é¡Œï¼Œæ¸›å°‘ç­‰å¾…æ™‚é–“\n",
    "â€¢ å€‹æ€§åŒ–æ•™å­¸ï¼šæ ¹æ“šå­¸ç”Ÿç¨‹åº¦èª¿æ•´è§£é‡‹æ–¹å¼\n",
    "â€¢ å‰µæ„æ¿€ç™¼ï¼šå¹«åŠ©å­¸ç”Ÿè…¦åŠ›æ¿€ç›ªå’Œå‰µæ„å¯«ä½œ\n",
    "â€¢ èªè¨€å­¸ç¿’ï¼šæä¾›å¤šèªè¨€å°è©±ç·´ç¿’æ©Ÿæœƒ\n",
    "â€¢ æ‰¹åˆ¤æ€ç¶­ï¼šéœ€è¦åŸ¹é¤Šå­¸ç”Ÿé©—è­‰AIå›ç­”çš„èƒ½åŠ›\n",
    "â€¢ å€«ç†è€ƒé‡ï¼šé¿å…å­¸è¡“ä¸èª å¯¦å’Œéåº¦ä¾è³´\n",
    "â€¢ æ•™å¸«è§’è‰²è½‰è®Šï¼šå¾çŸ¥è­˜å‚³æˆè€…è®Šæˆå­¸ç¿’å¼•å°è€…\n",
    "â€¢ è©•ä¼°æ–¹å¼æ”¹è®Šï¼šéœ€è¦é‡æ–°è¨­è¨ˆè€ƒè©•æ–¹æ³•\n",
    "\n",
    "## å°ˆå®¶æ´å¯Ÿ\n",
    "\n",
    "å¾æŠ€è¡“è§’åº¦ä¾†çœ‹ï¼ŒChatGPTçš„transformeræ¶æ§‹ä½¿å…¶èƒ½å¤ ç†è§£ä¸Šä¸‹æ–‡ä¸¦ç”Ÿæˆé€£è²«çš„å›æ‡‰ï¼Œé€™ç‚ºæ•™è‚²æ‡‰ç”¨æä¾›äº†å¼·å¤§çš„åŸºç¤ã€‚å…¶å¤šæ¨¡æ…‹èƒ½åŠ›çš„ç™¼å±•å°‡é€²ä¸€æ­¥æ“´å±•æ•™è‚²æ‡‰ç”¨å ´æ™¯ã€‚\n",
    "\n",
    "å¾å•†æ¥­è§’åº¦åˆ†æï¼Œæ•™è‚²ç§‘æŠ€å¸‚å ´æ­£åœ¨å¿«é€Ÿæ“´å¼µï¼ŒChatGPTç›¸é—œçš„æ•™è‚²æ‡‰ç”¨é è¨ˆå°‡å¸¶ä¾†æ•¸åå„„ç¾å…ƒçš„å¸‚å ´æ©Ÿæœƒã€‚è¨‚é–±åˆ¶å’Œä¼æ¥­ç´šè§£æ±ºæ–¹æ¡ˆå°‡æˆç‚ºä¸»è¦å•†æ¥­æ¨¡å¼ã€‚\n",
    "\n",
    "å¾æ”¿ç­–è§’åº¦è€ƒé‡ï¼Œéœ€è¦å»ºç«‹æ˜ç¢ºçš„AIä½¿ç”¨æŒ‡å¼•ï¼Œç¢ºä¿å­¸è¡“èª ä¿¡ï¼ŒåŒæ™‚é¿å…æ“´å¤§æ•¸ä½è½å·®ã€‚éš±ç§ä¿è­·å’Œæ•¸æ“šå®‰å…¨ä¹Ÿæ˜¯é‡è¦è­°é¡Œã€‚\n",
    "\"\"\",\n",
    "    \"statistics\": {\n",
    "        \"total_questions\": 5,\n",
    "        \"total_sources\": 10,\n",
    "        \"average_confidence\": 0.85\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… æ¨¡æ“¬ç ”ç©¶æ•¸æ“šå·²è¼‰å…¥\")\n",
    "print(f\"ä¸»é¡Œï¼š{sample_research_data['topic']}\")\n",
    "print(f\"å•é¡Œæ•¸ï¼š{sample_research_data['statistics']['total_questions']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. å¤§ç¶±ç”Ÿæˆå™¨\n",
    "\n",
    "### å¯¦ä½œçµæ§‹åŒ–å¤§ç¶±ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlineGenerator:\n",
    "    \"\"\"å¤§ç¶±ç”Ÿæˆå™¨ - å°‡ç ”ç©¶çµæœè½‰æ›ç‚ºçµæ§‹åŒ–å¤§ç¶±\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def generate_outline(self, research_data: Dict[str, Any], \n",
    "                        article_type: str = \"academic\", \n",
    "                        target_length: int = 3000) -> Dict[str, Any]:\n",
    "        \"\"\"ç”Ÿæˆæ–‡ç« å¤§ç¶±\"\"\"\n",
    "        \n",
    "        print(f\"ğŸ“‹ ç”Ÿæˆå¤§ç¶±ï¼š{research_data['topic']}\")\n",
    "        print(f\"   æ–‡ç« é¡å‹ï¼š{article_type}\")\n",
    "        print(f\"   ç›®æ¨™é•·åº¦ï¼š{target_length} å­—\")\n",
    "        \n",
    "        # å»ºç«‹å¤§ç¶±ç”Ÿæˆæç¤ºè©\n",
    "        outline_prompt = self._create_outline_prompt(\n",
    "            research_data, article_type, target_length\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=outline_prompt)])\n",
    "            outline_text = response.content\n",
    "            \n",
    "            # è§£æå¤§ç¶±çµæ§‹\n",
    "            parsed_outline = self._parse_outline(outline_text)\n",
    "            \n",
    "            # è¨ˆç®—å„éƒ¨åˆ†å­—æ•¸åˆ†é…\n",
    "            outline_with_allocation = self._allocate_word_counts(parsed_outline, target_length)\n",
    "            \n",
    "            result = {\n",
    "                'topic': research_data['topic'],\n",
    "                'article_type': article_type,\n",
    "                'target_length': target_length,\n",
    "                'outline_structure': outline_with_allocation,\n",
    "                'raw_outline': outline_text,\n",
    "                'generated_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… å¤§ç¶±ç”Ÿæˆå®Œæˆï¼Œå…± {len(outline_with_allocation)} å€‹ä¸»è¦ç« ç¯€\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å¤§ç¶±ç”Ÿæˆå¤±æ•—ï¼š{str(e)}\")\n",
    "            return self._create_fallback_outline(research_data, target_length)\n",
    "    \n",
    "    def _create_outline_prompt(self, research_data: Dict, article_type: str, target_length: int) -> str:\n",
    "        \"\"\"å»ºç«‹å¤§ç¶±ç”Ÿæˆæç¤ºè©\"\"\"\n",
    "        \n",
    "        # æå–é—œéµä¿¡æ¯\n",
    "        topic = research_data['topic']\n",
    "        report = research_data['comprehensive_report']\n",
    "        \n",
    "        # æ ¹æ“šæ–‡ç« é¡å‹èª¿æ•´è¦æ±‚\n",
    "        type_requirements = {\n",
    "            \"academic\": \"å­¸è¡“æ€§ã€åš´è¬¹ã€æœ‰é‚è¼¯å±¤æ¬¡ã€åŒ…å«å¼•è¨€å’Œçµè«–\",\n",
    "            \"blog\": \"æ˜“è®€ã€æœ‰å¸å¼•åŠ›ã€åŒ…å«å¯¦ä¾‹ã€é©åˆä¸€èˆ¬è®€è€…\",\n",
    "            \"report\": \"å°ˆæ¥­ã€çµæ§‹å®Œæ•´ã€åŒ…å«æ‘˜è¦å’Œå»ºè­°\",\n",
    "            \"tutorial\": \"å¾ªåºæ¼¸é€²ã€å¯¦ç”¨ã€åŒ…å«æ­¥é©Ÿå’Œç¯„ä¾‹\"\n",
    "        }\n",
    "        \n",
    "        requirements = type_requirements.get(article_type, type_requirements[\"academic\"])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ–‡ç« æ¶æ§‹å¸«ã€‚è«‹æ ¹æ“šä»¥ä¸‹ç ”ç©¶å ±å‘Šï¼Œç‚ºä¸»é¡Œã€Œ{topic}ã€è¨­è¨ˆä¸€å€‹{article_type}é¢¨æ ¼çš„æ–‡ç« å¤§ç¶±ã€‚\n",
    "\n",
    "ç ”ç©¶å ±å‘Šå…§å®¹ï¼š\n",
    "{report[:2000]}  # é™åˆ¶é•·åº¦é¿å…è¶…å‡ºtokené™åˆ¶\n",
    "\n",
    "æ–‡ç« è¦æ±‚ï¼š\n",
    "- æ–‡ç« é¡å‹ï¼š{article_type}\n",
    "- é¢¨æ ¼ç‰¹è‰²ï¼š{requirements}\n",
    "- ç›®æ¨™é•·åº¦ï¼šç´„{target_length}å­—\n",
    "- ç›®æ¨™è®€è€…ï¼šå°æ­¤ä¸»é¡Œæ„Ÿèˆˆè¶£çš„å°ˆæ¥­äººå£«å’Œå­¸ç¿’è€…\n",
    "\n",
    "è«‹è¨­è¨ˆä¸€å€‹çµæ§‹æ¸…æ™°ã€é‚è¼¯å®Œæ•´çš„å¤§ç¶±ã€‚å¤§ç¶±æ ¼å¼å¦‚ä¸‹ï¼š\n",
    "\n",
    "# æ–‡ç« æ¨™é¡Œ\n",
    "\n",
    "## 1. ç« ç¯€åç¨±\n",
    "### 1.1 å°ç¯€åç¨±\n",
    "- è¦é»èªªæ˜ï¼ˆ2-3å¥è©±æè¿°æ­¤éƒ¨åˆ†è¦æ¶µè“‹çš„å…§å®¹ï¼‰\n",
    "### 1.2 å°ç¯€åç¨±\n",
    "- è¦é»èªªæ˜\n",
    "\n",
    "## 2. ç« ç¯€åç¨±\n",
    "### 2.1 å°ç¯€åç¨±\n",
    "- è¦é»èªªæ˜\n",
    "\n",
    "ï¼ˆç¹¼çºŒå…¶ä»–ç« ç¯€...ï¼‰\n",
    "\n",
    "è«‹ç¢ºä¿ï¼š\n",
    "1. å¤§ç¶±æ¶µè“‹ç ”ç©¶å ±å‘Šçš„ä¸»è¦ç™¼ç¾\n",
    "2. ç« ç¯€ä¹‹é–“æœ‰æ¸…æ™°çš„é‚è¼¯é—œä¿‚\n",
    "3. æ¯å€‹å°ç¯€éƒ½æœ‰å…·é«”çš„å…§å®¹æè¿°\n",
    "4. é©åˆ{target_length}å­—å·¦å³çš„æ–‡ç« é•·åº¦\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def _parse_outline(self, outline_text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"è§£æå¤§ç¶±æ–‡æœ¬çµæ§‹\"\"\"\n",
    "        sections = []\n",
    "        current_section = None\n",
    "        current_subsection = None\n",
    "        \n",
    "        lines = outline_text.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # ä¸»æ¨™é¡Œ\n",
    "            if line.startswith('# '):\n",
    "                continue  # è·³éæ–‡ç« æ¨™é¡Œ\n",
    "            \n",
    "            # ç« ç¯€æ¨™é¡Œ (## é–‹é ­)\n",
    "            elif line.startswith('## '):\n",
    "                if current_section:\n",
    "                    sections.append(current_section)\n",
    "                \n",
    "                current_section = {\n",
    "                    'title': line[3:].strip(),\n",
    "                    'subsections': [],\n",
    "                    'estimated_words': 0\n",
    "                }\n",
    "                current_subsection = None\n",
    "            \n",
    "            # å°ç¯€æ¨™é¡Œ (### é–‹é ­)\n",
    "            elif line.startswith('### '):\n",
    "                if current_section:\n",
    "                    current_subsection = {\n",
    "                        'title': line[4:].strip(),\n",
    "                        'description': '',\n",
    "                        'estimated_words': 0\n",
    "                    }\n",
    "                    current_section['subsections'].append(current_subsection)\n",
    "            \n",
    "            # è¦é»æè¿° (- é–‹é ­)\n",
    "            elif line.startswith('- ') and current_subsection:\n",
    "                current_subsection['description'] += line[2:].strip() + ' '\n",
    "        \n",
    "        # åŠ å…¥æœ€å¾Œä¸€å€‹ç« ç¯€\n",
    "        if current_section:\n",
    "            sections.append(current_section)\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def _allocate_word_counts(self, outline_structure: List[Dict], target_length: int) -> List[Dict]:\n",
    "        \"\"\"ç‚ºå¤§ç¶±åˆ†é…å­—æ•¸\"\"\"\n",
    "        total_sections = len(outline_structure)\n",
    "        if total_sections == 0:\n",
    "            return outline_structure\n",
    "        \n",
    "        # åŸºæœ¬å­—æ•¸åˆ†é…\n",
    "        base_words_per_section = target_length // total_sections\n",
    "        \n",
    "        for section in outline_structure:\n",
    "            subsection_count = len(section['subsections'])\n",
    "            if subsection_count > 0:\n",
    "                words_per_subsection = base_words_per_section // subsection_count\n",
    "                \n",
    "                for subsection in section['subsections']:\n",
    "                    subsection['estimated_words'] = words_per_subsection\n",
    "                \n",
    "                section['estimated_words'] = base_words_per_section\n",
    "            else:\n",
    "                section['estimated_words'] = base_words_per_section\n",
    "        \n",
    "        return outline_structure\n",
    "    \n",
    "    def _create_fallback_outline(self, research_data: Dict, target_length: int) -> Dict[str, Any]:\n",
    "        \"\"\"å»ºç«‹å‚™ç”¨å¤§ç¶±\"\"\"\n",
    "        topic = research_data['topic']\n",
    "        \n",
    "        fallback_outline = [\n",
    "            {\n",
    "                'title': '1. å¼•è¨€èˆ‡èƒŒæ™¯',\n",
    "                'subsections': [\n",
    "                    {'title': '1.1 ç ”ç©¶èƒŒæ™¯', 'description': 'ä»‹ç´¹ç ”ç©¶ä¸»é¡Œçš„é‡è¦æ€§', 'estimated_words': target_length // 6},\n",
    "                    {'title': '1.2 ç ”ç©¶ç›®çš„', 'description': 'èªªæ˜ç ”ç©¶çš„ç›®æ¨™å’Œæ„ç¾©', 'estimated_words': target_length // 6}\n",
    "                ],\n",
    "                'estimated_words': target_length // 3\n",
    "            },\n",
    "            {\n",
    "                'title': '2. ä¸»è¦ç™¼ç¾èˆ‡åˆ†æ',\n",
    "                'subsections': [\n",
    "                    {'title': '2.1 é—œéµç™¼ç¾', 'description': 'ç¸½çµä¸»è¦ç ”ç©¶çµæœ', 'estimated_words': target_length // 6},\n",
    "                    {'title': '2.2 æ·±åº¦åˆ†æ', 'description': 'è©³ç´°åˆ†æç ”ç©¶ç™¼ç¾çš„æ„ç¾©', 'estimated_words': target_length // 6}\n",
    "                ],\n",
    "                'estimated_words': target_length // 3\n",
    "            },\n",
    "            {\n",
    "                'title': '3. çµè«–èˆ‡å»ºè­°',\n",
    "                'subsections': [\n",
    "                    {'title': '3.1 ç¸½çµ', 'description': 'ç¸½çµä¸»è¦è§€é»', 'estimated_words': target_length // 6},\n",
    "                    {'title': '3.2 æœªä¾†å±•æœ›', 'description': 'æå‡ºæœªä¾†ç™¼å±•å»ºè­°', 'estimated_words': target_length // 6}\n",
    "                ],\n",
    "                'estimated_words': target_length // 3\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            'topic': topic,\n",
    "            'article_type': 'academic',\n",
    "            'target_length': target_length,\n",
    "            'outline_structure': fallback_outline,\n",
    "            'raw_outline': 'å‚™ç”¨å¤§ç¶±',\n",
    "            'generated_at': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# å»ºç«‹å¤§ç¶±ç”Ÿæˆå™¨\n",
    "outline_generator = OutlineGenerator(llm)\n",
    "print(\"âœ… å¤§ç¶±ç”Ÿæˆå™¨å·²å»ºç«‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¸¬è©¦å¤§ç¶±ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæ–‡ç« å¤§ç¶±\n",
    "article_outline = outline_generator.generate_outline(\n",
    "    research_data=sample_research_data,\n",
    "    article_type=\"academic\",\n",
    "    target_length=2500\n",
    ")\n",
    "\n",
    "# é¡¯ç¤ºå¤§ç¶±çµæ§‹\n",
    "print(\"\\nğŸ“‹ ç”Ÿæˆçš„æ–‡ç« å¤§ç¶±ï¼š\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, section in enumerate(article_outline['outline_structure'], 1):\n",
    "    print(f\"\\n## {section['title']} ({section['estimated_words']}å­—)\")\n",
    "    \n",
    "    for j, subsection in enumerate(section['subsections'], 1):\n",
    "        print(f\"   ### {subsection['title']} ({subsection['estimated_words']}å­—)\")\n",
    "        print(f\"       - {subsection['description'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. å…§å®¹ç”Ÿæˆå™¨\n",
    "\n",
    "### å¯¦ä½œæ®µè½å…§å®¹å‰µä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentGenerator:\n",
    "    \"\"\"å…§å®¹ç”Ÿæˆå™¨ - æ ¹æ“šå¤§ç¶±ç”Ÿæˆå…·é«”æ–‡ç« å…§å®¹\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def generate_article_content(self, outline_data: Dict[str, Any], \n",
    "                                research_data: Dict[str, Any],\n",
    "                                writing_style: Dict[str, str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"ç”Ÿæˆå®Œæ•´æ–‡ç« å…§å®¹\"\"\"\n",
    "        \n",
    "        print(f\"âœï¸ é–‹å§‹ç”Ÿæˆæ–‡ç« å…§å®¹ï¼š{outline_data['topic']}\")\n",
    "        \n",
    "        # é è¨­å¯«ä½œé¢¨æ ¼\n",
    "        if writing_style is None:\n",
    "            writing_style = {\n",
    "                'tone': 'professional',  # professional, casual, academic\n",
    "                'perspective': 'third_person',  # first_person, third_person\n",
    "                'citation_style': 'inline',  # inline, footnote, endnote\n",
    "                'target_audience': 'professionals'  # general, students, professionals\n",
    "            }\n",
    "        \n",
    "        # ç”Ÿæˆæ–‡ç« æ¨™é¡Œ\n",
    "        article_title = self._generate_title(outline_data['topic'], writing_style)\n",
    "        \n",
    "        # ç”Ÿæˆå„ç« ç¯€å…§å®¹\n",
    "        article_sections = []\n",
    "        \n",
    "        for section_info in outline_data['outline_structure']:\n",
    "            print(f\"   ğŸ“ ç”Ÿæˆç« ç¯€ï¼š{section_info['title']}\")\n",
    "            \n",
    "            section_content = self._generate_section_content(\n",
    "                section_info, research_data, writing_style, outline_data['topic']\n",
    "            )\n",
    "            \n",
    "            article_sections.append(section_content)\n",
    "        \n",
    "        # çµ„è£å®Œæ•´æ–‡ç« \n",
    "        full_article = self._assemble_article(article_title, article_sections)\n",
    "        \n",
    "        result = {\n",
    "            'title': article_title,\n",
    "            'content': full_article,\n",
    "            'sections': article_sections,\n",
    "            'word_count': len(full_article.split()),\n",
    "            'writing_style': writing_style,\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'metadata': {\n",
    "                'topic': outline_data['topic'],\n",
    "                'target_length': outline_data['target_length'],\n",
    "                'actual_length': len(full_article.split())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nâœ… æ–‡ç« ç”Ÿæˆå®Œæˆï¼\")\n",
    "        print(f\"   æ¨™é¡Œï¼š{article_title}\")\n",
    "        print(f\"   å­—æ•¸ï¼š{result['word_count']} å­—\")\n",
    "        print(f\"   ç« ç¯€ï¼š{len(article_sections)} å€‹\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _generate_title(self, topic: str, style: Dict[str, str]) -> str:\n",
    "        \"\"\"ç”Ÿæˆæ–‡ç« æ¨™é¡Œ\"\"\"\n",
    "        \n",
    "        title_prompt = f\"\"\"\n",
    "è«‹ç‚ºä¸»é¡Œã€Œ{topic}ã€ç”Ÿæˆä¸€å€‹å¸å¼•äººä¸”å°ˆæ¥­çš„æ–‡ç« æ¨™é¡Œã€‚\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "- èªèª¿ï¼š{style['tone']}\n",
    "- ç›®æ¨™è®€è€…ï¼š{style['target_audience']}\n",
    "- æ¨™é¡Œè¦æº–ç¢ºåæ˜ ä¸»é¡Œå…§å®¹\n",
    "- æ¨™é¡Œè¦æœ‰å¸å¼•åŠ›ä½†ä¸èª‡å¤§\n",
    "- é•·åº¦é©ä¸­ï¼ˆ10-20å€‹å­—ï¼‰\n",
    "\n",
    "è«‹åªå›ç­”æ¨™é¡Œï¼Œä¸è¦å…¶ä»–èªªæ˜ï¼š\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=title_prompt)])\n",
    "            return response.content.strip()\n",
    "        except:\n",
    "            return f\"{topic}ï¼šæ·±åº¦åˆ†æèˆ‡æ¢è¨\"  # å‚™ç”¨æ¨™é¡Œ\n",
    "    \n",
    "    def _generate_section_content(self, section_info: Dict[str, Any], \n",
    "                                 research_data: Dict[str, Any],\n",
    "                                 style: Dict[str, str],\n",
    "                                 main_topic: str) -> Dict[str, Any]:\n",
    "        \"\"\"ç”Ÿæˆå–®ä¸€ç« ç¯€å…§å®¹\"\"\"\n",
    "        \n",
    "        section_title = section_info['title']\n",
    "        target_words = section_info['estimated_words']\n",
    "        \n",
    "        # ç‚ºæ¯å€‹å°ç¯€ç”Ÿæˆå…§å®¹\n",
    "        subsection_contents = []\n",
    "        \n",
    "        for subsection in section_info['subsections']:\n",
    "            subsection_content = self._generate_subsection_content(\n",
    "                subsection, research_data, style, main_topic\n",
    "            )\n",
    "            subsection_contents.append(subsection_content)\n",
    "        \n",
    "        # çµ„åˆç« ç¯€å…§å®¹\n",
    "        section_text = f\"\\n\\n## {section_title}\\n\\n\"\n",
    "        \n",
    "        for sub_content in subsection_contents:\n",
    "            section_text += sub_content + \"\\n\\n\"\n",
    "        \n",
    "        return {\n",
    "            'title': section_title,\n",
    "            'content': section_text,\n",
    "            'subsections': subsection_contents,\n",
    "            'word_count': len(section_text.split()),\n",
    "            'target_words': target_words\n",
    "        }\n",
    "    \n",
    "    def _generate_subsection_content(self, subsection_info: Dict[str, Any],\n",
    "                                    research_data: Dict[str, Any],\n",
    "                                    style: Dict[str, str],\n",
    "                                    main_topic: str) -> str:\n",
    "        \"\"\"ç”Ÿæˆå°ç¯€å…§å®¹\"\"\"\n",
    "        \n",
    "        subsection_title = subsection_info['title']\n",
    "        description = subsection_info['description']\n",
    "        target_words = subsection_info['estimated_words']\n",
    "        \n",
    "        # æå–ç›¸é—œç ”ç©¶å…§å®¹\n",
    "        research_context = research_data['comprehensive_report']\n",
    "        \n",
    "        content_prompt = f\"\"\"\n",
    "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å…§å®¹å‰µä½œè€…ã€‚è«‹æ ¹æ“šä»¥ä¸‹è³‡è¨Šæ’°å¯«æ–‡ç« å°ç¯€å…§å®¹ã€‚\n",
    "\n",
    "ä¸»é¡Œï¼š{main_topic}\n",
    "å°ç¯€æ¨™é¡Œï¼š{subsection_title}\n",
    "å°ç¯€è¦æ±‚ï¼š{description}\n",
    "ç›®æ¨™å­—æ•¸ï¼šç´„{target_words}å­—\n",
    "\n",
    "åƒè€ƒç ”ç©¶è³‡æ–™ï¼š\n",
    "{research_context[:1500]}\n",
    "\n",
    "å¯«ä½œé¢¨æ ¼ï¼š\n",
    "- èªèª¿ï¼š{style['tone']}\n",
    "- è¦–è§’ï¼š{style['perspective']}\n",
    "- ç›®æ¨™è®€è€…ï¼š{style['target_audience']}\n",
    "\n",
    "è«‹æ’°å¯«å…§å®¹ï¼Œè¦æ±‚ï¼š\n",
    "1. å…§å®¹è¦æœ‰é‚è¼¯æ€§å’ŒèªªæœåŠ›\n",
    "2. é©ç•¶å¼•ç”¨ç ”ç©¶è³‡æ–™ä¸­çš„é‡é»\n",
    "3. ä¿æŒèˆ‡æ•´é«”ä¸»é¡Œçš„ç›¸é—œæ€§\n",
    "4. èªè¨€è¦æ¸…æ™°æ˜“æ‡‚\n",
    "5. åŒ…å«å…·é«”ä¾‹å­æˆ–æ•¸æ“šæ”¯æŒè«–é»\n",
    "\n",
    "æ ¼å¼ï¼š\n",
    "### {subsection_title}\n",
    "\n",
    "[å…§å®¹æ­£æ–‡]\n",
    "\n",
    "è«‹é–‹å§‹æ’°å¯«ï¼š\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=content_prompt)])\n",
    "            return response.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ å°ç¯€ç”Ÿæˆå¤±æ•—ï¼š{str(e)}\")\n",
    "            return f\"### {subsection_title}\\n\\n[å…§å®¹ç”Ÿæˆä¸­é‡åˆ°å•é¡Œï¼Œè«‹é‡æ–°å˜—è©¦]\\n\"\n",
    "    \n",
    "    def _assemble_article(self, title: str, sections: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"çµ„è£å®Œæ•´æ–‡ç« \"\"\"\n",
    "        \n",
    "        article = f\"# {title}\\n\\n\"\n",
    "        \n",
    "        # åŠ å…¥ç”Ÿæˆè³‡è¨Š\n",
    "        article += f\"*æœ¬æ–‡ç« ç”±AIè¼”åŠ©ç”Ÿæˆï¼Œç”Ÿæˆæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}*\\n\\n\"\n",
    "        \n",
    "        # åŠ å…¥å„ç« ç¯€\n",
    "        for section in sections:\n",
    "            article += section['content']\n",
    "        \n",
    "        return article\n",
    "\n",
    "# å»ºç«‹å…§å®¹ç”Ÿæˆå™¨\n",
    "content_generator = ContentGenerator(llm)\n",
    "print(\"âœ… å…§å®¹ç”Ÿæˆå™¨å·²å»ºç«‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¸¬è©¦å…§å®¹ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šå¯«ä½œé¢¨æ ¼\n",
    "writing_style = {\n",
    "    'tone': 'professional',\n",
    "    'perspective': 'third_person',\n",
    "    'citation_style': 'inline',\n",
    "    'target_audience': 'professionals'\n",
    "}\n",
    "\n",
    "# ç”Ÿæˆæ–‡ç« å…§å®¹\n",
    "print(\"ğŸš€ é–‹å§‹ç”Ÿæˆæ–‡ç« å…§å®¹...\")\n",
    "article_content = content_generator.generate_article_content(\n",
    "    outline_data=article_outline,\n",
    "    research_data=sample_research_data,\n",
    "    writing_style=writing_style\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é è¦½ç”Ÿæˆçš„æ–‡ç« "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¡¯ç¤ºæ–‡ç« åŸºæœ¬è³‡è¨Š\n",
    "print(\"ğŸ“„ æ–‡ç« è³‡è¨Šï¼š\")\n",
    "print(f\"æ¨™é¡Œï¼š{article_content['title']}\")\n",
    "print(f\"å­—æ•¸ï¼š{article_content['word_count']} å­—\")\n",
    "print(f\"ç« ç¯€æ•¸ï¼š{len(article_content['sections'])} å€‹\")\n",
    "print(f\"ç›®æ¨™å­—æ•¸ï¼š{article_content['metadata']['target_length']} å­—\")\n",
    "\n",
    "# é¡¯ç¤ºæ–‡ç« é–‹é ­\n",
    "print(\"\\nğŸ“– æ–‡ç« é è¦½ï¼ˆå‰500å­—ï¼‰ï¼š\")\n",
    "print(\"=\" * 60)\n",
    "preview_content = article_content['content'][:500]\n",
    "print(preview_content + \"...\")\n",
    "\n",
    "# é¡¯ç¤ºç« ç¯€çµæ§‹\n",
    "print(\"\\nğŸ“‹ ç« ç¯€çµæ§‹ï¼š\")\n",
    "for i, section in enumerate(article_content['sections'], 1):\n",
    "    print(f\"{i}. {section['title']} - {section['word_count']}å­—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. å“è³ªæª¢æŸ¥å™¨\n",
    "\n",
    "### å¯¦ä½œå…§å®¹å“è³ªæª¢æŸ¥èˆ‡å„ªåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityChecker:\n",
    "    \"\"\"å“è³ªæª¢æŸ¥å™¨ - æª¢æŸ¥å’Œå„ªåŒ–æ–‡ç« å“è³ª\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def check_article_quality(self, article_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"æª¢æŸ¥æ–‡ç« å“è³ª\"\"\"\n",
    "        \n",
    "        print(\"ğŸ” é–‹å§‹å“è³ªæª¢æŸ¥...\")\n",
    "        \n",
    "        article_content = article_data['content']\n",
    "        \n",
    "        # åŸ·è¡Œå„é …æª¢æŸ¥\n",
    "        checks = {\n",
    "            'readability': self._check_readability(article_content),\n",
    "            'coherence': self._check_coherence(article_content),\n",
    "            'completeness': self._check_completeness(article_data),\n",
    "            'style_consistency': self._check_style_consistency(article_content),\n",
    "            'structure': self._check_structure(article_data)\n",
    "        }\n",
    "        \n",
    "        # è¨ˆç®—ç¸½é«”å“è³ªåˆ†æ•¸\n",
    "        overall_score = sum(checks.values()) / len(checks)\n",
    "        \n",
    "        # ç”Ÿæˆæ”¹é€²å»ºè­°\n",
    "        suggestions = self._generate_improvement_suggestions(checks, article_content)\n",
    "        \n",
    "        result = {\n",
    "            'overall_score': overall_score,\n",
    "            'detailed_scores': checks,\n",
    "            'suggestions': suggestions,\n",
    "            'check_date': datetime.now().isoformat(),\n",
    "            'article_stats': {\n",
    "                'word_count': article_data['word_count'],\n",
    "                'section_count': len(article_data['sections']),\n",
    "                'avg_section_length': article_data['word_count'] / len(article_data['sections'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… å“è³ªæª¢æŸ¥å®Œæˆï¼ç¸½é«”åˆ†æ•¸ï¼š{overall_score:.2f}/1.0\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _check_readability(self, content: str) -> float:\n",
    "        \"\"\"æª¢æŸ¥å¯è®€æ€§\"\"\"\n",
    "        # ç°¡åŒ–çš„å¯è®€æ€§æª¢æŸ¥\n",
    "        sentences = content.split('ã€‚')\n",
    "        words = content.split()\n",
    "        \n",
    "        if len(sentences) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        avg_sentence_length = len(words) / len(sentences)\n",
    "        \n",
    "        # ç†æƒ³å¥å­é•·åº¦ç´„15-25å­—\n",
    "        if 15 <= avg_sentence_length <= 25:\n",
    "            readability_score = 1.0\n",
    "        elif 10 <= avg_sentence_length <= 35:\n",
    "            readability_score = 0.8\n",
    "        else:\n",
    "            readability_score = 0.6\n",
    "        \n",
    "        return readability_score\n",
    "    \n",
    "    def _check_coherence(self, content: str) -> float:\n",
    "        \"\"\"æª¢æŸ¥é€£è²«æ€§\"\"\"\n",
    "        # ä½¿ç”¨ LLM è©•ä¼°é€£è²«æ€§\n",
    "        coherence_prompt = f\"\"\"\n",
    "è«‹è©•ä¼°ä»¥ä¸‹æ–‡ç« çš„é‚è¼¯é€£è²«æ€§å’Œçµæ§‹å®Œæ•´æ€§ã€‚\n",
    "\n",
    "æ–‡ç« å…§å®¹ï¼ˆå‰1000å­—ï¼‰ï¼š\n",
    "{content[:1000]}\n",
    "\n",
    "è«‹å¾ä»¥ä¸‹è§’åº¦è©•ä¼°ä¸¦çµ¦å‡º0-10åˆ†çš„è©•åˆ†ï¼š\n",
    "1. æ®µè½ä¹‹é–“çš„é‚è¼¯é€£æ¥æ˜¯å¦è‡ªç„¶\n",
    "2. è«–é»ç™¼å±•æ˜¯å¦æœ‰æ¢ç†\n",
    "3. æ•´é«”çµæ§‹æ˜¯å¦æ¸…æ™°\n",
    "\n",
    "è«‹åªå›ç­”ä¸€å€‹0-10çš„æ•¸å­—åˆ†æ•¸ï¼š\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=coherence_prompt)])\n",
    "            score_text = response.content.strip()\n",
    "            \n",
    "            # æå–åˆ†æ•¸\n",
    "            import re\n",
    "            score_match = re.search(r'\\d+(?:\\.\\d+)?', score_text)\n",
    "            if score_match:\n",
    "                score = float(score_match.group())\n",
    "                return min(score / 10.0, 1.0)\n",
    "            else:\n",
    "                return 0.7  # é è¨­åˆ†æ•¸\n",
    "        except:\n",
    "            return 0.7\n",
    "    \n",
    "    def _check_completeness(self, article_data: Dict[str, Any]) -> float:\n",
    "        \"\"\"æª¢æŸ¥å®Œæ•´æ€§\"\"\"\n",
    "        target_length = article_data['metadata']['target_length']\n",
    "        actual_length = article_data['word_count']\n",
    "        \n",
    "        # å­—æ•¸å®Œæ•´åº¦\n",
    "        length_ratio = actual_length / target_length\n",
    "        \n",
    "        if 0.8 <= length_ratio <= 1.2:\n",
    "            length_score = 1.0\n",
    "        elif 0.6 <= length_ratio <= 1.5:\n",
    "            length_score = 0.8\n",
    "        else:\n",
    "            length_score = 0.5\n",
    "        \n",
    "        # çµæ§‹å®Œæ•´åº¦\n",
    "        section_count = len(article_data['sections'])\n",
    "        if section_count >= 3:\n",
    "            structure_score = 1.0\n",
    "        elif section_count >= 2:\n",
    "            structure_score = 0.8\n",
    "        else:\n",
    "            structure_score = 0.5\n",
    "        \n",
    "        return (length_score + structure_score) / 2\n",
    "    \n",
    "    def _check_style_consistency(self, content: str) -> float:\n",
    "        \"\"\"æª¢æŸ¥é¢¨æ ¼ä¸€è‡´æ€§\"\"\"\n",
    "        # ç°¡åŒ–çš„é¢¨æ ¼ä¸€è‡´æ€§æª¢æŸ¥\n",
    "        \n",
    "        # æª¢æŸ¥äººç¨±ä¸€è‡´æ€§\n",
    "        first_person_count = content.count('æˆ‘') + content.count('æˆ‘å€‘')\n",
    "        third_person_indicators = content.count('ä»–') + content.count('å¥¹') + content.count('å®ƒå€‘')\n",
    "        \n",
    "        total_words = len(content.split())\n",
    "        \n",
    "        if total_words > 0:\n",
    "            person_inconsistency = abs(first_person_count - third_person_indicators) / total_words\n",
    "            style_score = max(0.5, 1.0 - person_inconsistency * 100)\n",
    "        else:\n",
    "            style_score = 0.5\n",
    "        \n",
    "        return min(style_score, 1.0)\n",
    "    \n",
    "    def _check_structure(self, article_data: Dict[str, Any]) -> float:\n",
    "        \"\"\"æª¢æŸ¥çµæ§‹å“è³ª\"\"\"\n",
    "        sections = article_data['sections']\n",
    "        \n",
    "        if not sections:\n",
    "            return 0.0\n",
    "        \n",
    "        # æª¢æŸ¥å­—æ•¸åˆ†ä½ˆæ˜¯å¦å¹³è¡¡\n",
    "        word_counts = [section['word_count'] for section in sections]\n",
    "        avg_words = sum(word_counts) / len(word_counts)\n",
    "        \n",
    "        # è¨ˆç®—æ¨™æº–å·®\n",
    "        variance = sum((count - avg_words) ** 2 for count in word_counts) / len(word_counts)\n",
    "        std_dev = variance ** 0.5\n",
    "        \n",
    "        # æ¨™æº–å·®è¶Šå°ï¼Œåˆ†ä½ˆè¶Šå‡å‹»\n",
    "        if avg_words > 0:\n",
    "            balance_ratio = std_dev / avg_words\n",
    "            balance_score = max(0.5, 1.0 - balance_ratio)\n",
    "        else:\n",
    "            balance_score = 0.5\n",
    "        \n",
    "        return min(balance_score, 1.0)\n",
    "    \n",
    "    def _generate_improvement_suggestions(self, checks: Dict[str, float], content: str) -> List[str]:\n",
    "        \"\"\"ç”Ÿæˆæ”¹é€²å»ºè­°\"\"\"\n",
    "        suggestions = []\n",
    "        \n",
    "        if checks['readability'] < 0.7:\n",
    "            suggestions.append(\"å»ºè­°èª¿æ•´å¥å­é•·åº¦ï¼Œè®“æ–‡ç« æ›´æ˜“è®€\")\n",
    "        \n",
    "        if checks['coherence'] < 0.7:\n",
    "            suggestions.append(\"å»ºè­°åŠ å¼·æ®µè½é–“çš„é‚è¼¯é€£æ¥\")\n",
    "        \n",
    "        if checks['completeness'] < 0.8:\n",
    "            suggestions.append(\"å»ºè­°èª¿æ•´æ–‡ç« é•·åº¦æˆ–å¢åŠ å…§å®¹å®Œæ•´æ€§\")\n",
    "        \n",
    "        if checks['style_consistency'] < 0.7:\n",
    "            suggestions.append(\"å»ºè­°æª¢æŸ¥ä¸¦çµ±ä¸€å¯«ä½œé¢¨æ ¼\")\n",
    "        \n",
    "        if checks['structure'] < 0.7:\n",
    "            suggestions.append(\"å»ºè­°å¹³è¡¡å„ç« ç¯€çš„å…§å®¹é•·åº¦\")\n",
    "        \n",
    "        if not suggestions:\n",
    "            suggestions.append(\"æ–‡ç« å“è³ªè‰¯å¥½ï¼Œç„¡éœ€ç‰¹åˆ¥æ”¹é€²\")\n",
    "        \n",
    "        return suggestions\n",
    "\n",
    "# å»ºç«‹å“è³ªæª¢æŸ¥å™¨\n",
    "quality_checker = QualityChecker(llm)\n",
    "print(\"âœ… å“è³ªæª¢æŸ¥å™¨å·²å»ºç«‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¸¬è©¦å“è³ªæª¢æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸ·è¡Œå“è³ªæª¢æŸ¥\n",
    "quality_report = quality_checker.check_article_quality(article_content)\n",
    "\n",
    "# é¡¯ç¤ºå“è³ªå ±å‘Š\n",
    "print(\"\\nğŸ“Š å“è³ªæª¢æŸ¥å ±å‘Šï¼š\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"ç¸½é«”å“è³ªåˆ†æ•¸ï¼š{quality_report['overall_score']:.2f}/1.0\")\n",
    "\n",
    "print(\"\\nè©³ç´°è©•åˆ†ï¼š\")\n",
    "for metric, score in quality_report['detailed_scores'].items():\n",
    "    print(f\"  {metric}: {score:.2f}\")\n",
    "\n",
    "print(\"\\næ”¹é€²å»ºè­°ï¼š\")\n",
    "for i, suggestion in enumerate(quality_report['suggestions'], 1):\n",
    "    print(f\"  {i}. {suggestion}\")\n",
    "\n",
    "print(\"\\næ–‡ç« çµ±è¨ˆï¼š\")\n",
    "stats = quality_report['article_stats']\n",
    "print(f\"  ç¸½å­—æ•¸ï¼š{stats['word_count']}\")\n",
    "print(f\"  ç« ç¯€æ•¸ï¼š{stats['section_count']}\")\n",
    "print(f\"  å¹³å‡ç« ç¯€é•·åº¦ï¼š{stats['avg_section_length']:.0f} å­—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. å®Œæ•´å¯«ä½œæ™ºèƒ½é«”\n",
    "\n",
    "### æ•´åˆæ‰€æœ‰å¯«ä½œåŠŸèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WritingAgent:\n",
    "    \"\"\"å¯«ä½œæ™ºèƒ½é«” - æ•´åˆå¤§ç¶±ç”Ÿæˆã€å…§å®¹å‰µä½œã€å“è³ªæª¢æŸ¥\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.outline_generator = OutlineGenerator(llm)\n",
    "        self.content_generator = ContentGenerator(llm)\n",
    "        self.quality_checker = QualityChecker(llm)\n",
    "    \n",
    "    def write_article(self, research_data: Dict[str, Any],\n",
    "                     article_config: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"å®Œæ•´çš„æ–‡ç« å¯«ä½œæµç¨‹\"\"\"\n",
    "        \n",
    "        print(f\"ğŸ“ é–‹å§‹å¯«ä½œæµç¨‹ï¼š{research_data['topic']}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # é è¨­é…ç½®\n",
    "        if article_config is None:\n",
    "            article_config = {\n",
    "                'article_type': 'academic',\n",
    "                'target_length': 2500,\n",
    "                'writing_style': {\n",
    "                    'tone': 'professional',\n",
    "                    'perspective': 'third_person',\n",
    "                    'citation_style': 'inline',\n",
    "                    'target_audience': 'professionals'\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # ç¬¬ä¸€éšæ®µï¼šç”Ÿæˆå¤§ç¶±\n",
    "        print(\"\\nğŸ“‹ éšæ®µ 1: ç”Ÿæˆæ–‡ç« å¤§ç¶±\")\n",
    "        outline_data = self.outline_generator.generate_outline(\n",
    "            research_data=research_data,\n",
    "            article_type=article_config['article_type'],\n",
    "            target_length=article_config['target_length']\n",
    "        )\n",
    "        \n",
    "        # ç¬¬äºŒéšæ®µï¼šç”Ÿæˆå…§å®¹\n",
    "        print(\"\\nâœï¸ éšæ®µ 2: ç”Ÿæˆæ–‡ç« å…§å®¹\")\n",
    "        article_content = self.content_generator.generate_article_content(\n",
    "            outline_data=outline_data,\n",
    "            research_data=research_data,\n",
    "            writing_style=article_config['writing_style']\n",
    "        )\n",
    "        \n",
    "        # ç¬¬ä¸‰éšæ®µï¼šå“è³ªæª¢æŸ¥\n",
    "        print(\"\\nğŸ” éšæ®µ 3: å“è³ªæª¢æŸ¥èˆ‡å„ªåŒ–\")\n",
    "        quality_report = self.quality_checker.check_article_quality(article_content)\n",
    "        \n",
    "        # ç¬¬å››éšæ®µï¼šæœ€çµ‚æ•´ç†\n",
    "        print(\"\\nğŸ“¦ éšæ®µ 4: æœ€çµ‚æ•´ç†\")\n",
    "        final_result = self._finalize_article(\n",
    "            research_data, outline_data, article_content, quality_report, article_config\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nğŸ‰ æ–‡ç« å¯«ä½œå®Œæˆï¼\")\n",
    "        print(f\"   æ¨™é¡Œï¼š{final_result['title']}\")\n",
    "        print(f\"   å­—æ•¸ï¼š{final_result['word_count']} å­—\")\n",
    "        print(f\"   å“è³ªåˆ†æ•¸ï¼š{final_result['quality_score']:.2f}/1.0\")\n",
    "        \n",
    "        return final_result\n",
    "    \n",
    "    def _finalize_article(self, research_data: Dict, outline_data: Dict, \n",
    "                         article_content: Dict, quality_report: Dict,\n",
    "                         article_config: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"æœ€çµ‚æ•´ç†æ–‡ç« \"\"\"\n",
    "        \n",
    "        # åŠ å…¥å…ƒæ•¸æ“š\n",
    "        final_article = {\n",
    "            'title': article_content['title'],\n",
    "            'content': article_content['content'],\n",
    "            'word_count': article_content['word_count'],\n",
    "            'quality_score': quality_report['overall_score'],\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            \n",
    "            # è©³ç´°è³‡æ–™\n",
    "            'metadata': {\n",
    "                'source_topic': research_data['topic'],\n",
    "                'article_type': article_config['article_type'],\n",
    "                'target_length': article_config['target_length'],\n",
    "                'actual_length': article_content['word_count'],\n",
    "                'completion_ratio': article_content['word_count'] / article_config['target_length'],\n",
    "                'section_count': len(article_content['sections']),\n",
    "                'writing_style': article_config['writing_style']\n",
    "            },\n",
    "            \n",
    "            # å“è³ªè³‡æ–™\n",
    "            'quality_report': quality_report,\n",
    "            \n",
    "            # çµæ§‹è³‡æ–™\n",
    "            'outline': outline_data,\n",
    "            'sections': article_content['sections'],\n",
    "            \n",
    "            # ä¾†æºè³‡æ–™\n",
    "            'research_source': {\n",
    "                'research_questions': len(research_data.get('expert_questions', {})),\n",
    "                'source_confidence': research_data.get('statistics', {}).get('average_confidence', 0.8)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return final_article\n",
    "\n",
    "# å»ºç«‹å®Œæ•´çš„å¯«ä½œæ™ºèƒ½é«”\n",
    "writing_agent = WritingAgent(llm)\n",
    "print(\"âœ… å¯«ä½œæ™ºèƒ½é«”å·²å»ºç«‹å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. å®Œæ•´æ¸¬è©¦\n",
    "\n",
    "### åŸ·è¡Œå®Œæ•´å¯«ä½œæµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šæ–‡ç« é…ç½®\n",
    "article_config = {\n",
    "    'article_type': 'academic',  # academic, blog, report, tutorial\n",
    "    'target_length': 2000,       # ç›®æ¨™å­—æ•¸\n",
    "    'writing_style': {\n",
    "        'tone': 'professional',\n",
    "        'perspective': 'third_person',\n",
    "        'citation_style': 'inline',\n",
    "        'target_audience': 'professionals'\n",
    "    }\n",
    "}\n",
    "\n",
    "# åŸ·è¡Œå®Œæ•´å¯«ä½œæµç¨‹\n",
    "final_article = writing_agent.write_article(\n",
    "    research_data=sample_research_data,\n",
    "    article_config=article_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æŸ¥çœ‹æœ€çµ‚çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¡¯ç¤ºæ–‡ç« æ‘˜è¦è³‡è¨Š\n",
    "print(\"ğŸ“„ æœ€çµ‚æ–‡ç« è³‡è¨Šï¼š\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"æ¨™é¡Œï¼š{final_article['title']}\")\n",
    "print(f\"å­—æ•¸ï¼š{final_article['word_count']} å­—\")\n",
    "print(f\"å“è³ªåˆ†æ•¸ï¼š{final_article['quality_score']:.2f}/1.0\")\n",
    "print(f\"å®Œæˆç‡ï¼š{final_article['metadata']['completion_ratio']:.1%}\")\n",
    "print(f\"ç« ç¯€æ•¸ï¼š{final_article['metadata']['section_count']}\")\n",
    "\n",
    "# å“è³ªæª¢æŸ¥çµæœ\n",
    "print(\"\\nğŸ” å“è³ªæª¢æŸ¥è©³æƒ…ï¼š\")\n",
    "for metric, score in final_article['quality_report']['detailed_scores'].items():\n",
    "    status = \"âœ…\" if score >= 0.7 else \"âš ï¸\" if score >= 0.5 else \"âŒ\"\n",
    "    print(f\"  {status} {metric}: {score:.2f}\")\n",
    "\n",
    "# æ”¹é€²å»ºè­°\n",
    "print(\"\\nğŸ’¡ æ”¹é€²å»ºè­°ï¼š\")\n",
    "for suggestion in final_article['quality_report']['suggestions']:\n",
    "    print(f\"  â€¢ {suggestion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¿å­˜å®Œæ•´æ–‡ç« "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ç‚º Markdown æª”æ¡ˆ\n",
    "safe_title = re.sub(r'[^\\w\\s-]', '', final_article['title']).strip().replace(' ', '_')\n",
    "markdown_filename = f\"article_{safe_title}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "\n",
    "try:\n",
    "    with open(markdown_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(final_article['content'])\n",
    "        \n",
    "        # åŠ å…¥å…ƒæ•¸æ“š\n",
    "        f.write(\"\\n\\n---\\n\\n\")\n",
    "        f.write(\"## æ–‡ç« å…ƒæ•¸æ“š\\n\\n\")\n",
    "        f.write(f\"- **ç”Ÿæˆæ™‚é–“**: {final_article['created_at']}\\n\")\n",
    "        f.write(f\"- **å­—æ•¸**: {final_article['word_count']}\\n\")\n",
    "        f.write(f\"- **å“è³ªåˆ†æ•¸**: {final_article['quality_score']:.2f}/1.0\\n\")\n",
    "        f.write(f\"- **æ–‡ç« é¡å‹**: {final_article['metadata']['article_type']}\\n\")\n",
    "        f.write(f\"- **å¯«ä½œé¢¨æ ¼**: {final_article['metadata']['writing_style']['tone']}\\n\")\n",
    "    \n",
    "    print(f\"âœ… æ–‡ç« å·²ä¿å­˜ç‚ºï¼š{markdown_filename}\")\n",
    "    print(f\"   æª”æ¡ˆå¤§å°ï¼š{os.path.getsize(markdown_filename)} bytes\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ä¿å­˜å¤±æ•—ï¼š{str(e)}\")\n",
    "\n",
    "# ä¿å­˜å®Œæ•´è³‡æ–™ç‚º JSON\n",
    "json_filename = f\"article_full_{safe_title}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "try:\n",
    "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_article, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… å®Œæ•´è³‡æ–™å·²ä¿å­˜ç‚ºï¼š{json_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ JSONä¿å­˜å¤±æ•—ï¼š{str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ æœ¬ç« å°çµ\n",
    "\n",
    "### å®ŒæˆåŠŸèƒ½\n",
    "âœ… **çµæ§‹åŒ–å¤§ç¶±ç”Ÿæˆ**ï¼šåŸºæ–¼ç ”ç©¶çµæœç”Ÿæˆé‚è¼¯æ¸…æ™°çš„æ–‡ç« å¤§ç¶±  \n",
    "âœ… **æ™ºèƒ½å…§å®¹å‰µä½œ**ï¼šæ ¹æ“šå¤§ç¶±é€ç« ç¯€ç”Ÿæˆé«˜å“è³ªå…§å®¹  \n",
    "âœ… **å¯«ä½œé¢¨æ ¼æ§åˆ¶**ï¼šæ”¯æ´å¤šç¨®å¯«ä½œé¢¨æ ¼å’Œç›®æ¨™å—çœ¾  \n",
    "âœ… **å“è³ªæª¢æŸ¥æ©Ÿåˆ¶**ï¼šå¤šç¶­åº¦è©•ä¼°æ–‡ç« å“è³ªä¸¦æä¾›æ”¹é€²å»ºè­°  \n",
    "âœ… **å®Œæ•´å·¥ä½œæµç¨‹**ï¼šå¾å¤§ç¶±åˆ°æœ€çµ‚æ–‡ç« çš„ç«¯åˆ°ç«¯è‡ªå‹•åŒ–  \n",
    "\n",
    "### æŠ€è¡“ç‰¹è‰²\n",
    "1. **æ¨¡çµ„åŒ–è¨­è¨ˆ**ï¼šå¤§ç¶±ç”Ÿæˆã€å…§å®¹å‰µä½œã€å“è³ªæª¢æŸ¥å„è‡ªç¨ç«‹\n",
    "2. **é…ç½®é©…å‹•**ï¼šæ”¯æ´å¤šç¨®æ–‡ç« é¡å‹å’Œå¯«ä½œé¢¨æ ¼\n",
    "3. **å“è³ªå°å‘**ï¼šå…§å»ºå¤šé …å“è³ªæª¢æŸ¥æŒ‡æ¨™\n",
    "4. **çµæ§‹åŒ–è¼¸å‡º**ï¼šå®Œæ•´çš„å…ƒæ•¸æ“šå’Œçµæ§‹è³‡è¨Š\n",
    "\n",
    "### æ”¯æ´çš„æ–‡ç« é¡å‹\n",
    "- **Academic**: å­¸è¡“æ€§æ–‡ç« ï¼Œåš´è¬¹é‚è¼¯\n",
    "- **Blog**: éƒ¨è½æ ¼æ–‡ç« ï¼Œæ˜“è®€æœ‰è¶£\n",
    "- **Report**: å°ˆæ¥­å ±å‘Šï¼Œçµæ§‹å®Œæ•´\n",
    "- **Tutorial**: æ•™å­¸æ–‡ä»¶ï¼Œå¾ªåºæ¼¸é€²\n",
    "\n",
    "### å“è³ªè©•ä¼°ç¶­åº¦\n",
    "- **å¯è®€æ€§**: å¥å­é•·åº¦ã€èªè¨€æ¸…æ™°åº¦\n",
    "- **é€£è²«æ€§**: é‚è¼¯çµæ§‹ã€æ®µè½éŠœæ¥\n",
    "- **å®Œæ•´æ€§**: å­—æ•¸é”æˆã€çµæ§‹å®Œæ•´\n",
    "- **é¢¨æ ¼ä¸€è‡´æ€§**: èªèª¿ã€äººç¨±çµ±ä¸€\n",
    "- **çµæ§‹å“è³ª**: ç« ç¯€å¹³è¡¡ã€çµ„ç¹”åˆç†\n",
    "\n",
    "### è¼¸å‡ºæ ¼å¼\n",
    "- **Markdown æ–‡ä»¶**: é©åˆç™¼å¸ƒå’Œåˆ†äº«\n",
    "- **JSON è³‡æ–™**: åŒ…å«å®Œæ•´å…ƒæ•¸æ“šå’Œçµæ§‹è³‡è¨Š\n",
    "- **å“è³ªå ±å‘Š**: è©³ç´°çš„è©•ä¼°çµæœå’Œæ”¹é€²å»ºè­°\n",
    "\n",
    "### ä¸‹ä¸€æ­¥é å‘Š\n",
    "åœ¨ä¸‹å€‹ notebookã€Œ04_æ‰‹å‹•ç‰ˆ_å®Œæ•´æµç¨‹ã€ä¸­ï¼Œæˆ‘å€‘å°‡ï¼š\n",
    "1. æ•´åˆç ”ç©¶æ™ºèƒ½é«”èˆ‡å¯«ä½œæ™ºèƒ½é«”\n",
    "2. å»ºç«‹å®Œæ•´çš„ STORM æµç¨‹\n",
    "3. å¯¦ç¾æ™ºèƒ½é«”é–“çš„å”ä½œæ©Ÿåˆ¶\n",
    "4. é€²è¡Œç«¯åˆ°ç«¯çš„ç³»çµ±æ¸¬è©¦\n",
    "\n",
    "---\n",
    "\n",
    "*å¯«ä½œæ™ºèƒ½é«”é–‹ç™¼å®Œæˆï¼ç¾åœ¨æˆ‘å€‘æ“æœ‰äº†ä¸€å€‹èƒ½å¤ æ ¹æ“šç ”ç©¶çµæœç”Ÿæˆé«˜å“è³ªé•·æ–‡çš„æ™ºèƒ½å¯«ä½œç³»çµ±ï¼Œå…·å‚™å®Œæ•´çš„å¤§ç¶±è¦åŠƒã€å…§å®¹å‰µä½œå’Œå“è³ªæª¢æŸ¥èƒ½åŠ›ã€‚*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}