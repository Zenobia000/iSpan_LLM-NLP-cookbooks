{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_æ‰‹å‹•ç‰ˆ_ç ”ç©¶æ™ºèƒ½é«”\n",
    "\n",
    "## ğŸ¯ å­¸ç¿’ç›®æ¨™\n",
    "- å¯¦ä½œ STORM ç¬¬ä¸€éšæ®µï¼šçŸ¥è­˜æ¢ç´¢\n",
    "- é–‹ç™¼å¤šè§’åº¦å•é¡Œç”Ÿæˆæ©Ÿåˆ¶\n",
    "- å»ºç«‹ç¶²è·¯è³‡æ–™æ”¶é›†ç³»çµ±\n",
    "- å¯¦ç¾è³‡è¨Šæ•´ç†èˆ‡é©—è­‰åŠŸèƒ½\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒæº–å‚™\n",
    "\n",
    "### è¼‰å…¥å¿…è¦å¥—ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¤å¥—ä»¶\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# LangChain ç›¸é—œ\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# ç¶²è·¯æœå°‹èˆ‡å…§å®¹æ“·å–\n",
    "from duckduckgo_search import DDGS\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ç’°å¢ƒè®Šæ•¸\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… å¥—ä»¶è¼‰å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆå§‹åŒ–åŸºç¤å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ– LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "print(\"âœ… LLM åˆå§‹åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. å°ˆå®¶è§’è‰²å®šç¾©\n",
    "\n",
    "### å»ºç«‹å¤šå…ƒå°ˆå®¶è¦–è§’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertPersona:\n",
    "    \"\"\"å°ˆå®¶è§’è‰²é¡åˆ¥\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, expertise: str, perspective: str, question_style: str):\n",
    "        self.name = name\n",
    "        self.expertise = expertise\n",
    "        self.perspective = perspective\n",
    "        self.question_style = question_style\n",
    "    \n",
    "    def get_system_prompt(self) -> str:\n",
    "        \"\"\"å–å¾—å°ˆå®¶çš„ç³»çµ±æç¤ºè©\"\"\"\n",
    "        return f\"\"\"\n",
    "ä½ æ˜¯ä¸€ä½{self.expertise}å°ˆå®¶ï¼Œåå«{self.name}ã€‚\n",
    "\n",
    "ä½ çš„å°ˆæ¥­èƒŒæ™¯ï¼š{self.expertise}\n",
    "ä½ çš„è§€é»ç‰¹è‰²ï¼š{self.perspective}\n",
    "ä½ çš„æå•é¢¨æ ¼ï¼š{self.question_style}\n",
    "\n",
    "è«‹æ ¹æ“šä½ çš„å°ˆæ¥­èƒŒæ™¯å’Œè§€é»ï¼Œé‡å°çµ¦å®šçš„ä¸»é¡Œæå‡ºæ·±åº¦å•é¡Œã€‚\n",
    "ä½ çš„å•é¡Œæ‡‰è©²ï¼š\n",
    "1. é«”ç¾ä½ çš„å°ˆæ¥­é ˜åŸŸç‰¹è‰²\n",
    "2. å…·æœ‰å¯¦éš›åƒ¹å€¼å’Œå¯ç ”ç©¶æ€§\n",
    "3. èƒ½å¤ æŒ–æ˜ä¸»é¡Œçš„æ·±å±¤é¢å‘\n",
    "4. ç¬¦åˆä½ çš„æå•é¢¨æ ¼\n",
    "\"\"\"\n",
    "\n",
    "# å®šç¾©å¤šç¨®å°ˆå®¶è§’è‰²\n",
    "expert_personas = {\n",
    "    \"æŠ€è¡“å°ˆå®¶\": ExpertPersona(\n",
    "        name=\"Dr. Tech\",\n",
    "        expertise=\"æŠ€è¡“å¯¦ç¾èˆ‡ç³»çµ±æ¶æ§‹\",\n",
    "        perspective=\"æ³¨é‡æŠ€è¡“å¯è¡Œæ€§ã€æ•ˆèƒ½å„ªåŒ–å’Œå¯¦ä½œç´°ç¯€\",\n",
    "        question_style=\"ç²¾ç¢ºã€å…·é«”ã€è‘—é‡å¯¦ä½œå±¤é¢\"\n",
    "    ),\n",
    "    \n",
    "    \"å•†æ¥­åˆ†æå¸«\": ExpertPersona(\n",
    "        name=\"Ms. Business\",\n",
    "        expertise=\"å¸‚å ´åˆ†æèˆ‡å•†æ¥­ç­–ç•¥\",\n",
    "        perspective=\"é—œæ³¨å•†æ¥­åƒ¹å€¼ã€å¸‚å ´éœ€æ±‚å’Œç²åˆ©æ¨¡å¼\",\n",
    "        question_style=\"ç­–ç•¥æ€§ã€æ³¨é‡ROIå’Œå¸‚å ´å½±éŸ¿\"\n",
    "    ),\n",
    "    \n",
    "    \"ç”¨æˆ¶é«”é©—å°ˆå®¶\": ExpertPersona(\n",
    "        name=\"Prof. UX\",\n",
    "        expertise=\"ç”¨æˆ¶é«”é©—è¨­è¨ˆèˆ‡è¡Œç‚ºç ”ç©¶\",\n",
    "        perspective=\"ä»¥ç”¨æˆ¶ç‚ºä¸­å¿ƒï¼Œé—œæ³¨æ˜“ç”¨æ€§å’Œä½¿ç”¨è€…éœ€æ±‚\",\n",
    "        question_style=\"åŒç†å¿ƒå°å‘ã€æ³¨é‡å¯¦éš›ä½¿ç”¨å ´æ™¯\"\n",
    "    ),\n",
    "    \n",
    "    \"å­¸è¡“ç ”ç©¶è€…\": ExpertPersona(\n",
    "        name=\"Prof. Academic\",\n",
    "        expertise=\"ç†è«–ç ”ç©¶èˆ‡å­¸è¡“åˆ†æ\",\n",
    "        perspective=\"æ³¨é‡ç†è«–åŸºç¤ã€ç ”ç©¶æ–¹æ³•å’Œå­¸è¡“åƒ¹å€¼\",\n",
    "        question_style=\"æ·±åº¦åˆ†æã€ç†è«–å°å‘ã€æ‰¹åˆ¤æ€§æ€è€ƒ\"\n",
    "    ),\n",
    "    \n",
    "    \"æ”¿ç­–åˆ¶å®šè€…\": ExpertPersona(\n",
    "        name=\"Dr. Policy\",\n",
    "        expertise=\"æ”¿ç­–åˆ¶å®šèˆ‡ç¤¾æœƒå½±éŸ¿è©•ä¼°\",\n",
    "        perspective=\"é—œæ³¨ç¤¾æœƒå½±éŸ¿ã€å€«ç†è€ƒé‡å’Œæ”¿ç­–å¯è¡Œæ€§\",\n",
    "        question_style=\"å…¨é¢æ€§è€ƒé‡ã€æ³¨é‡ç¤¾æœƒè²¬ä»»\"\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"âœ… å·²å®šç¾© {len(expert_personas)} ç¨®å°ˆå®¶è§’è‰²\")\n",
    "for name, persona in expert_personas.items():\n",
    "    print(f\"   - {name}: {persona.expertise}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. å¤šè§’åº¦å•é¡Œç”Ÿæˆå™¨\n",
    "\n",
    "### å¯¦ä½œå•é¡Œç”Ÿæˆé‚è¼¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGenerator:\n",
    "    \"\"\"å¤šè§’åº¦å•é¡Œç”Ÿæˆå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, expert_personas):\n",
    "        self.llm = llm\n",
    "        self.expert_personas = expert_personas\n",
    "    \n",
    "    def generate_questions_from_expert(self, topic: str, expert_name: str, num_questions: int = 3) -> List[str]:\n",
    "        \"\"\"å¾ç‰¹å®šå°ˆå®¶è§’åº¦ç”Ÿæˆå•é¡Œ\"\"\"\n",
    "        if expert_name not in self.expert_personas:\n",
    "            raise ValueError(f\"æœªçŸ¥çš„å°ˆå®¶è§’è‰²: {expert_name}\")\n",
    "        \n",
    "        expert = self.expert_personas[expert_name]\n",
    "        \n",
    "        # å»ºç«‹æç¤ºè©\n",
    "        system_prompt = expert.get_system_prompt()\n",
    "        human_prompt = f\"\"\"\n",
    "ä¸»é¡Œï¼š{topic}\n",
    "\n",
    "è«‹é‡å°é€™å€‹ä¸»é¡Œï¼Œæå‡º {num_questions} å€‹å…·æœ‰æ·±åº¦ä¸”æœ‰åƒ¹å€¼çš„ç ”ç©¶å•é¡Œã€‚\n",
    "æ¯å€‹å•é¡Œéƒ½è¦ï¼š\n",
    "1. é«”ç¾ä½ çš„å°ˆæ¥­ç‰¹è‰²\n",
    "2. å…·æœ‰å¯ç ”ç©¶æ€§\n",
    "3. èƒ½å¤ ç”¢ç”Ÿæœ‰ç”¨çš„æ´å¯Ÿ\n",
    "\n",
    "è«‹ä»¥ä»¥ä¸‹æ ¼å¼å›ç­”ï¼Œæ¯å€‹å•é¡Œä¸€è¡Œï¼š\n",
    "Q1: [å•é¡Œå…§å®¹]\n",
    "Q2: [å•é¡Œå…§å®¹]\n",
    "Q3: [å•é¡Œå…§å®¹]\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # å‘¼å« LLM\n",
    "            messages = [\n",
    "                SystemMessage(content=system_prompt),\n",
    "                HumanMessage(content=human_prompt)\n",
    "            ]\n",
    "            \n",
    "            response = self.llm.invoke(messages)\n",
    "            \n",
    "            # è§£æå›æ‡‰\n",
    "            questions = []\n",
    "            lines = response.content.strip().split('\\n')\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line.startswith('Q') and ':' in line:\n",
    "                    question = line.split(':', 1)[1].strip()\n",
    "                    questions.append(question)\n",
    "            \n",
    "            return questions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ç”Ÿæˆå•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def generate_comprehensive_questions(self, topic: str, questions_per_expert: int = 2) -> Dict[str, List[str]]:\n",
    "        \"\"\"å¾æ‰€æœ‰å°ˆå®¶è§’åº¦ç”Ÿæˆç¶œåˆå•é¡Œé›†\"\"\"\n",
    "        all_questions = {}\n",
    "        \n",
    "        print(f\"ğŸ¤” æ­£åœ¨å¾å¤šå€‹å°ˆå®¶è§’åº¦åˆ†æä¸»é¡Œï¼š{topic}\")\n",
    "        \n",
    "        for expert_name in self.expert_personas.keys():\n",
    "            print(f\"   â³ {expert_name} æ€è€ƒä¸­...\")\n",
    "            \n",
    "            questions = self.generate_questions_from_expert(\n",
    "                topic, expert_name, questions_per_expert\n",
    "            )\n",
    "            \n",
    "            all_questions[expert_name] = questions\n",
    "            \n",
    "            print(f\"   âœ… {expert_name} æå‡ºäº† {len(questions)} å€‹å•é¡Œ\")\n",
    "            \n",
    "            # é¿å… API é™åˆ¶ï¼Œç¨ä½œå»¶é²\n",
    "            time.sleep(1)\n",
    "        \n",
    "        return all_questions\n",
    "\n",
    "# å»ºç«‹å•é¡Œç”Ÿæˆå™¨\n",
    "question_generator = QuestionGenerator(llm, expert_personas)\n",
    "print(\"âœ… å•é¡Œç”Ÿæˆå™¨å·²å»ºç«‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¸¬è©¦å•é¡Œç”ŸæˆåŠŸèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦ä¸»é¡Œ\n",
    "test_topic = \"äººå·¥æ™ºæ…§åœ¨æ•™è‚²ä¸­çš„æ‡‰ç”¨\"\n",
    "\n",
    "print(f\"ğŸ¯ æ¸¬è©¦ä¸»é¡Œï¼š{test_topic}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# å¾å–®ä¸€å°ˆå®¶è§’åº¦ç”Ÿæˆå•é¡Œ\n",
    "tech_questions = question_generator.generate_questions_from_expert(\n",
    "    test_topic, \"æŠ€è¡“å°ˆå®¶\", 3\n",
    ")\n",
    "\n",
    "print(\"ğŸ”§ æŠ€è¡“å°ˆå®¶çš„å•é¡Œï¼š\")\n",
    "for i, question in enumerate(tech_questions, 1):\n",
    "    print(f\"   {i}. {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. ç¶²è·¯è³‡æ–™æ”¶é›†å™¨\n",
    "\n",
    "### å¯¦ä½œæœå°‹èˆ‡å…§å®¹æ“·å–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebResearcher:\n",
    "    \"\"\"ç¶²è·¯ç ”ç©¶å“¡ - è² è²¬æœå°‹å’Œæ“·å–ç¶²è·¯å…§å®¹\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ddgs = DDGS()\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    def search_web(self, query: str, max_results: int = 5) -> List[Dict]:\n",
    "        \"\"\"ç¶²è·¯æœå°‹\"\"\"\n",
    "        try:\n",
    "            print(f\"   ğŸ” æœå°‹ï¼š{query}\")\n",
    "            results = list(self.ddgs.text(query, max_results=max_results))\n",
    "            print(f\"   ğŸ“ æ‰¾åˆ° {len(results)} ç­†çµæœ\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ æœå°‹å¤±æ•—: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_content(self, url: str, max_length: int = 2000) -> str:\n",
    "        \"\"\"æ“·å–ç¶²é å…§å®¹\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ \n",
    "            for element in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\"]):\n",
    "                element.decompose()\n",
    "            \n",
    "            # æ“·å–æ–‡å­—\n",
    "            text = soup.get_text()\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            clean_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            # é™åˆ¶é•·åº¦\n",
    "            if len(clean_text) > max_length:\n",
    "                clean_text = clean_text[:max_length] + \"...\"\n",
    "            \n",
    "            return clean_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"å…§å®¹æ“·å–å¤±æ•—: {str(e)}\"\n",
    "    \n",
    "    def research_question(self, question: str, max_sources: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"é‡å°ç‰¹å®šå•é¡Œé€²è¡Œç ”ç©¶\"\"\"\n",
    "        print(f\"ğŸ“š ç ”ç©¶å•é¡Œï¼š{question}\")\n",
    "        \n",
    "        # æœå°‹ç›¸é—œè³‡æ–™\n",
    "        search_results = self.search_web(question, max_sources)\n",
    "        \n",
    "        # æ“·å–å…§å®¹\n",
    "        sources = []\n",
    "        for result in search_results:\n",
    "            print(f\"   ğŸ“– æ“·å–ï¼š{result['title'][:50]}...\")\n",
    "            \n",
    "            content = self.extract_content(result['href'])\n",
    "            \n",
    "            source_info = {\n",
    "                'title': result['title'],\n",
    "                'url': result['href'],\n",
    "                'snippet': result.get('body', ''),\n",
    "                'content': content[:1500] if content else '',  # é™åˆ¶å…§å®¹é•·åº¦\n",
    "                'extracted_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            sources.append(source_info)\n",
    "            \n",
    "            # é¿å…éæ–¼é »ç¹çš„è«‹æ±‚\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'sources': sources,\n",
    "            'research_date': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# å»ºç«‹ç¶²è·¯ç ”ç©¶å“¡\n",
    "web_researcher = WebResearcher()\n",
    "print(\"âœ… ç¶²è·¯ç ”ç©¶å“¡å·²å»ºç«‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¸¬è©¦ç¶²è·¯ç ”ç©¶åŠŸèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦å–®ä¸€å•é¡Œç ”ç©¶\n",
    "test_question = \"AIæ•™è‚²æ‡‰ç”¨çš„æœ€æ–°æŠ€è¡“è¶¨å‹¢æ˜¯ä»€éº¼ï¼Ÿ\"\n",
    "\n",
    "research_result = web_researcher.research_question(test_question, 2)\n",
    "\n",
    "print(\"\\nğŸ“Š ç ”ç©¶çµæœæ‘˜è¦ï¼š\")\n",
    "print(f\"å•é¡Œï¼š{research_result['question']}\")\n",
    "print(f\"æ‰¾åˆ° {len(research_result['sources'])} å€‹ä¾†æº\")\n",
    "\n",
    "for i, source in enumerate(research_result['sources'], 1):\n",
    "    print(f\"\\nä¾†æº {i}:\")\n",
    "    print(f\"  æ¨™é¡Œï¼š{source['title'][:60]}...\")\n",
    "    print(f\"  ç¶²å€ï¼š{source['url'][:60]}...\")\n",
    "    print(f\"  å…§å®¹é•·åº¦ï¼š{len(source['content'])} å­—å…ƒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. è³‡è¨Šåˆ†æèˆ‡æ‘˜è¦å™¨\n",
    "\n",
    "### å¯¦ä½œå…§å®¹åˆ†æåŠŸèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentAnalyzer:\n",
    "    \"\"\"å…§å®¹åˆ†æå™¨ - åˆ†æå’Œæ‘˜è¦ç ”ç©¶å…§å®¹\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def analyze_research_content(self, research_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"åˆ†æç ”ç©¶å…§å®¹ä¸¦ç”Ÿæˆæ‘˜è¦\"\"\"\n",
    "        question = research_data['question']\n",
    "        sources = research_data['sources']\n",
    "        \n",
    "        print(f\"ğŸ§  åˆ†æç ”ç©¶å…§å®¹ï¼š{question[:50]}...\")\n",
    "        \n",
    "        # åˆä½µæ‰€æœ‰ä¾†æºå…§å®¹\n",
    "        all_content = \"\"\n",
    "        for i, source in enumerate(sources, 1):\n",
    "            if source['content'] and 'æ“·å–å¤±æ•—' not in source['content']:\n",
    "                all_content += f\"\\n\\nä¾†æº {i} ({source['title'][:30]}...):\\n{source['content']}\"\n",
    "        \n",
    "        if not all_content.strip():\n",
    "            return {\n",
    "                'question': question,\n",
    "                'summary': 'ç„¡æ³•ç²å¾—æœ‰æ•ˆå…§å®¹é€²è¡Œåˆ†æ',\n",
    "                'key_points': [],\n",
    "                'insights': '',\n",
    "                'confidence': 0.0\n",
    "            }\n",
    "        \n",
    "        # ç”Ÿæˆåˆ†ææç¤ºè©\n",
    "        analysis_prompt = f\"\"\"\n",
    "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç ”ç©¶åˆ†æå¸«ã€‚è«‹é‡å°ä»¥ä¸‹å•é¡Œå’Œç›¸é—œè³‡æ–™é€²è¡Œæ·±åº¦åˆ†æã€‚\n",
    "\n",
    "ç ”ç©¶å•é¡Œï¼š{question}\n",
    "\n",
    "ç›¸é—œè³‡æ–™ï¼š\n",
    "{all_content[:3000]}  # é™åˆ¶å…§å®¹é•·åº¦é¿å…è¶…å‡ºtokené™åˆ¶\n",
    "\n",
    "è«‹æä¾›ä»¥ä¸‹åˆ†æï¼š\n",
    "\n",
    "1. ç¶œåˆæ‘˜è¦ (2-3å¥è©±ç¸½çµå›ç­”)ï¼š\n",
    "[æ‘˜è¦å…§å®¹]\n",
    "\n",
    "2. é—œéµè¦é» (3-5å€‹é‡é»ï¼Œæ¯å€‹ä¸€è¡Œ)ï¼š\n",
    "- [è¦é»1]\n",
    "- [è¦é»2]\n",
    "- [è¦é»3]\n",
    "\n",
    "3. æ·±åº¦æ´å¯Ÿ (å°ˆæ¥­è¦‹è§£å’Œè¶¨å‹¢åˆ†æ)ï¼š\n",
    "[æ´å¯Ÿå…§å®¹]\n",
    "\n",
    "4. å¯ä¿¡åº¦è©•ä¼° (1-10åˆ†ï¼Œ10åˆ†æœ€é«˜)ï¼š\n",
    "[åˆ†æ•¸]\n",
    "\n",
    "è«‹ç¢ºä¿åˆ†æå®¢è§€ã€æº–ç¢ºï¼ŒåŸºæ–¼æä¾›çš„è³‡æ–™å…§å®¹ã€‚\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=analysis_prompt)])\n",
    "            analysis_text = response.content\n",
    "            \n",
    "            # è§£æå›æ‡‰å…§å®¹\n",
    "            summary = self._extract_section(analysis_text, '1. ç¶œåˆæ‘˜è¦', '2. é—œéµè¦é»')\n",
    "            key_points = self._extract_key_points(analysis_text)\n",
    "            insights = self._extract_section(analysis_text, '3. æ·±åº¦æ´å¯Ÿ', '4. å¯ä¿¡åº¦è©•ä¼°')\n",
    "            confidence = self._extract_confidence(analysis_text)\n",
    "            \n",
    "            return {\n",
    "                'question': question,\n",
    "                'summary': summary,\n",
    "                'key_points': key_points,\n",
    "                'insights': insights,\n",
    "                'confidence': confidence,\n",
    "                'source_count': len(sources),\n",
    "                'analysis_date': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å…§å®¹åˆ†æå¤±æ•—: {str(e)}\")\n",
    "            return {\n",
    "                'question': question,\n",
    "                'summary': 'åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤',\n",
    "                'key_points': [],\n",
    "                'insights': '',\n",
    "                'confidence': 0.0\n",
    "            }\n",
    "    \n",
    "    def _extract_section(self, text: str, start_marker: str, end_marker: str) -> str:\n",
    "        \"\"\"å¾æ–‡æœ¬ä¸­æå–ç‰¹å®šå€æ®µ\"\"\"\n",
    "        start_idx = text.find(start_marker)\n",
    "        if start_idx == -1:\n",
    "            return \"\"\n",
    "        \n",
    "        start_idx = text.find(':', start_idx) + 1\n",
    "        end_idx = text.find(end_marker, start_idx)\n",
    "        \n",
    "        if end_idx == -1:\n",
    "            section = text[start_idx:]\n",
    "        else:\n",
    "            section = text[start_idx:end_idx]\n",
    "        \n",
    "        return section.strip().strip('[]')\n",
    "    \n",
    "    def _extract_key_points(self, text: str) -> List[str]:\n",
    "        \"\"\"æå–é—œéµè¦é»\"\"\"\n",
    "        key_points = []\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        in_key_points_section = False\n",
    "        for line in lines:\n",
    "            if '2. é—œéµè¦é»' in line:\n",
    "                in_key_points_section = True\n",
    "                continue\n",
    "            elif '3. æ·±åº¦æ´å¯Ÿ' in line:\n",
    "                break\n",
    "            \n",
    "            if in_key_points_section and line.strip().startswith('-'):\n",
    "                point = line.strip()[1:].strip().strip('[]')\n",
    "                if point:\n",
    "                    key_points.append(point)\n",
    "        \n",
    "        return key_points\n",
    "    \n",
    "    def _extract_confidence(self, text: str) -> float:\n",
    "        \"\"\"æå–å¯ä¿¡åº¦åˆ†æ•¸\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # å°‹æ‰¾æ•¸å­—åˆ†æ•¸\n",
    "        confidence_match = re.search(r'4\\. å¯ä¿¡åº¦è©•ä¼°[^\\d]*(\\d+(?:\\.\\d+)?)', text)\n",
    "        if confidence_match:\n",
    "            try:\n",
    "                score = float(confidence_match.group(1))\n",
    "                return min(score / 10.0, 1.0)  # è½‰æ›ç‚º 0-1 ç¯„åœ\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return 0.5  # é è¨­ä¸­ç­‰å¯ä¿¡åº¦\n",
    "\n",
    "# å»ºç«‹å…§å®¹åˆ†æå™¨\n",
    "content_analyzer = ContentAnalyzer(llm)\n",
    "print(\"âœ… å…§å®¹åˆ†æå™¨å·²å»ºç«‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¸¬è©¦å…§å®¹åˆ†æåŠŸèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æä¹‹å‰çš„ç ”ç©¶çµæœ\n",
    "analysis_result = content_analyzer.analyze_research_content(research_result)\n",
    "\n",
    "print(\"\\nğŸ“ˆ åˆ†æçµæœï¼š\")\n",
    "print(f\"å•é¡Œï¼š{analysis_result['question']}\")\n",
    "print(f\"\\næ‘˜è¦ï¼š{analysis_result['summary']}\")\n",
    "print(f\"\\né—œéµè¦é»ï¼š\")\n",
    "for i, point in enumerate(analysis_result['key_points'], 1):\n",
    "    print(f\"  {i}. {point}\")\n",
    "print(f\"\\næ·±åº¦æ´å¯Ÿï¼š{analysis_result['insights']}\")\n",
    "print(f\"\\nå¯ä¿¡åº¦ï¼š{analysis_result['confidence']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. å®Œæ•´ç ”ç©¶æ™ºèƒ½é«”\n",
    "\n",
    "### æ•´åˆæ‰€æœ‰åŠŸèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAgent:\n",
    "    \"\"\"ç ”ç©¶æ™ºèƒ½é«” - æ•´åˆå•é¡Œç”Ÿæˆã€è³‡æ–™æœå°‹ã€å…§å®¹åˆ†æ\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, expert_personas):\n",
    "        self.llm = llm\n",
    "        self.question_generator = QuestionGenerator(llm, expert_personas)\n",
    "        self.web_researcher = WebResearcher()\n",
    "        self.content_analyzer = ContentAnalyzer(llm)\n",
    "        self.expert_personas = expert_personas\n",
    "    \n",
    "    def research_topic(self, topic: str, questions_per_expert: int = 2, sources_per_question: int = 2) -> Dict[str, Any]:\n",
    "        \"\"\"å°ä¸»é¡Œé€²è¡Œå®Œæ•´ç ”ç©¶\"\"\"\n",
    "        print(f\"ğŸ”¬ é–‹å§‹ç ”ç©¶ä¸»é¡Œï¼š{topic}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå¤šè§’åº¦å•é¡Œ\n",
    "        print(\"\\nğŸ“‹ éšæ®µ 1: ç”Ÿæˆç ”ç©¶å•é¡Œ\")\n",
    "        all_questions = self.question_generator.generate_comprehensive_questions(\n",
    "            topic, questions_per_expert\n",
    "        )\n",
    "        \n",
    "        # æ•´ç†æ‰€æœ‰å•é¡Œ\n",
    "        research_questions = []\n",
    "        for expert_name, questions in all_questions.items():\n",
    "            for question in questions:\n",
    "                research_questions.append({\n",
    "                    'question': question,\n",
    "                    'expert_perspective': expert_name,\n",
    "                    'expert_info': self.expert_personas[expert_name].expertise\n",
    "                })\n",
    "        \n",
    "        print(f\"\\nâœ… ç¸½å…±ç”Ÿæˆäº† {len(research_questions)} å€‹ç ”ç©¶å•é¡Œ\")\n",
    "        \n",
    "        # ç¬¬äºŒæ­¥ï¼šé‡å°æ¯å€‹å•é¡Œé€²è¡Œç ”ç©¶\n",
    "        print(\"\\nğŸ” éšæ®µ 2: è³‡æ–™æ”¶é›†èˆ‡åˆ†æ\")\n",
    "        research_results = []\n",
    "        \n",
    "        for i, q_info in enumerate(research_questions, 1):\n",
    "            print(f\"\\n--- ç ”ç©¶å•é¡Œ {i}/{len(research_questions)} ---\")\n",
    "            print(f\"å°ˆå®¶è§’åº¦ï¼š{q_info['expert_perspective']}\")\n",
    "            \n",
    "            # ç¶²è·¯æœå°‹\n",
    "            raw_research = self.web_researcher.research_question(\n",
    "                q_info['question'], sources_per_question\n",
    "            )\n",
    "            \n",
    "            # å…§å®¹åˆ†æ\n",
    "            analysis = self.content_analyzer.analyze_research_content(raw_research)\n",
    "            \n",
    "            # æ•´åˆçµæœ\n",
    "            research_results.append({\n",
    "                'question_info': q_info,\n",
    "                'raw_research': raw_research,\n",
    "                'analysis': analysis\n",
    "            })\n",
    "            \n",
    "            print(f\"âœ… å®Œæˆåˆ†æï¼Œå¯ä¿¡åº¦ï¼š{analysis['confidence']:.1%}\")\n",
    "        \n",
    "        # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆç¶œåˆå ±å‘Š\n",
    "        print(\"\\nğŸ“Š éšæ®µ 3: ç”Ÿæˆç¶œåˆå ±å‘Š\")\n",
    "        comprehensive_report = self._generate_comprehensive_report(topic, research_results)\n",
    "        \n",
    "        final_result = {\n",
    "            'topic': topic,\n",
    "            'research_date': datetime.now().isoformat(),\n",
    "            'expert_questions': all_questions,\n",
    "            'detailed_research': research_results,\n",
    "            'comprehensive_report': comprehensive_report,\n",
    "            'statistics': {\n",
    "                'total_questions': len(research_questions),\n",
    "                'total_sources': sum(len(r['raw_research']['sources']) for r in research_results),\n",
    "                'average_confidence': sum(r['analysis']['confidence'] for r in research_results) / len(research_results)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nğŸ‰ ç ”ç©¶å®Œæˆï¼\")\n",
    "        print(f\"   - ç ”ç©¶å•é¡Œï¼š{final_result['statistics']['total_questions']} å€‹\")\n",
    "        print(f\"   - è³‡æ–™ä¾†æºï¼š{final_result['statistics']['total_sources']} å€‹\")\n",
    "        print(f\"   - å¹³å‡å¯ä¿¡åº¦ï¼š{final_result['statistics']['average_confidence']:.1%}\")\n",
    "        \n",
    "        return final_result\n",
    "    \n",
    "    def _generate_comprehensive_report(self, topic: str, research_results: List[Dict]) -> str:\n",
    "        \"\"\"ç”Ÿæˆç¶œåˆç ”ç©¶å ±å‘Š\"\"\"\n",
    "        # å½™æ•´æ‰€æœ‰é—œéµç™¼ç¾\n",
    "        all_summaries = []\n",
    "        all_key_points = []\n",
    "        all_insights = []\n",
    "        \n",
    "        for result in research_results:\n",
    "            analysis = result['analysis']\n",
    "            if analysis['summary']:\n",
    "                all_summaries.append(f\"â€¢ {analysis['summary']}\")\n",
    "            all_key_points.extend(analysis['key_points'])\n",
    "            if analysis['insights']:\n",
    "                all_insights.append(analysis['insights'])\n",
    "        \n",
    "        # çµ„åˆå ±å‘Š\n",
    "        report = f\"\"\"\n",
    "# {topic} - ç¶œåˆç ”ç©¶å ±å‘Š\n",
    "\n",
    "## é—œéµç™¼ç¾æ‘˜è¦\n",
    "{\"\".join([f\"\\n{summary}\" for summary in all_summaries[:5]])}\n",
    "\n",
    "## é‡è¦è¦é»\n",
    "{\"\".join([f\"\\nâ€¢ {point}\" for point in all_key_points[:10]])}\n",
    "\n",
    "## å°ˆå®¶æ´å¯Ÿ\n",
    "{\"\".join([f\"\\n\\n{insight}\" for insight in all_insights[:3]])}\n",
    "\n",
    "---\n",
    "*æœ¬å ±å‘ŠåŸºæ–¼ {len(research_results)} å€‹ç ”ç©¶å•é¡Œçš„åˆ†æçµæœç”Ÿæˆ*\n",
    "\"\"\"\n",
    "        \n",
    "        return report.strip()\n",
    "\n",
    "# å»ºç«‹å®Œæ•´çš„ç ”ç©¶æ™ºèƒ½é«”\n",
    "research_agent = ResearchAgent(llm, expert_personas)\n",
    "print(\"âœ… ç ”ç©¶æ™ºèƒ½é«”å·²å»ºç«‹å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. å®Œæ•´æ¸¬è©¦\n",
    "\n",
    "### åŸ·è¡Œå®Œæ•´ç ”ç©¶æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸ·è¡Œå®Œæ•´ç ”ç©¶\n",
    "research_topic = \"ChatGPTå°å‚³çµ±æ•™è‚²æ¨¡å¼çš„å½±éŸ¿\"\n",
    "\n",
    "# æ³¨æ„ï¼šé€™å°‡èŠ±è²»ä¸€äº›æ™‚é–“å’ŒAPIè²»ç”¨\n",
    "# å»ºè­°å…ˆç”¨è¼ƒå°‘çš„åƒæ•¸é€²è¡Œæ¸¬è©¦\n",
    "full_research = research_agent.research_topic(\n",
    "    topic=research_topic,\n",
    "    questions_per_expert=1,  # æ¯å€‹å°ˆå®¶1å€‹å•é¡Œ\n",
    "    sources_per_question=2   # æ¯å€‹å•é¡Œ2å€‹ä¾†æº\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æŸ¥çœ‹ç ”ç©¶çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¡¯ç¤ºç¶œåˆå ±å‘Š\n",
    "print(\"ğŸ“Š ç¶œåˆç ”ç©¶å ±å‘Š:\")\n",
    "print(\"=\" * 60)\n",
    "print(full_research['comprehensive_report'])\n",
    "\n",
    "print(\"\\n\\nğŸ“ˆ ç ”ç©¶çµ±è¨ˆ:\")\n",
    "print(\"=\" * 30)\n",
    "stats = full_research['statistics']\n",
    "print(f\"ç ”ç©¶å•é¡Œç¸½æ•¸ï¼š{stats['total_questions']}\")\n",
    "print(f\"è³‡æ–™ä¾†æºç¸½æ•¸ï¼š{stats['total_sources']}\")\n",
    "print(f\"å¹³å‡å¯ä¿¡åº¦ï¼š{stats['average_confidence']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è©³ç´°å•é¡Œåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹å„å°ˆå®¶çš„å•é¡Œ\n",
    "print(\"ğŸ¤” å„å°ˆå®¶è§’åº¦çš„ç ”ç©¶å•é¡Œ:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for expert_name, questions in full_research['expert_questions'].items():\n",
    "    print(f\"\\nğŸ‘¨â€ğŸ« {expert_name}:\")\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"   {i}. {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. çµæœä¿å­˜\n",
    "\n",
    "### å°‡ç ”ç©¶çµæœä¿å­˜ç‚º JSON æª”æ¡ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ç ”ç©¶çµæœ\n",
    "import json\n",
    "\n",
    "# ç”¢ç”Ÿæª”æ¡ˆåç¨±\n",
    "safe_topic = research_topic.replace(' ', '_').replace('/', '_')\n",
    "filename = f\"research_result_{safe_topic}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "# ä¿å­˜æª”æ¡ˆ\n",
    "try:\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(full_research, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… ç ”ç©¶çµæœå·²ä¿å­˜è‡³ï¼š{filename}\")\n",
    "    print(f\"   æª”æ¡ˆå¤§å°ï¼š{os.path.getsize(filename)} bytes\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ä¿å­˜å¤±æ•—ï¼š{str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ æœ¬ç« å°çµ\n",
    "\n",
    "### å®ŒæˆåŠŸèƒ½\n",
    "âœ… **å¤šè§’åº¦å•é¡Œç”Ÿæˆ**ï¼š5ç¨®å°ˆå®¶è§’è‰²ï¼Œå¾ä¸åŒè¦–è§’æå‡ºç ”ç©¶å•é¡Œ  \n",
    "âœ… **ç¶²è·¯è³‡æ–™æ”¶é›†**ï¼šè‡ªå‹•æœå°‹ä¸¦æ“·å–ç›¸é—œç¶²é å…§å®¹  \n",
    "âœ… **å…§å®¹åˆ†ææ‘˜è¦**ï¼šä½¿ç”¨ LLM åˆ†æå’Œæ‘˜è¦ç ”ç©¶å…§å®¹  \n",
    "âœ… **ç¶œåˆå ±å‘Šç”Ÿæˆ**ï¼šæ•´åˆæ‰€æœ‰ç ”ç©¶çµæœç”Ÿæˆå®Œæ•´å ±å‘Š  \n",
    "âœ… **çµæœæŒä¹…åŒ–**ï¼šå°‡ç ”ç©¶çµæœä¿å­˜ç‚º JSON æ ¼å¼  \n",
    "\n",
    "### æŠ€è¡“äº®é»\n",
    "1. **æ¨¡çµ„åŒ–è¨­è¨ˆ**ï¼šæ¯å€‹åŠŸèƒ½ç¨ç«‹å¯¦ä½œï¼Œæ˜“æ–¼æ¸¬è©¦å’Œç¶­è­·\n",
    "2. **å°ˆå®¶è§’è‰²ç³»çµ±**ï¼šæ¨¡æ“¬å¤šç¨®å°ˆæ¥­èƒŒæ™¯çš„æ€è€ƒæ–¹å¼\n",
    "3. **éŒ¯èª¤è™•ç†**ï¼šå®Œå–„çš„ç•°å¸¸è™•ç†æ©Ÿåˆ¶\n",
    "4. **å¯é…ç½®åƒæ•¸**ï¼šå•é¡Œæ•¸é‡ã€ä¾†æºæ•¸é‡ç­‰å¯èª¿æ•´\n",
    "\n",
    "### æ€§èƒ½è€ƒé‡\n",
    "- **APIå‘¼å«å„ªåŒ–**ï¼šé©ç•¶çš„å»¶é²é¿å…è§¸ç™¼é™åˆ¶\n",
    "- **å…§å®¹é•·åº¦æ§åˆ¶**ï¼šé¿å…è¶…å‡ºtokené™åˆ¶\n",
    "- **å¿«å–æ©Ÿåˆ¶**ï¼šé¿å…é‡è¤‡æœå°‹ç›¸åŒå…§å®¹\n",
    "\n",
    "### ä¸‹ä¸€æ­¥é å‘Š\n",
    "åœ¨ä¸‹å€‹ notebookã€Œ03_æ‰‹å‹•ç‰ˆ_å¯«ä½œæ™ºèƒ½é«”ã€ä¸­ï¼Œæˆ‘å€‘å°‡ï¼š\n",
    "1. åˆ©ç”¨ç ”ç©¶çµæœç”Ÿæˆæ–‡ç« å¤§ç¶±\n",
    "2. å¯¦ä½œæ®µè½å…§å®¹ç”ŸæˆåŠŸèƒ½\n",
    "3. å»ºç«‹é¢¨æ ¼ä¸€è‡´æ€§æ§åˆ¶æ©Ÿåˆ¶\n",
    "4. æ•´åˆå¼•ç”¨å’Œä¾†æºæ¨™è¨»\n",
    "\n",
    "---\n",
    "\n",
    "*ç ”ç©¶æ™ºèƒ½é«”é–‹ç™¼å®Œæˆï¼ç¾åœ¨æˆ‘å€‘æœ‰äº†ä¸€å€‹å¼·å¤§çš„å·¥å…·ï¼Œèƒ½å¤ å¾å¤šå€‹å°ˆæ¥­è§’åº¦æ·±åº¦ç ”ç©¶ä»»ä½•ä¸»é¡Œï¼Œç‚ºå¾ŒçºŒçš„å¯«ä½œæä¾›è±å¯Œçš„ç´ æã€‚*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}