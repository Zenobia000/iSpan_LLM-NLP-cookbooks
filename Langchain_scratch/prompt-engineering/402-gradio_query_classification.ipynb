{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "## 設定 OpenAI API Key 變數\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API response by openai GPT x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pprint import pprint as pp\n",
    "\n",
    "def get_completion(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=300, functions=None, function_call=None, format_type=None):\n",
    "  payload = { \"model\": model, \"temperature\": temperature, \"messages\": messages, \"max_tokens\": max_tokens }\n",
    "  if functions:\n",
    "    payload[\"functions\"] = functions\n",
    "  if function_call:\n",
    "    payload[\"function_call\"] = function_call\n",
    "  if format_type:\n",
    "    payload[\"response_format\"] =  { \"type\": format_type }\n",
    "\n",
    "  headers = { \"Authorization\": f'Bearer {openai_api_key}', \"Content-Type\": \"application/json\" }\n",
    "  response = requests.post('https://api.openai.com/v1/chat/completions', headers = headers, data = json.dumps(payload) )\n",
    "  obj = json.loads(response.text)\n",
    "  if response.status_code == 200 :\n",
    "    return obj[\"choices\"][0][\"message\"]\n",
    "  else :\n",
    "    return obj[\"error\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## long text summarization module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary_prompt = \"\"\" \n",
    "# (zh-tw)\n",
    "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary. If the lines are meaningless just return NONE\n",
    "\n",
    "EXAMPLE\n",
    "Current summary:\n",
    "The human asks who is the lead singer of Motorhead. The AI responds Lemmy Kilmister.\n",
    "\n",
    "New lines of conversation:\n",
    "Human: What are the other members of Motorhead?\n",
    "AI: The original members included Lemmy Kilmister (vocals, bass), Larry Wallis (guitar), and Lucas Fox (drums), with notable members throughout the years including \\\"Fast\\\" Eddie Clarke (guitar), Phil \\\"Philthy Animal\\\" Taylor (drums), and Mikkey Dee (drums).\n",
    "\n",
    "New summary:\n",
    "The human asks who is the lead singer and other members of Motorhead. The AI responds Lemmy Kilmister is the lead singer and other original members include Larry Wallis, and Lucas Fox, with notable past members including \\\"Fast\\\" Eddie Clarke, Phil \\\"Philthy Animal\\\" Taylor, and Mikkey Dee.\n",
    "END OF EXAMPLE\n",
    "\n",
    "Current summary:\n",
    "{prev_summary}\n",
    "New lines of conversation:\n",
    "{messages_joined}\n",
    "New summary:\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat completion mod 的 token 數量計算\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# 出自 https://platform.openai.com/docs/guides/gpt/managing-tokens\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "  \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "  try:\n",
    "      encoding = tiktoken.encoding_for_model(model)\n",
    "  except KeyError:\n",
    "      encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "  if model == \"gpt-3.5-turbo-0613\":  # note: future models may deviate from this\n",
    "      num_tokens = 0\n",
    "      for message in messages:\n",
    "          num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "          for key, value in message.items():\n",
    "              num_tokens += len(encoding.encode(value))\n",
    "              if key == \"name\":  # if there's a name, the role is omitted\n",
    "                  num_tokens += -1  # role is always required and always 1 token\n",
    "      num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "      return num_tokens\n",
    "  else:\n",
    "      raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
    "  See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_context = None # 這個變數保存目前的對話摘要\n",
    "\n",
    "def messages_to_string(messages):\n",
    "    # 只抓 user 和 assistant 的 messages\n",
    "    messages = filter(lambda m: (m[\"role\"] == 'user' or m[\"role\"] == 'assistant'), messages )\n",
    "\n",
    "    # 將 messages 轉成 string\n",
    "    dialogue = []\n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "\n",
    "        if role == \"user\":\n",
    "            dialogue.append(f\"user: {content}\")\n",
    "        elif role == \"assistant\":\n",
    "            dialogue.append(f\"assistant: {content}\")\n",
    "\n",
    "    return \"\\n\".join(dialogue)\n",
    "\n",
    "# 當輸入的 messages 超過 max_tokens 時，將所有 user 跟 assistant messages 壓縮成一個 system message\n",
    "def handle_compaction(messages, max_tokens = 1000):\n",
    "  \n",
    "  if num_tokens_from_messages(messages) < max_tokens:\n",
    "    return messages\n",
    "  \n",
    "  else:\n",
    "    # 當字數超過時，觸發摘要動作\n",
    "    global current_context\n",
    "    str = messages_to_string(messages)\n",
    "    summary_user_prompt = summary_prompt.format(prev_summary = current_context, messages_joined = str)\n",
    "    response = get_completion( [{\"role\": \"user\", \"content\": summary_user_prompt}], temperature=0)\n",
    "    current_context = response[\"content\"] ## 更新對話摘要\n",
    "\n",
    "    # 丟棄 user 和 assistant messages，只保留 system messages\n",
    "    existing_system_messages = list( filter(lambda m: (m[\"role\"] == 'system'), messages ) )\n",
    "    # 加入最新的對話摘要 system message\n",
    "    new_system_messages = [{\"role\": \"system\", \"content\": f\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n{current_context} \"\"\"} ]\n",
    "    \n",
    "    #　系統對話 + 摘要對話\n",
    "    return existing_system_messages + new_system_messages\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 實作具備問題分類的橋接器模組 (處理用戶非結構化數據)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "\n",
    "請分類用戶問題，將查詢分類為 primary 類別和 secondary 類別。請以 `JSON` 格式提供輸出，使用以下key: primary 和 secondary\n",
    "\n",
    " `JSON` 要用以下格式，最終輸出不能有任何錯誤:\n",
    "[\n",
    "  \"user request\": {\n",
    "    \"primary\": \"string\", // 主要類別\n",
    "    \"secondary\": \"string\", // 次要類別\n",
    "}]\n",
    "\n",
    "ps: 以下是公司提供的技術支援類別清單，請確保您的回應符合這個規範。若用戶回答不符合規範，請將之歸類為 un-related。 `\n",
    "primary 類別有：Billing、Technical Support, Account Management 或 General Inquiry 務必遵守這些指示，以便為用戶提供最佳的支援。\n",
    "\n",
    "### Billing: 次要類別有 (務必遵守這些分類，以便為用戶提供最佳的支援。)\n",
    "1. 取消訂閱或升級\n",
    "2. 添加付款方式\n",
    "3. 收費解釋\n",
    "4. 爭議收費\n",
    "5. 延長服務期限\n",
    "6. 發票問題處理\n",
    "\n",
    "### Technical Support: 次要類別有 (務必遵守這些分類，以便為用戶提供最佳的支援。)\n",
    "1. 故障排除\n",
    "2. 設備兼容性\n",
    "3. 軟件更新\n",
    "4. 網絡連接問題\n",
    "5. 數據恢復和備份\n",
    "6. 病毒和惡意軟件清除\n",
    " \n",
    "### Account Management: 次要類別有 (務必遵守這些分類，以便為用戶提供最佳的支援。)\n",
    "1. 重置密碼\n",
    "2. 更新個人信息\n",
    "3. 關閉帳戶\n",
    "4. 帳戶安全\n",
    "5. 訂閱管理\n",
    "6. 帳戶活動記錄查詢\n",
    "\n",
    "### General Inquiry: 次要類別有 (務必遵守這些分類，以便為用戶提供最佳的支援。)\n",
    "1. 產品信息\n",
    "2. 價格\n",
    "3. 反饋\n",
    "4. 與人聯絡\n",
    "5. 促銷和優惠\n",
    "6. 常見問題解答\n",
    "\n",
    "### un-related: 次要類別有 (務必遵守這些分類，以便為用戶提供最佳的支援。)\n",
    "1. 未知問題\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.48.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import gradio as gr\n",
    "\n",
    "desc = \"這個 **Router Assistance Bot** 提供客戶關於路由器問題的解決方案。\"\\\n",
    "       \"它能夠指導客戶進行基本的故障排除，並在必要時將他們轉接至IT支援。\"\n",
    "\n",
    "article = \"<h1> 路由器客服支援 </h1>\"\\\n",
    "          \"<h3>如何使用：</h3> \" \\\n",
    "          \"<ul><li>在提供的輸入框中描述您的問題。會有專人為你服務</li>\" \\\n",
    "          \"<li>按照線上客服工程師提供的指示進行檢查和操作。</li>\" \\\n",
    "          \"<li>若您的問題並非路由器相關問題，我們將轉接其他專員為您服務。</li>\" \\\n",
    "          \"<li>如果問題無法解決，輸入 'IT support requested' 將您轉接至IT支援。</li></ul>\"\n",
    "\n",
    "categories = []\n",
    "\n",
    "def handle_compaction(history_openai_format):\n",
    "    # Compaction logic can be added here if needed\n",
    "    return history_openai_format\n",
    "\n",
    "def router_assistance_bot(message, history):\n",
    "    global categories \n",
    "    \n",
    "    history_openai_format = [\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": system_prompt\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    for human, assistant in history:\n",
    "        history_openai_format.append({\"role\": \"user\", \"content\": human})\n",
    "        history_openai_format.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    \n",
    "    history_openai_format.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    messages = handle_compaction(history_openai_format)\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-4-turbo-preview',\n",
    "        messages=messages,\n",
    "        temperature=0.1,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    partial_message = \"\"\n",
    "    for chunk in response:\n",
    "        try:\n",
    "            partial_message += chunk.choices[0].delta.content\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        yield partial_message\n",
    "\n",
    "# Close all previous Gradio interfaces\n",
    "gr.close_all()\n",
    "\n",
    "# Launch the Gradio Chat Interface\n",
    "gr.ChatInterface(router_assistance_bot, \n",
    "                 theme=\"Soft\",   \n",
    "                 description=\"Assistance bot for routing requests\",\n",
    "                 title=\"Router Assistance Bot\"\n",
    "                ).queue().launch(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有辦法做到問題分類的模型不多，只靠 prompt 工程還是有可能輸出不穩定，必須仰賴模型的理解力，\n",
    "後續會介紹 function call 功能，確保輸出是 JSON \n",
    "\n",
    "**先透過問題分類再轉外部工具或知識庫的效果可能比較好**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
