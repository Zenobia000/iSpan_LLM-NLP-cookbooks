{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1476ee-4cac-4c75-939e-c6872292a6cc",
   "metadata": {},
   "source": [
    "# LangChain basic - component I.\n",
    "## LLMs, Prompt Templates, Caching, Streaming, Chains\n",
    "\n",
    "This notebook uses the latest versions of the OpenAI and LangChain libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7b118e8-d6e2-4deb-b621-c2f90bc8bd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dataprep 0.4.5 requires pandas<2.0,>=1.1, but you have pandas 2.1.2 which is incompatible.\n",
      "dataprep 0.4.5 requires regex<2022.0.0,>=2021.8.3, but you have regex 2023.12.25 which is incompatible.\n",
      "dataprep 0.4.5 requires sqlalchemy==1.3.24, but you have sqlalchemy 2.0.22 which is incompatible.\n",
      "shioaji 1.1.0 requires filelock==3.4.1, but you have filelock 3.12.0 which is incompatible.\n",
      "ydata-profiling 4.5.1 requires pandas!=1.4.0,<2.1,>1.1, but you have pandas 2.1.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72695b0b-0069-484d-9edd-9b75faaa8307",
   "metadata": {},
   "source": [
    "Download [requirements.txt](https://drive.google.com/file/d/1UpURYL9kqjXfe9J8o-_Dq5KJTbQpzMef/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57410c7b-edb3-4843-98b5-d71dd2d5bf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade -q langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc5a125-4737-4ab8-ba14-7d4af2685e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5444b4-384d-440e-a9a3-2b4c3b5f837c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 1.12.0\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: \n",
      "Location: c:\\users\\xdxd2\\appdata\\roaming\\python\\python310\\site-packages\n",
      "Requires: anyio, distro, httpx, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: langchain-openai, llama-index\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06fd9c8e-65ce-4422-b733-ee14876b2614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.1.6\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: c:\\users\\xdxd2\\appdata\\roaming\\python\\python310\\site-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: langchain-experimental, llama-index\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee9a618-d423-4cce-8e32-ed937698c0d5",
   "metadata": {},
   "source": [
    "### Python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e052148c-1dd2-4cd2-8878-fd90e58dabfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# loading the API Keys from .env\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654afaf9-00bc-4d4d-99ab-907055d4139c",
   "metadata": {},
   "source": [
    "## Chat Models: GPT-3.5 Turbo and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2eb526e-4105-4301-b931-f285f258035b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum mechanics is a branch of physics that deals with phenomena on a very small scale, such as molecules, atoms, and subatomic particles, where particles can exist in multiple states at once and are affected by observation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()  \n",
    "\n",
    "# invoking the llm (running the prompt)\n",
    "'''\n",
    "model（模型）: 指定生成回應所使用的模型，例如 'gpt-3.5-turbo' or 'gpt-4'，這是決定語言模型功能和效能的關鍵因素。\n",
    "temperature（溫度）: 控制輸出的隨機性，影響生成文本的創造性和多樣性。較低的溫度使得模型回應更為確定和保守，較高的溫度則鼓勵模型產生更多元和創新的內容。\n",
    "max_tokens（最大令牌數）: 定義生成回應的最大長度，以令牌（token）計算。這個參數能夠限制回應的詳細程度和範圍。\n",
    "'''\n",
    "# output = llm.invoke('Explain quantum mechanics in one sentence.', model='gpt-3.5-turbo', temperature=0.1)\n",
    "output = llm.invoke('Explain quantum mechanics in one sentence.', model='gpt-4-0613', temperature=0.1)\n",
    "print(output.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5b306-6905-4c58-82ef-17cc9be9777d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(ChatOpenAI)  # see the llm constructor arguments with its defaults\n",
    "\n",
    "# name: An optional string to name the instance. Useful for identifying or logging purposes.\n",
    "# cache: An optional boolean to enable or disable caching of responses. If enabled, repeated prompts might return cached responses for efficiency.\n",
    "# verbose: A boolean flag that, when set to True, enables more detailed logs or outputs for debugging or informational purposes.\n",
    "# callbacks: A list of callback handlers or a callback manager. These are used for event-driven programming, allowing you to execute custom code at certain points in the request/response lifecycle.\n",
    "# callback_manager: An optional instance of a BaseCallbackManager that manages the lifecycle and invocation of callback handlers.\n",
    "# tags: An optional list of strings that can be used for tagging or categorizing instances for organizational purposes.\n",
    "# metadata: An optional dictionary of arbitrary key-value pairs that you can attach to the instance for custom metadata.\n",
    "# client, async_client: Clients used for making synchronous or asynchronous HTTP requests to the OpenAI API.\n",
    "# model: The default model to use for generating responses, such as 'gpt-3.5-turbo'.(重要)\n",
    "# temperature: The default randomness parameter for the model's responses, affecting creativity and determinism. (重要)\n",
    "# model_kwargs: A dictionary of additional keyword arguments to pass to the model. These could include any of the model-specific parameters like max_tokens, frequency_penalty, etc.\n",
    "# api_key: An optional string for the OpenAI API key, required for authenticating requests.\n",
    "# base_url: The base URL for the OpenAI API, which could be customized for different environments or proxies.\n",
    "# organization: An optional string specifying the OpenAI organization under which the requests should be made.\n",
    "# openai_proxy: An optional proxy URL for the OpenAI API requests.\n",
    "# timeout: Configures the timeout for HTTP requests. This can be a single float or a tuple of floats specifying the connect and read timeouts.\n",
    "# max_retries: The number of times a failed request should be retried before giving up.(重要)\n",
    "# streaming: A boolean flag indicating whether responses should be streamed. Useful for large responses or real-time interaction.\n",
    "# n: The number of responses to generate for each prompt. Useful for generating multiple outputs for comparison.(重要)\n",
    "# max_tokens: The default maximum number of tokens to generate in the response. This limits the length of the output.(重要)\n",
    "# tiktoken_model_name: An optional parameter for specifying a model name when using TikToken for authentication or request routing.\n",
    "# default_headers, default_query: Optional mappings for specifying default HTTP headers and query parameters for requests.\n",
    "# http_client: An optional parameter to specify a custom HTTP client for making requests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da2baa8e-8440-4c16-a1bb-91f8ff0ac556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量子力学是研究物质和光在微观尺度上的行为的物理学分支。\n"
     ]
    }
   ],
   "source": [
    "# using Chat Completions API Messages: System, Assistant and Human\n",
    "from langchain.schema import(\n",
    "    SystemMessage, \n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content='You are a physicist and respond only in chinese.'),\n",
    "    HumanMessage(content='Explain quantum mechanics in one sentence.')\n",
    "]\n",
    "\n",
    "\n",
    "output = llm.invoke(messages, model='gpt-4-0613', temperature=0.6)\n",
    "print(output.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e837c-d307-4165-ad99-551a2bee6cf5",
   "metadata": {},
   "source": [
    "## Caching LLM Responses\n",
    "\n",
    "Caching is the practice of storing frequently accessed data or result in a temporary faster storage layer.<br>\n",
    "Caching optimize interactions with LLMs by reducing API calls and speeding up applications. resulting in a more efficient user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379171e-2081-4cc1-a834-89ebff34e84c",
   "metadata": {},
   "source": [
    "### 1. In-Memory Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a73a4e4-d351-4f4c-b149-151fe3b2c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20232db3-97a5-498e-805f-fbcb24d14584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 1.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n目前尚無確定的完結日期，原作漫畫作者富樫雅廣曾表示希望在2020年完成，但因為多次休刊和延期，目前仍無法確定。另外，動畫和電視劇版的製作也可能會影響故事進程和完結時間。'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "set_llm_cache(InMemoryCache())\n",
    "prompt = '獵人何時完結?'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18d58e8e-c8b3-4719-b4ad-a3d03bfb7655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 282 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n目前尚無確定的完結日期，原作漫畫作者富樫雅廣曾表示希望在2020年完成，但因為多次休刊和延期，目前仍無法確定。另外，動畫和電視劇版的製作也可能會影響故事進程和完結時間。'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "llm.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6269939d-b92f-406a-a53f-9a7bf561c5a2",
   "metadata": {},
   "source": [
    "### 2. SQLite Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9271fe21-1ca3-4cb2-8599-9a368071482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a13f4281-b6c7-4a46-ab08-b02528f4111e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 696 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy couldn't the bicycle stand up by itself? Because it was two-tired!\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# First request (not in cache, takes longer)\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3fe8a61-0812-427b-bdae-06adc67988f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 438 ms\n",
      "Wall time: 630 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy couldn't the bicycle stand up by itself? Because it was two-tired!\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Second request (cached, faster)\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56aa044-5842-4d50-b3e9-0207421df1cf",
   "metadata": {},
   "source": [
    "## LLM Streaming\n",
    "\n",
    "Streaming refer to the process of delivering the response in a continuous stream of data instead of sending the entire response at once <br>\n",
    "This allows the user to receive the response piece by place as it is generated, which can improve the user experience and reduce the overall latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "174adda0-ce4a-40bb-8a28-31abc494f266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "在這城市的熙來攘往，尋找一份獨特的滋味，\n",
      "超派雞排在手，咬一口，心情瞬間飛揚起來，\n",
      "那酥脆的外皮，嫩滑的雞肉，讓人忍不住再嚐一口，\n",
      "就像愛情的甜蜜，讓人沉醉，忘記了所有的煩惱。\n",
      "\n",
      "(Chorus)\n",
      "超派雞排，超派鐵拳，我們的愛情就像這樣，\n",
      "無論是甜蜜或苦澀，我們都會一起面對，\n",
      "超派雞排，超派鐵拳，我們的愛情就像這樣，\n",
      "無論風雨或晴天，我們都會一起走過。\n",
      "\n",
      "(Verse 2)\n",
      "在這城市的喧囂中，尋找一份屬於我們的寧靜，\n",
      "超派鐵拳在手，握一握，心情瞬間堅定起來，\n",
      "那堅韌的外殼，柔軟的內心，讓人忍不住再握一次，\n",
      "就像愛情的堅定，讓人安心，忘記了所有的困擾。\n",
      "\n",
      "(Chorus)\n",
      "超派雞排，超派鐵拳，我們的愛情就像這樣，\n",
      "無論是甜蜜或苦澀，我們都會一起面對，\n",
      "超派雞排，超派鐵拳，我們的愛情就像這樣，\n",
      "無論風雨或晴天，我們都會一起走過。\n",
      "\n",
      "(Outro)\n",
      "超派雞排，超派鐵拳，這就是我們的愛情，\n",
      "就像這城市的熙來攘往，有你在，一切都變得美好。\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "llm = ChatOpenAI(model='gpt-4-0613', temperature=0.4)\n",
    "prompt = '以 \"超派雞排\" 和 \"超派鐵拳\" 寫一首抒情歌'\n",
    "print(llm.invoke(prompt).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a94d50a4-60ca-4009-a051-b83cf70c7a45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一天，一隻兔子走進了一家理髮店，問理髮師：“你們有胡蘿蔔嗎？”理髮師回答：“這裡是理髮店，我們不賣胡蘿蔔。”第二天，兔子又來了，又問：“你們有胡蘿蔔嗎？”理髮師有點不耐煩：“我昨天就告訴你了，這裡是理髮店，我們不賣胡蘿蔔。”第三天，兔子還是來了，還是問：“你們有胡蘿蔔嗎？”理髮師生氣了：“我已經告訴你很多次了，這裡是理髮店，我們不賣胡蘿蔔！如果你再來問，我就用釘書機把你的耳朵釘在牆上！”第四天，兔子又來了，它問：“你們有釘書機嗎？”理髮師回答：“沒有。”兔子說：“那你們有胡蘿蔔嗎？”"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)\n",
    "\n",
    "# ps: 注意 InMemoryCache() 沒有起作用，下面為改寫範例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1ca41d",
   "metadata": {},
   "source": [
    "### 整合文本相似度 搭配 streaming + caching technique，減少 call API 的開銷 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4cc46c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "在繁華的街頭，我尋找那份滋味，\n",
      "超派雞排的香氣，引領我走向你。\n",
      "你的笑容如陽光，照亮我孤單的夜，\n",
      "我們的故事，就在這裡開始。\n",
      "\n",
      "(Chorus)\n",
      "超派雞排，你的味道我無法忘懷，\n",
      "就像你的愛，深深烙印在我心間。\n",
      "超派鐵拳，你的力量讓我驚嘆，\n",
      "你的堅定，就像你的鐵拳，永不退縮。\n",
      "\n",
      "(Verse 2)\n",
      "我們在夜晚的街頭，分享著那份美味，\n",
      "你的笑容在燈光下，如此動人。\n",
      "超派鐵拳的力量，就像你的堅定，\n",
      "讓我知道，有你在我身邊。\n",
      "\n",
      "(Chorus)\n",
      "超派雞排，你的味道我無法忘懷，\n",
      "就像你的愛，深深烙印在我心間。\n",
      "超派鐵拳，你的力量讓我驚嘆，\n",
      "你的堅定，就像你的鐵拳，永不退縮。\n",
      "\n",
      "(Bridge)\n",
      "我們的愛情，就像超派雞排的味道，\n",
      "深深吸引著我，讓我無法忘懷。\n",
      "你的堅定，就像超派鐵拳的力量，\n",
      "讓我知道，你會永遠守護我。\n",
      "\n",
      "(Chorus)\n",
      "超派雞排，你的味道我無法忘懷，\n",
      "就像你的愛，深深烙印在我心間。\n",
      "超派鐵拳，你的力量讓我驚嘆，\n",
      "你的堅定，就像你的鐵拳，永不退縮。\n",
      "\n",
      "(Outro)\n",
      "在這繁華的街頭，我們的愛情綻放，\n",
      "超派雞排的味道，超派鐵拳的力量，\n",
      "都是你，我愛你，超派的愛情。"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.cache import InMemoryCache\n",
    "import hashlib\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class CachedChatOpenAI:\n",
    "    def __init__(self, model, temperature=0.4, cache_file='chat_cache.json'):\n",
    "        self.llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = self.load_cache()\n",
    "        self.vectorizer = TfidfVectorizer()  # 初始化TF-IDF向量化器\n",
    "\n",
    "    def load_cache(self):\n",
    "        # 从文件加载缓存\n",
    "        if os.path.exists(self.cache_file):\n",
    "            with open(self.cache_file, 'r', encoding='utf-8') as file:\n",
    "                return json.load(file)\n",
    "        return {}\n",
    "\n",
    "    def save_cache(self):\n",
    "        # 将缓存保存到文件，确保中文被正确写入\n",
    "        with open(self.cache_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(self.cache, file, ensure_ascii=False)\n",
    "\n",
    "    def update_vectorizer(self):\n",
    "        # 用当前缓存的prompts更新TF-IDF向量化器\n",
    "        if self.cache:\n",
    "            self.vectorizer.fit(self.cache.keys())\n",
    "    \n",
    "    def fuzzy_match_keys(self, prompt):\n",
    "        # 用TF-IDF和余弦相似度实现模糊匹配\n",
    "        if not self.cache:  # 如果缓存为空，则直接返回空列表\n",
    "            return []\n",
    "        self.update_vectorizer()  # 更新TF-IDF向量化器\n",
    "        prompt_vec = self.vectorizer.transform([prompt])\n",
    "        cache_vecs = self.vectorizer.transform(self.cache.keys())\n",
    "        cosine_sim = cosine_similarity(prompt_vec, cache_vecs)\n",
    "        \n",
    "        if np.max(cosine_sim) > 0.6:  # 设置一个阈值，比如0.6\n",
    "            most_similar_index = np.argmax(cosine_sim)\n",
    "            return [list(self.cache.keys())[most_similar_index]]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def stream(self, prompt):\n",
    "        # 修改stream方法以保存更新后的缓存\n",
    "        matched_keys = self.fuzzy_match_keys(prompt)\n",
    "        if matched_keys:\n",
    "            random_key = random.choice(matched_keys)\n",
    "            cached_result = self.cache[random_key]\n",
    "            for chunk in cached_result:\n",
    "                print(chunk, end='', flush=True)\n",
    "        else:\n",
    "            result_chunks = []\n",
    "            for chunk in self.llm.stream(prompt):\n",
    "                print(chunk.content, end='', flush=True)\n",
    "                result_chunks.append(chunk.content)\n",
    "            self.cache[prompt] = result_chunks\n",
    "            self.save_cache()  # 更新缓存后保存\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    cached_llm = CachedChatOpenAI(model='gpt-4-0613', temperature=0.4)\n",
    "    prompt = '以 \"超派雞排\" 和 \"超派鐵拳\" 寫一首抒情歌'\n",
    "\n",
    "    cached_llm.stream(prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84844cfe-7469-47a6-841c-0fc31d8cdb49",
   "metadata": {},
   "source": [
    "## Templates\n",
    "\n",
    "A prompt refers to the input to the model<br>\n",
    "Prompt template are a way to create dynamic prompt for LLMs<br>\n",
    "A prompt template takes a piece of text and injects a user's input into that piece of text.<br>\n",
    "in langchain there are \"promptTemplate\" and \"chatPromptTemplates\"<br>\n",
    "\n",
    "## PromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34389e61-d08b-440a-8a6f-96e99b5bee8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n你現在是位社會學家，針對 兩性平權 的議題，\\n請用申論架構搭配白話的方式解釋給一般社會大眾，\\n並提供中文和 日文 的對照版本，文長限制 100 字。\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define a template for the prompt\n",
    "template = '''\n",
    "你現在是位社會學家，針對 {topic} 的議題，\n",
    "請用申論架構搭配白話的方式解釋給一般社會大眾，\n",
    "並提供中文和 {language} 的對照版本，文長限制 {token} 字。\n",
    "'''\n",
    "\n",
    "# Create a PromptTemplate object from the template\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# Fill in the variable: virus and language\n",
    "prompt = prompt_template.format(topic='兩性平權', language='日文', token=\"100\")\n",
    "prompt  # Returns the generated prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "602e2f25-4ba8-4a48-b116-4589af56f9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文版本：\n",
      "兩性平權是指男性和女性享有相同的權利和機會，無論在教育、工作還是家庭中。不同性別的人應該被平等對待，不應受到性別歧視的影響。我們應該推動性別平等，讓每個人都能擁有公平的待遇。\n",
      "\n",
      "日文版本：\n",
      "男女平等は、男性と女性が教育、仕事、家庭などで同等の権利と機会を持つことを意味します。性別に関わらず、すべての人が平等に扱われ、性別差別の影響を受けないようにすべきです。私たちは、すべての人が公平な待遇を受けることができるように、性別平等を推進すべきです。"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-4-0613', temperature=0.6)\n",
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8084261-b4c6-47e6-8a38-03f339f1f5c5",
   "metadata": {},
   "source": [
    "## ChatPromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fab3930-930d-4c45-a3d8-a6b213d77270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You respond only in the JSON format.'), HumanMessage(content='Top 5 countries in World by population.')]\n"
     ]
    }
   ],
   "source": [
    "# from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# Create a chat template with system and human messages\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond only in the JSON format.'), # 理解成系統 config \n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.') # 根據 use case 動態填入\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fill in the specific values for n and area\n",
    "messages = chat_template.format_messages(n='5', area='World')\n",
    "print(messages)  # Outputs the formatted chat messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a768271-ff8d-4d4c-a7c3-21ebed1abd6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"country\": \"China\",\n",
      "      \"population\": 1439323776\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"India\",\n",
      "      \"population\": 1380004385\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"United States\",\n",
      "      \"population\": 331002651\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Indonesia\",\n",
      "      \"population\": 273523615\n",
      "    },\n",
      "    {\n",
      "      \"country\": \"Pakistan\",\n",
      "      \"population\": 220892340\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name='gpt-4-0613', temperature=0.6)\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e355ee7",
   "metadata": {},
   "source": [
    "### summary\n",
    "\n",
    "\n",
    "with prompt template，there are three features include reusability, scalability, and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2eb160-df97-4229-8115-2e8864b400d3",
   "metadata": {},
   "source": [
    "## Simple Chains\n",
    "\n",
    "Chain are a series of steps and actions.\n",
    "\n",
    "Chains allow us to combine multiple components together to solve a specific task and build an entire LLM application.\n",
    "\n",
    "這種寫法比較好，較類似物件宣告的方式，可以將元件乾淨的切分開\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e2aef22-26c3-4e8c-8bbd-99383b6052e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "你現在是位社會學家，針對 少子化 的議題，\n",
      "請用申論架構搭配白話的方式解釋給一般社會大眾，文長限制 100 字。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4-0613', temperature=0.6)\n",
    "template = '''\n",
    "你現在是位社會學家，針對 {topic} 的議題，\n",
    "請用申論架構搭配白話的方式解釋給一般社會大眾，文長限制 {token} 字。'''\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "output = chain.invoke({'topic': '少子化', 'token': '100'})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "161f079d-a38d-48f5-8466-9b2c185446f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': '少子化', 'token': '100', 'text': '少子化是指生育率持續下降，新一代人口數量少於前一代，造成人口結構老化。社會學觀點，這是因為現代人對於生活品質追求提高，工作壓力加大，以及教育、養育成本增加等因素，導致人們選擇晚婚或不生子。這將帶來社會老年人口增加，勞動力短缺等問題。'}\n"
     ]
    }
   ],
   "source": [
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7402fb5",
   "metadata": {},
   "source": [
    "### 改寫動態輸入參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c23cc0a0-6b02-4007-9192-80f00d7576a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is the capital of Taiwan?. List the top 3 places to visit in that city. Use bullet points with chinese language\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The capital of Taiwan is Taipei. \n",
      "\n",
      "Top 3 places to visit in Taipei:\n",
      "\n",
      "- 國立故宮博物院 (National Palace Museum)\n",
      "- 台北101 (Taipei 101)\n",
      "- 西門町 (Ximending)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-4-0613', temperature=0.6)\n",
    "template = 'What is the capital of {country}?. List the top 3 places to visit in that city. Use bullet points with chinese language'\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# Initialize an LLMChain with the ChatOpenAI model and the prompt template\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "country = input('Enter Country: ')\n",
    "\n",
    "# Invoke the chain with specific virus and language values\n",
    "output = chain.invoke(country)\n",
    "print(output['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963536b8-76e7-4681-bb0d-8309795e2fa3",
   "metadata": {},
   "source": [
    "## Sequential Chains\n",
    "\n",
    "with sequential chains, you can make a series of calls to one or more LLMs\n",
    "you can take the output from one chain and use it as the input to another chain.\n",
    "\n",
    "there are two types of sequential chains:\n",
    "- SimpleSequentialChain\n",
    "- General from of sequential chains\n",
    "\n",
    "### SimpleSequentialChain: ex: Writing a Piece of Code\n",
    "\n",
    "1. **Step 1: Clarify Requirements**\n",
    "   - **Input:** A general idea of the project.\n",
    "   - **Output:** Detailed requirements for the code.\n",
    "\n",
    "2. **Step 2: Generate Pseudo-Code**\n",
    "   - **Input:** Detailed requirements (Output from Step 1).\n",
    "   - **Output:** Pseudo-code outlining the logic and steps needed to implement the requirements.\n",
    "\n",
    "3. **Step 3: Convert Pseudo-Code to Actual Code**\n",
    "   - **Input:** Pseudo-code (Output from Step 2).\n",
    "   - **Output:** Actual code implementing the logic described in the pseudo-code.\n",
    "\n",
    "\n",
    "### General Form of Sequential Chains: ex: Developing a Machine Learning Model\n",
    "\n",
    "1. **Step 1: Data Preprocessing**\n",
    "   - **Input:** Raw dataset.\n",
    "   - **Output:** Preprocessed dataset ready for model training.\n",
    "\n",
    "2. **Step 2: Model Training**\n",
    "   - **Input:** Preprocessed dataset (Output from Step 1).\n",
    "   - **Output:** Trained model.\n",
    "\n",
    "3. **Step 3: Model Evaluation**\n",
    "   - **Input:** Trained model (Output from Step 2).\n",
    "   - **Output:** Evaluation metrics to assess model performance.\n",
    "\n",
    "4. **Step 4 (Conditional): Iterative Improvement**\n",
    "   - **Condition:** If evaluation metrics are below the desired threshold.\n",
    "   - **Action:** Modify preprocessing or model parameters based on evaluation.\n",
    "   - **Loop Back:** Depending on the modification, return to Step 1 or Step 2 with updated parameters or data. Repeat the process until the evaluation metrics meet the desired criteria.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9e711bc-8bcf-45ce-b993-52e42fb650cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an experienced scientist and Python programmer. Write a function that implements the concept of linear regression.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mSure! Here's an example of a Python function that implements the concept of linear regression using the least squares method:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def linear_regression(x, y):\n",
      "    # Calculate the number of data points\n",
      "    n = len(x)\n",
      "    \n",
      "    # Calculate the mean of x and y\n",
      "    x_mean = np.mean(x)\n",
      "    y_mean = np.mean(y)\n",
      "    \n",
      "    # Calculate the sum of the products of x and y\n",
      "    xy_sum = np.sum(x * y)\n",
      "    \n",
      "    # Calculate the sum of the squares of x\n",
      "    x_squared_sum = np.sum(x**2)\n",
      "    \n",
      "    # Calculate the slope (m) and y-intercept (b) of the regression line\n",
      "    m = (n * xy_sum - np.sum(x) * np.sum(y)) / (n * x_squared_sum - np.sum(x)**2)\n",
      "    b = y_mean - m * x_mean\n",
      "    \n",
      "    return m, b\n",
      "```\n",
      "\n",
      "To use this function, you can pass in your x and y data as numpy arrays or lists. Here's an example usage:\n",
      "\n",
      "```python\n",
      "x = np.array([1, 2, 3, 4, 5])\n",
      "y = np.array([2, 4, 6, 8, 10])\n",
      "\n",
      "m, b = linear_regression(x, y)\n",
      "print(\"Slope (m):\", m)\n",
      "print(\"Y-intercept (b):\", b)\n",
      "```\n",
      "\n",
      "This will output:\n",
      "\n",
      "```\n",
      "Slope (m): 2.0\n",
      "Y-intercept (b): 0.0\n",
      "```\n",
      "\n",
      "The function returns the slope (m) and y-intercept (b) of the regression line, which can be used to predict y values for new x values using the equation `y = mx + b`.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the Python function Sure! Here's an example of a Python function that implements the concept of linear regression using the least squares method:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def linear_regression(x, y):\n",
      "    # Calculate the number of data points\n",
      "    n = len(x)\n",
      "    \n",
      "    # Calculate the mean of x and y\n",
      "    x_mean = np.mean(x)\n",
      "    y_mean = np.mean(y)\n",
      "    \n",
      "    # Calculate the sum of the products of x and y\n",
      "    xy_sum = np.sum(x * y)\n",
      "    \n",
      "    # Calculate the sum of the squares of x\n",
      "    x_squared_sum = np.sum(x**2)\n",
      "    \n",
      "    # Calculate the slope (m) and y-intercept (b) of the regression line\n",
      "    m = (n * xy_sum - np.sum(x) * np.sum(y)) / (n * x_squared_sum - np.sum(x)**2)\n",
      "    b = y_mean - m * x_mean\n",
      "    \n",
      "    return m, b\n",
      "```\n",
      "\n",
      "To use this function, you can pass in your x and y data as numpy arrays or lists. Here's an example usage:\n",
      "\n",
      "```python\n",
      "x = np.array([1, 2, 3, 4, 5])\n",
      "y = np.array([2, 4, 6, 8, 10])\n",
      "\n",
      "m, b = linear_regression(x, y)\n",
      "print(\"Slope (m):\", m)\n",
      "print(\"Y-intercept (b):\", b)\n",
      "```\n",
      "\n",
      "This will output:\n",
      "\n",
      "```\n",
      "Slope (m): 2.0\n",
      "Y-intercept (b): 0.0\n",
      "```\n",
      "\n",
      "The function returns the slope (m) and y-intercept (b) of the regression line, which can be used to predict y values for new x values using the equation `y = mx + b`., describe it as detailed as possible.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe function `linear_regression` is designed to perform a simple linear regression analysis using the least squares method, a foundational technique in statistics for estimating the best-fitting straight line through a set of points. This serves numerous applications, from predicting future values to understanding relationships between variables. The implementation largely relies on NumPy, a popular Python library for numerical computing. Here's a breakdown of how the function operates:\n",
      "\n",
      "### Parameters\n",
      "- **x**: An array or list containing the independent variable data points.\n",
      "- **y**: An array or list containing the dependent variable data points.\n",
      "\n",
      "Both `x` and `y` are expected to be of the same length because each `x[i]` corresponds to each `y[i]`, representing a point `(x[i], y[i])` in the XY-plane.\n",
      "\n",
      "### Process Details:\n",
      "\n",
      "1. **Calculate Data Length**\n",
      "   - The number of data points `n` is determined using `len(x)`, which must be equal to the length of `y`.\n",
      "\n",
      "2. **Calculate Means of x and y**\n",
      "   - The function computes means of the `x` values (`x_mean`) and `y` values (`y_mean`) using `np.mean()`, which are used as part of calculating the linear regression coefficients.\n",
      "\n",
      "3. **Calculate Sum of x*y and Sum of x Squared**\n",
      "   - `xy_sum` represents the sum of the product of each corresponding pair of `x` and `y`.\n",
      "   - `x_squared_sum` represents the sum of each `x` value squared. These are essential for the slope calculations.\n",
      "\n",
      "4. **Calculate the Slope (m)**\n",
      "   - The slope `m` represents the rate of change in `y` for a one-unit change in `x`. It's calculated based on the formula derived from the least squares method. The numerator involves the product of the number of points `n` and `xy_sum`, from which the product of the sums of `x` and `y` is subtracted. The denominator involves the product of `n` and `x_squared_sum`, from which the square of the sum of `x` is subtracted.\n",
      "\n",
      "5. **Calculate the Y-Intercept (b)**\n",
      "   - The y-intercept `b` indicates the point where the regression line crosses the Y-axis. It's calculated by subtracting the product of the slope `m` and `x_mean` from `y_mean`.\n",
      "\n",
      "### Return Values\n",
      "\n",
      "- **m (slope)**: Indicates how much `y` changes for a unit change in `x`.\n",
      "- **b (y-intercept)**: Represents the `y` value when `x` is 0, essentially where the line intersects the Y-axis.\n",
      "\n",
      "### Usage\n",
      "\n",
      "With numpy arrays or lists for `x` and `y`, the function is called to return the slope `m` and y-intercept `b` of the best-fitting line, according to least squares criteria. The example provided underlines how the function can be utilized, showing a direct proportionality between `x` and `y` (a slope of 2 and a y-intercept of 0), confirming that for this dataset, the relationship is `y = 2x + 0`.\n",
      "\n",
      "This `linear_regression` function is a compact yet powerful tool for linear analysis, providing essential insights into the relationship between two variables in a straightforward manner.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "# Initialize the first ChatOpenAI model (gpt-3.5-turbo) with specific temperature\n",
    "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.3)\n",
    "\n",
    "# Define the first prompt template\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template='You are an experienced scientist and Python programmer. Write a function that implements the concept of {concept}.'\n",
    ")\n",
    "# Create an LLMChain using the first model and the prompt template\n",
    "chain1 = LLMChain(llm=llm1, \n",
    "                  prompt=prompt_template1,\n",
    "                  verbose=True\n",
    "                  )\n",
    "\n",
    "# Initialize the second ChatOpenAI model (gpt-4-turbo) with specific temperature\n",
    "llm2 = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=1.2)\n",
    "\n",
    "# Define the second prompt template\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
    ")\n",
    "# Create another LLMChain using the second model and the prompt template\n",
    "chain2 = LLMChain(llm=llm2, \n",
    "                  prompt=prompt_template2,\n",
    "                  verbose=True\n",
    "                  )\n",
    "\n",
    "# Combine both chains into a SimpleSequentialChain\n",
    "'''\n",
    "prompt1 -> chat answser1 -> prompt2 -> chat answser2\n",
    "'''\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], \n",
    "                                      verbose=True)\n",
    "\n",
    "# Invoke the overall chain with the concept \"linear regression\"\n",
    "output = overall_chain.invoke('linear regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b365ca5e-84f6-484f-829b-1ba9ff9798e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function `linear_regression` is designed to perform a simple linear regression analysis using the least squares method, a foundational technique in statistics for estimating the best-fitting straight line through a set of points. This serves numerous applications, from predicting future values to understanding relationships between variables. The implementation largely relies on NumPy, a popular Python library for numerical computing. Here's a breakdown of how the function operates:\n",
      "\n",
      "### Parameters\n",
      "- **x**: An array or list containing the independent variable data points.\n",
      "- **y**: An array or list containing the dependent variable data points.\n",
      "\n",
      "Both `x` and `y` are expected to be of the same length because each `x[i]` corresponds to each `y[i]`, representing a point `(x[i], y[i])` in the XY-plane.\n",
      "\n",
      "### Process Details:\n",
      "\n",
      "1. **Calculate Data Length**\n",
      "   - The number of data points `n` is determined using `len(x)`, which must be equal to the length of `y`.\n",
      "\n",
      "2. **Calculate Means of x and y**\n",
      "   - The function computes means of the `x` values (`x_mean`) and `y` values (`y_mean`) using `np.mean()`, which are used as part of calculating the linear regression coefficients.\n",
      "\n",
      "3. **Calculate Sum of x*y and Sum of x Squared**\n",
      "   - `xy_sum` represents the sum of the product of each corresponding pair of `x` and `y`.\n",
      "   - `x_squared_sum` represents the sum of each `x` value squared. These are essential for the slope calculations.\n",
      "\n",
      "4. **Calculate the Slope (m)**\n",
      "   - The slope `m` represents the rate of change in `y` for a one-unit change in `x`. It's calculated based on the formula derived from the least squares method. The numerator involves the product of the number of points `n` and `xy_sum`, from which the product of the sums of `x` and `y` is subtracted. The denominator involves the product of `n` and `x_squared_sum`, from which the square of the sum of `x` is subtracted.\n",
      "\n",
      "5. **Calculate the Y-Intercept (b)**\n",
      "   - The y-intercept `b` indicates the point where the regression line crosses the Y-axis. It's calculated by subtracting the product of the slope `m` and `x_mean` from `y_mean`.\n",
      "\n",
      "### Return Values\n",
      "\n",
      "- **m (slope)**: Indicates how much `y` changes for a unit change in `x`.\n",
      "- **b (y-intercept)**: Represents the `y` value when `x` is 0, essentially where the line intersects the Y-axis.\n",
      "\n",
      "### Usage\n",
      "\n",
      "With numpy arrays or lists for `x` and `y`, the function is called to return the slope `m` and y-intercept `b` of the best-fitting line, according to least squares criteria. The example provided underlines how the function can be utilized, showing a direct proportionality between `x` and `y` (a slope of 2 and a y-intercept of 0), confirming that for this dataset, the relationship is `y = 2x + 0`.\n",
      "\n",
      "This `linear_regression` function is a compact yet powerful tool for linear analysis, providing essential insights into the relationship between two variables in a straightforward manner.\n"
     ]
    }
   ],
   "source": [
    "print(output['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
