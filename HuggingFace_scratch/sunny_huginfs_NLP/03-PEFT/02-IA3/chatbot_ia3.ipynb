{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IA3 实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 24409\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"c-s-ale/alpaca-gpt4-data-zh\", split=\"train[:50%]\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': ['保持健康的三个提示。', '三原色是什么？', '描述原子的结构。'],\n",
       " 'input': ['', '', ''],\n",
       " 'output': ['以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。',\n",
       "  '三原色通常指的是红色、绿色和蓝色（RGB）。它们是通过加色混合原理创建色彩的三种基础颜色。在以发光为基础的显示设备中（如电视、计算机显示器、智能手机和平板电脑显示屏）, 三原色可混合产生大量色彩。其中红色和绿色可以混合生成黄色，红色和蓝色可以混合生成品红色，蓝色和绿色可以混合生成青色。当红色、绿色和蓝色按相等比例混合时，可以产生白色或灰色。\\n\\n此外，在印刷和绘画中，三原色指的是以颜料为基础的红、黄和蓝颜色（RYB）。这三种颜色用以通过减色混合原理来创建色彩。不过，三原色的具体定义并不唯一，不同的颜色系统可能会采用不同的三原色。',\n",
       "  '原子是物质的基本单位，它由三种基本粒子组成：质子、中子和电子。质子和中子形成原子核，位于原子中心，核外的电子围绕着原子核运动。\\n\\n原子结构具有层次性。原子核中，质子带正电，中子不带电（中性）。原子核非常小且致密，占据了原子总质量的绝大部分。电子带负电，通常围绕核运动，形成若干层次，称为壳层或电子层。电子数量与质子数量相等，使原子呈电中性。\\n\\n电子在每个壳层中都呈规律分布，并且不同壳层所能容纳的电子数也不同。在最里面的壳层一般只能容纳2个电子，其次一层最多可容纳8个电子，再往外的壳层可容纳的电子数逐层递增。\\n\\n原子核主要受到两种相互作用力的影响：强力和电磁力。强力的作用范围非常小，主要限制在原子核内，具有极强的吸引作用，使核子（质子和中子）紧密结合在一起。电磁力的作用范围较大，主要通过核外的电子与原子核相互作用，发挥作用。\\n\\n这就是原子的基本结构。原子内部结构复杂多样，不同元素的原子核中质子、中子数量不同，核外电子排布分布也不同，形成了丰富多彩的化学世界。']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomTokenizerFast(name_or_path='Langboat/bloom-1b4-zh', vocab_size=46145, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Langboat/bloom-1b4-zh\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 256\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\"\\n\".join([\"Human: \" + example[\"instruction\"], example[\"input\"]]).strip() + \"\\n\\nAssistant: \")\n",
    "    response = tokenizer(example[\"output\"] + tokenizer.eos_token)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 24409\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: 三原色是什么？\\n\\nAssistant: 三原色通常指的是红色、绿色和蓝色（RGB）。它们是通过加色混合原理创建色彩的三种基础颜色。在以发光为基础的显示设备中（如电视、计算机显示器、智能手机和平板电脑显示屏）, 三原色可混合产生大量色彩。其中红色和绿色可以混合生成黄色，红色和蓝色可以混合生成品红色，蓝色和绿色可以混合生成青色。当红色、绿色和蓝色按相等比例混合时，可以产生白色或灰色。\\n\\n此外，在印刷和绘画中，三原色指的是以颜料为基础的红、黄和蓝颜色（RYB）。这三种颜色用以通过减色混合原理来创建色彩。不过，三原色的具体定义并不唯一，不同的颜色系统可能会采用不同的三原色。</s>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_ds[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'三原色通常指的是红色、绿色和蓝色（RGB）。它们是通过加色混合原理创建色彩的三种基础颜色。在以发光为基础的显示设备中（如电视、计算机显示器、智能手机和平板电脑显示屏）, 三原色可混合产生大量色彩。其中红色和绿色可以混合生成黄色，红色和蓝色可以混合生成品红色，蓝色和绿色可以混合生成青色。当红色、绿色和蓝色按相等比例混合时，可以产生白色或灰色。\\n\\n此外，在印刷和绘画中，三原色指的是以颜料为基础的红、黄和蓝颜色（RYB）。这三种颜色用以通过减色混合原理来创建色彩。不过，三原色的具体定义并不唯一，不同的颜色系统可能会采用不同的三原色。</s>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_ds[1][\"labels\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Langboat/bloom-1b4-zh\", low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IA3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT Step1 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IA3Config(peft_type=<PeftType.IA3: 'IA3'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, target_modules=None, feedforward_modules=None, fan_in_fan_out=False, modules_to_save=None, init_ia3_weights=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import IA3Config, TaskType, get_peft_model\n",
    "\n",
    "config = IA3Config(task_type=TaskType.CAUSAL_LM)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT Step2 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IA3Config(peft_type=<PeftType.IA3: 'IA3'>, auto_mapping=None, base_model_name_or_path='Langboat/bloom-1b4-zh', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, target_modules=['query_key_value', 'mlp.dense_4h_to_h'], feedforward_modules=['mlp.dense_4h_to_h'], fan_in_fan_out=False, modules_to_save=None, init_ia3_weights=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): IA3Model(\n",
       "    (model): BloomForCausalLM(\n",
       "      (transformer): BloomModel(\n",
       "        (word_embeddings): Embedding(46145, 2048)\n",
       "        (word_embeddings_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x BloomBlock(\n",
       "            (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (self_attention): BloomAttention(\n",
       "              (query_key_value): Linear(\n",
       "                in_features=2048, out_features=6144, bias=True\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 6144x1])\n",
       "              )\n",
       "              (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): BloomMLP(\n",
       "              (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "              (gelu_impl): BloomGelu()\n",
       "              (dense_4h_to_h): Linear(\n",
       "                in_features=8192, out_features=2048, bias=True\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 1x8192])\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=46145, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 344,064 || all params: 1,303,455,744 || trainable%: 0.026396293206254036\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./chatbot\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=3e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_ds, \n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdd81176c6e41129555e28730514506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5497, 'grad_norm': 0.22104710340499878, 'learning_rate': 0.0029901671583087513, 'epoch': 0.0}\n",
      "{'loss': 2.4903, 'grad_norm': 0.13658294081687927, 'learning_rate': 0.0029803343166175025, 'epoch': 0.01}\n",
      "{'loss': 2.3971, 'grad_norm': 0.13377226889133453, 'learning_rate': 0.0029705014749262537, 'epoch': 0.01}\n",
      "{'loss': 2.307, 'grad_norm': 0.16269288957118988, 'learning_rate': 0.002960668633235005, 'epoch': 0.01}\n",
      "{'loss': 2.3881, 'grad_norm': 0.163996160030365, 'learning_rate': 0.002950835791543756, 'epoch': 0.02}\n",
      "{'loss': 2.4343, 'grad_norm': 0.10901594161987305, 'learning_rate': 0.0029410029498525077, 'epoch': 0.02}\n",
      "{'loss': 2.2091, 'grad_norm': 0.1687869429588318, 'learning_rate': 0.002931170108161259, 'epoch': 0.02}\n",
      "{'loss': 2.2699, 'grad_norm': 0.3412438631057739, 'learning_rate': 0.00292133726647001, 'epoch': 0.03}\n",
      "{'loss': 2.4191, 'grad_norm': 0.2162906974554062, 'learning_rate': 0.0029115044247787613, 'epoch': 0.03}\n",
      "{'loss': 2.25, 'grad_norm': 0.10036714375019073, 'learning_rate': 0.0029016715830875125, 'epoch': 0.03}\n",
      "{'loss': 2.2896, 'grad_norm': 0.10813340544700623, 'learning_rate': 0.0028918387413962637, 'epoch': 0.04}\n",
      "{'loss': 2.2552, 'grad_norm': 0.10734071582555771, 'learning_rate': 0.002882005899705015, 'epoch': 0.04}\n",
      "{'loss': 2.2269, 'grad_norm': 0.1997501403093338, 'learning_rate': 0.002872173058013766, 'epoch': 0.04}\n",
      "{'loss': 2.2567, 'grad_norm': 0.13727781176567078, 'learning_rate': 0.0028623402163225173, 'epoch': 0.05}\n",
      "{'loss': 2.2372, 'grad_norm': 0.15644967555999756, 'learning_rate': 0.0028525073746312685, 'epoch': 0.05}\n",
      "{'loss': 2.4539, 'grad_norm': 0.22974656522274017, 'learning_rate': 0.0028426745329400197, 'epoch': 0.05}\n",
      "{'loss': 2.1269, 'grad_norm': 0.07594510167837143, 'learning_rate': 0.002832841691248771, 'epoch': 0.06}\n",
      "{'loss': 2.2227, 'grad_norm': 0.09972430765628815, 'learning_rate': 0.002823008849557522, 'epoch': 0.06}\n",
      "{'loss': 2.145, 'grad_norm': 0.12039814889431, 'learning_rate': 0.0028131760078662733, 'epoch': 0.06}\n",
      "{'loss': 2.2647, 'grad_norm': 0.20263344049453735, 'learning_rate': 0.0028033431661750245, 'epoch': 0.07}\n",
      "{'loss': 2.2008, 'grad_norm': 0.18108013272285461, 'learning_rate': 0.0027935103244837757, 'epoch': 0.07}\n",
      "{'loss': 2.1842, 'grad_norm': 0.0995166003704071, 'learning_rate': 0.002783677482792527, 'epoch': 0.07}\n",
      "{'loss': 2.1578, 'grad_norm': 0.22537904977798462, 'learning_rate': 0.002773844641101278, 'epoch': 0.08}\n",
      "{'loss': 2.2053, 'grad_norm': 0.1390531212091446, 'learning_rate': 0.0027640117994100293, 'epoch': 0.08}\n",
      "{'loss': 2.2272, 'grad_norm': 0.11434859782457352, 'learning_rate': 0.0027541789577187805, 'epoch': 0.08}\n",
      "{'loss': 2.2978, 'grad_norm': 0.15523672103881836, 'learning_rate': 0.002744346116027532, 'epoch': 0.09}\n",
      "{'loss': 2.2781, 'grad_norm': 0.1756824553012848, 'learning_rate': 0.0027345132743362833, 'epoch': 0.09}\n",
      "{'loss': 2.2439, 'grad_norm': 0.09947682172060013, 'learning_rate': 0.0027246804326450345, 'epoch': 0.09}\n",
      "{'loss': 2.2677, 'grad_norm': 0.13554571568965912, 'learning_rate': 0.0027148475909537857, 'epoch': 0.1}\n",
      "{'loss': 2.2701, 'grad_norm': 0.12584461271762848, 'learning_rate': 0.002705014749262537, 'epoch': 0.1}\n",
      "{'loss': 2.2206, 'grad_norm': 0.09757895022630692, 'learning_rate': 0.002695181907571288, 'epoch': 0.1}\n",
      "{'loss': 2.2515, 'grad_norm': 0.10527478903532028, 'learning_rate': 0.0026853490658800393, 'epoch': 0.1}\n",
      "{'loss': 2.1201, 'grad_norm': 0.15738508105278015, 'learning_rate': 0.0026755162241887905, 'epoch': 0.11}\n",
      "{'loss': 2.224, 'grad_norm': 0.10867458581924438, 'learning_rate': 0.0026656833824975417, 'epoch': 0.11}\n",
      "{'loss': 2.1332, 'grad_norm': 0.1902724802494049, 'learning_rate': 0.002655850540806293, 'epoch': 0.11}\n",
      "{'loss': 2.0867, 'grad_norm': 0.12485598027706146, 'learning_rate': 0.002646017699115044, 'epoch': 0.12}\n",
      "{'loss': 2.2761, 'grad_norm': 0.1030341163277626, 'learning_rate': 0.0026361848574237953, 'epoch': 0.12}\n",
      "{'loss': 2.0495, 'grad_norm': 0.1415359526872635, 'learning_rate': 0.002626352015732547, 'epoch': 0.12}\n",
      "{'loss': 2.1414, 'grad_norm': 0.10949329286813736, 'learning_rate': 0.002616519174041298, 'epoch': 0.13}\n",
      "{'loss': 2.0968, 'grad_norm': 0.1264771968126297, 'learning_rate': 0.0026066863323500494, 'epoch': 0.13}\n",
      "{'loss': 2.1539, 'grad_norm': 0.17637717723846436, 'learning_rate': 0.0025968534906588006, 'epoch': 0.13}\n",
      "{'loss': 2.2284, 'grad_norm': 0.23555594682693481, 'learning_rate': 0.0025870206489675518, 'epoch': 0.14}\n",
      "{'loss': 2.1711, 'grad_norm': 0.18974214792251587, 'learning_rate': 0.002577187807276303, 'epoch': 0.14}\n",
      "{'loss': 2.2127, 'grad_norm': 0.18075044453144073, 'learning_rate': 0.002567354965585054, 'epoch': 0.14}\n",
      "{'loss': 2.1401, 'grad_norm': 0.16550198197364807, 'learning_rate': 0.0025575221238938054, 'epoch': 0.15}\n",
      "{'loss': 2.2272, 'grad_norm': 0.12409904599189758, 'learning_rate': 0.0025476892822025566, 'epoch': 0.15}\n",
      "{'loss': 2.1049, 'grad_norm': 0.11477286368608475, 'learning_rate': 0.0025378564405113078, 'epoch': 0.15}\n",
      "{'loss': 2.1529, 'grad_norm': 0.19799970090389252, 'learning_rate': 0.002528023598820059, 'epoch': 0.16}\n",
      "{'loss': 2.1694, 'grad_norm': 0.09587467461824417, 'learning_rate': 0.0025181907571288106, 'epoch': 0.16}\n",
      "{'loss': 2.0522, 'grad_norm': 0.1315665990114212, 'learning_rate': 0.002508357915437562, 'epoch': 0.16}\n",
      "{'loss': 2.1998, 'grad_norm': 0.15052852034568787, 'learning_rate': 0.002498525073746313, 'epoch': 0.17}\n",
      "{'loss': 2.1605, 'grad_norm': 0.11625215411186218, 'learning_rate': 0.002488692232055064, 'epoch': 0.17}\n",
      "{'loss': 2.1792, 'grad_norm': 0.10095636546611786, 'learning_rate': 0.0024788593903638154, 'epoch': 0.17}\n",
      "{'loss': 2.1742, 'grad_norm': 0.12105818092823029, 'learning_rate': 0.002469026548672566, 'epoch': 0.18}\n",
      "{'loss': 2.0023, 'grad_norm': 0.1295364797115326, 'learning_rate': 0.0024591937069813174, 'epoch': 0.18}\n",
      "{'loss': 2.2735, 'grad_norm': 0.1918369084596634, 'learning_rate': 0.0024493608652900686, 'epoch': 0.18}\n",
      "{'loss': 2.1906, 'grad_norm': 0.144258514046669, 'learning_rate': 0.0024395280235988198, 'epoch': 0.19}\n",
      "{'loss': 2.0627, 'grad_norm': 0.21212798357009888, 'learning_rate': 0.0024296951819075714, 'epoch': 0.19}\n",
      "{'loss': 2.1, 'grad_norm': 0.13467244803905487, 'learning_rate': 0.0024198623402163226, 'epoch': 0.19}\n",
      "{'loss': 2.2115, 'grad_norm': 0.18792107701301575, 'learning_rate': 0.002410029498525074, 'epoch': 0.2}\n",
      "{'loss': 2.1967, 'grad_norm': 0.1934157907962799, 'learning_rate': 0.002400196656833825, 'epoch': 0.2}\n",
      "{'loss': 2.0293, 'grad_norm': 0.11688755452632904, 'learning_rate': 0.0023903638151425762, 'epoch': 0.2}\n",
      "{'loss': 2.1604, 'grad_norm': 0.11254589259624481, 'learning_rate': 0.0023805309734513274, 'epoch': 0.21}\n",
      "{'loss': 2.0826, 'grad_norm': 0.10186208784580231, 'learning_rate': 0.0023706981317600786, 'epoch': 0.21}\n",
      "{'loss': 2.2337, 'grad_norm': 0.12923243641853333, 'learning_rate': 0.00236086529006883, 'epoch': 0.21}\n",
      "{'loss': 2.2103, 'grad_norm': 0.16605931520462036, 'learning_rate': 0.002351032448377581, 'epoch': 0.22}\n",
      "{'loss': 2.2629, 'grad_norm': 0.1465129405260086, 'learning_rate': 0.0023411996066863322, 'epoch': 0.22}\n",
      "{'loss': 2.178, 'grad_norm': 0.08660602569580078, 'learning_rate': 0.0023313667649950834, 'epoch': 0.22}\n",
      "{'loss': 1.9792, 'grad_norm': 0.16614283621311188, 'learning_rate': 0.002321533923303835, 'epoch': 0.23}\n",
      "{'loss': 2.2078, 'grad_norm': 0.12266521900892258, 'learning_rate': 0.0023117010816125863, 'epoch': 0.23}\n",
      "{'loss': 2.1927, 'grad_norm': 0.16667555272579193, 'learning_rate': 0.0023018682399213375, 'epoch': 0.23}\n",
      "{'loss': 2.0999, 'grad_norm': 0.14265021681785583, 'learning_rate': 0.0022920353982300887, 'epoch': 0.24}\n",
      "{'loss': 2.1771, 'grad_norm': 0.11062934249639511, 'learning_rate': 0.00228220255653884, 'epoch': 0.24}\n",
      "{'loss': 2.1095, 'grad_norm': 0.125440776348114, 'learning_rate': 0.002272369714847591, 'epoch': 0.24}\n",
      "{'loss': 2.088, 'grad_norm': 0.09141828864812851, 'learning_rate': 0.0022625368731563423, 'epoch': 0.25}\n",
      "{'loss': 2.1259, 'grad_norm': 0.23230768740177155, 'learning_rate': 0.0022527040314650935, 'epoch': 0.25}\n",
      "{'loss': 2.0878, 'grad_norm': 0.09110409021377563, 'learning_rate': 0.0022428711897738447, 'epoch': 0.25}\n",
      "{'loss': 2.0334, 'grad_norm': 0.17158007621765137, 'learning_rate': 0.002233038348082596, 'epoch': 0.26}\n",
      "{'loss': 2.1249, 'grad_norm': 0.15383118391036987, 'learning_rate': 0.002223205506391347, 'epoch': 0.26}\n",
      "{'loss': 2.1306, 'grad_norm': 0.2020001858472824, 'learning_rate': 0.0022133726647000983, 'epoch': 0.26}\n",
      "{'loss': 2.0457, 'grad_norm': 0.09696806967258453, 'learning_rate': 0.00220353982300885, 'epoch': 0.27}\n",
      "{'loss': 2.1107, 'grad_norm': 0.11699971556663513, 'learning_rate': 0.002193706981317601, 'epoch': 0.27}\n",
      "{'loss': 2.1968, 'grad_norm': 0.16869375109672546, 'learning_rate': 0.0021838741396263523, 'epoch': 0.27}\n",
      "{'loss': 2.1091, 'grad_norm': 0.13313494622707367, 'learning_rate': 0.0021740412979351035, 'epoch': 0.28}\n",
      "{'loss': 2.1928, 'grad_norm': 0.08874205499887466, 'learning_rate': 0.0021642084562438547, 'epoch': 0.28}\n",
      "{'loss': 2.1145, 'grad_norm': 0.14405813813209534, 'learning_rate': 0.002154375614552606, 'epoch': 0.28}\n",
      "{'loss': 2.1528, 'grad_norm': 0.27342140674591064, 'learning_rate': 0.002144542772861357, 'epoch': 0.29}\n",
      "{'loss': 2.0441, 'grad_norm': 0.29378336668014526, 'learning_rate': 0.0021347099311701083, 'epoch': 0.29}\n",
      "{'loss': 2.1606, 'grad_norm': 0.12858420610427856, 'learning_rate': 0.002124877089478859, 'epoch': 0.29}\n",
      "{'loss': 2.0734, 'grad_norm': 0.07335921376943588, 'learning_rate': 0.0021150442477876107, 'epoch': 0.29}\n",
      "{'loss': 2.1828, 'grad_norm': 0.1246367022395134, 'learning_rate': 0.002105211406096362, 'epoch': 0.3}\n",
      "{'loss': 2.0712, 'grad_norm': 0.12150128930807114, 'learning_rate': 0.002095378564405113, 'epoch': 0.3}\n",
      "{'loss': 2.0512, 'grad_norm': 0.08950890600681305, 'learning_rate': 0.0020855457227138643, 'epoch': 0.3}\n",
      "{'loss': 2.0749, 'grad_norm': 0.2568162977695465, 'learning_rate': 0.0020757128810226155, 'epoch': 0.31}\n",
      "{'loss': 2.137, 'grad_norm': 0.2828044891357422, 'learning_rate': 0.0020658800393313667, 'epoch': 0.31}\n",
      "{'loss': 2.0888, 'grad_norm': 0.30925774574279785, 'learning_rate': 0.002056047197640118, 'epoch': 0.31}\n",
      "{'loss': 2.0103, 'grad_norm': 0.1254754513502121, 'learning_rate': 0.002046214355948869, 'epoch': 0.32}\n",
      "{'loss': 2.2088, 'grad_norm': 0.18250304460525513, 'learning_rate': 0.0020363815142576203, 'epoch': 0.32}\n",
      "{'loss': 2.1539, 'grad_norm': 0.10398618131875992, 'learning_rate': 0.0020265486725663715, 'epoch': 0.32}\n",
      "{'loss': 2.2047, 'grad_norm': 0.10560086369514465, 'learning_rate': 0.0020167158308751227, 'epoch': 0.33}\n",
      "{'loss': 2.1082, 'grad_norm': 0.16868185997009277, 'learning_rate': 0.0020068829891838743, 'epoch': 0.33}\n",
      "{'loss': 2.1275, 'grad_norm': 0.2776874303817749, 'learning_rate': 0.0019970501474926255, 'epoch': 0.33}\n",
      "{'loss': 2.0807, 'grad_norm': 0.14339731633663177, 'learning_rate': 0.0019872173058013767, 'epoch': 0.34}\n",
      "{'loss': 2.0586, 'grad_norm': 0.07907287776470184, 'learning_rate': 0.001977384464110128, 'epoch': 0.34}\n",
      "{'loss': 2.0364, 'grad_norm': 0.15080368518829346, 'learning_rate': 0.001967551622418879, 'epoch': 0.34}\n",
      "{'loss': 2.2032, 'grad_norm': 0.2762439548969269, 'learning_rate': 0.0019577187807276303, 'epoch': 0.35}\n",
      "{'loss': 2.2774, 'grad_norm': 0.11891813576221466, 'learning_rate': 0.0019478859390363815, 'epoch': 0.35}\n",
      "{'loss': 2.1053, 'grad_norm': 0.1686933934688568, 'learning_rate': 0.0019380530973451327, 'epoch': 0.35}\n",
      "{'loss': 2.1502, 'grad_norm': 0.08337170630693436, 'learning_rate': 0.001928220255653884, 'epoch': 0.36}\n",
      "{'loss': 2.0968, 'grad_norm': 0.0976562425494194, 'learning_rate': 0.0019183874139626354, 'epoch': 0.36}\n",
      "{'loss': 2.0205, 'grad_norm': 0.21999990940093994, 'learning_rate': 0.0019085545722713866, 'epoch': 0.36}\n",
      "{'loss': 2.1229, 'grad_norm': 0.19673436880111694, 'learning_rate': 0.0018987217305801378, 'epoch': 0.37}\n",
      "{'loss': 2.2064, 'grad_norm': 0.19080597162246704, 'learning_rate': 0.001888888888888889, 'epoch': 0.37}\n",
      "{'loss': 2.105, 'grad_norm': 0.17862790822982788, 'learning_rate': 0.0018790560471976402, 'epoch': 0.37}\n",
      "{'loss': 2.0866, 'grad_norm': 0.1560545414686203, 'learning_rate': 0.0018692232055063916, 'epoch': 0.38}\n",
      "{'loss': 2.156, 'grad_norm': 0.15712116658687592, 'learning_rate': 0.0018593903638151428, 'epoch': 0.38}\n",
      "{'loss': 2.1611, 'grad_norm': 0.08272405713796616, 'learning_rate': 0.001849557522123894, 'epoch': 0.38}\n",
      "{'loss': 2.0793, 'grad_norm': 0.12044923007488251, 'learning_rate': 0.0018397246804326452, 'epoch': 0.39}\n",
      "{'loss': 2.1763, 'grad_norm': 0.19319522380828857, 'learning_rate': 0.0018298918387413964, 'epoch': 0.39}\n",
      "{'loss': 2.1769, 'grad_norm': 0.07748595625162125, 'learning_rate': 0.0018200589970501476, 'epoch': 0.39}\n",
      "{'loss': 2.2383, 'grad_norm': 0.16830502450466156, 'learning_rate': 0.001810226155358899, 'epoch': 0.4}\n",
      "{'loss': 2.1961, 'grad_norm': 0.1980762779712677, 'learning_rate': 0.0018003933136676502, 'epoch': 0.4}\n",
      "{'loss': 2.115, 'grad_norm': 0.1401393711566925, 'learning_rate': 0.0017905604719764014, 'epoch': 0.4}\n",
      "{'loss': 2.1602, 'grad_norm': 0.15816548466682434, 'learning_rate': 0.0017807276302851526, 'epoch': 0.41}\n",
      "{'loss': 2.0446, 'grad_norm': 0.11857808381319046, 'learning_rate': 0.0017708947885939036, 'epoch': 0.41}\n",
      "{'loss': 2.1028, 'grad_norm': 0.30306702852249146, 'learning_rate': 0.0017610619469026548, 'epoch': 0.41}\n",
      "{'loss': 2.1048, 'grad_norm': 0.16707967221736908, 'learning_rate': 0.001751229105211406, 'epoch': 0.42}\n",
      "{'loss': 2.0488, 'grad_norm': 0.16101182997226715, 'learning_rate': 0.0017413962635201572, 'epoch': 0.42}\n",
      "{'loss': 2.0851, 'grad_norm': 0.17474228143692017, 'learning_rate': 0.0017315634218289084, 'epoch': 0.42}\n",
      "{'loss': 2.0706, 'grad_norm': 0.16888682544231415, 'learning_rate': 0.0017217305801376598, 'epoch': 0.43}\n",
      "{'loss': 2.1542, 'grad_norm': 0.09723833203315735, 'learning_rate': 0.001711897738446411, 'epoch': 0.43}\n",
      "{'loss': 2.1195, 'grad_norm': 0.19785712659358978, 'learning_rate': 0.0017020648967551622, 'epoch': 0.43}\n",
      "{'loss': 2.1375, 'grad_norm': 0.13097916543483734, 'learning_rate': 0.0016922320550639134, 'epoch': 0.44}\n",
      "{'loss': 2.0355, 'grad_norm': 0.26173463463783264, 'learning_rate': 0.0016823992133726646, 'epoch': 0.44}\n",
      "{'loss': 2.1899, 'grad_norm': 0.09574948996305466, 'learning_rate': 0.0016725663716814158, 'epoch': 0.44}\n",
      "{'loss': 2.1621, 'grad_norm': 0.08635043352842331, 'learning_rate': 0.0016627335299901672, 'epoch': 0.45}\n",
      "{'loss': 2.1132, 'grad_norm': 0.2046229988336563, 'learning_rate': 0.0016529006882989184, 'epoch': 0.45}\n",
      "{'loss': 2.1058, 'grad_norm': 0.1147518903017044, 'learning_rate': 0.0016430678466076696, 'epoch': 0.45}\n",
      "{'loss': 2.086, 'grad_norm': 0.15151435136795044, 'learning_rate': 0.0016332350049164208, 'epoch': 0.46}\n",
      "{'loss': 2.2051, 'grad_norm': 0.11601775139570236, 'learning_rate': 0.001623402163225172, 'epoch': 0.46}\n",
      "{'loss': 2.0687, 'grad_norm': 0.13768872618675232, 'learning_rate': 0.0016135693215339234, 'epoch': 0.46}\n",
      "{'loss': 2.1506, 'grad_norm': 0.14781887829303741, 'learning_rate': 0.0016037364798426746, 'epoch': 0.47}\n",
      "{'loss': 2.0734, 'grad_norm': 0.110589899122715, 'learning_rate': 0.0015939036381514258, 'epoch': 0.47}\n",
      "{'loss': 2.2276, 'grad_norm': 0.2569752037525177, 'learning_rate': 0.001584070796460177, 'epoch': 0.47}\n",
      "{'loss': 2.0504, 'grad_norm': 0.24612723290920258, 'learning_rate': 0.0015742379547689282, 'epoch': 0.48}\n",
      "{'loss': 2.1146, 'grad_norm': 0.0972072035074234, 'learning_rate': 0.0015644051130776794, 'epoch': 0.48}\n",
      "{'loss': 2.0959, 'grad_norm': 0.1303960233926773, 'learning_rate': 0.0015545722713864308, 'epoch': 0.48}\n",
      "{'loss': 2.1854, 'grad_norm': 0.15245534479618073, 'learning_rate': 0.001544739429695182, 'epoch': 0.49}\n",
      "{'loss': 2.094, 'grad_norm': 0.2360520213842392, 'learning_rate': 0.0015349065880039332, 'epoch': 0.49}\n",
      "{'loss': 2.0418, 'grad_norm': 0.08870510756969452, 'learning_rate': 0.0015250737463126844, 'epoch': 0.49}\n",
      "{'loss': 2.1255, 'grad_norm': 0.11096459627151489, 'learning_rate': 0.0015152409046214356, 'epoch': 0.49}\n",
      "{'loss': 2.0732, 'grad_norm': 0.2222297340631485, 'learning_rate': 0.0015054080629301868, 'epoch': 0.5}\n",
      "{'loss': 2.0283, 'grad_norm': 0.13570573925971985, 'learning_rate': 0.001495575221238938, 'epoch': 0.5}\n",
      "{'loss': 2.0529, 'grad_norm': 0.16064254939556122, 'learning_rate': 0.0014857423795476893, 'epoch': 0.5}\n",
      "{'loss': 2.1614, 'grad_norm': 0.20923671126365662, 'learning_rate': 0.0014759095378564405, 'epoch': 0.51}\n",
      "{'loss': 2.0067, 'grad_norm': 0.10652212798595428, 'learning_rate': 0.0014660766961651917, 'epoch': 0.51}\n",
      "{'loss': 2.0357, 'grad_norm': 0.08302981406450272, 'learning_rate': 0.001456243854473943, 'epoch': 0.51}\n",
      "{'loss': 2.2443, 'grad_norm': 0.11156615614891052, 'learning_rate': 0.0014464110127826943, 'epoch': 0.52}\n",
      "{'loss': 2.141, 'grad_norm': 0.20743408799171448, 'learning_rate': 0.0014365781710914455, 'epoch': 0.52}\n",
      "{'loss': 2.2039, 'grad_norm': 0.13746041059494019, 'learning_rate': 0.0014267453294001967, 'epoch': 0.52}\n",
      "{'loss': 2.0932, 'grad_norm': 0.10774939507246017, 'learning_rate': 0.0014169124877089479, 'epoch': 0.53}\n",
      "{'loss': 2.0448, 'grad_norm': 0.10105330497026443, 'learning_rate': 0.001407079646017699, 'epoch': 0.53}\n",
      "{'loss': 2.0975, 'grad_norm': 0.16067257523536682, 'learning_rate': 0.0013972468043264505, 'epoch': 0.53}\n",
      "{'loss': 2.0956, 'grad_norm': 0.20871932804584503, 'learning_rate': 0.0013874139626352017, 'epoch': 0.54}\n",
      "{'loss': 2.1004, 'grad_norm': 0.09671492129564285, 'learning_rate': 0.0013775811209439529, 'epoch': 0.54}\n",
      "{'loss': 1.9397, 'grad_norm': 0.17990311980247498, 'learning_rate': 0.001367748279252704, 'epoch': 0.54}\n",
      "{'loss': 2.0015, 'grad_norm': 0.10007728636264801, 'learning_rate': 0.0013579154375614553, 'epoch': 0.55}\n",
      "{'loss': 2.1081, 'grad_norm': 0.10412196815013885, 'learning_rate': 0.0013480825958702065, 'epoch': 0.55}\n",
      "{'loss': 2.0819, 'grad_norm': 0.13089978694915771, 'learning_rate': 0.0013382497541789577, 'epoch': 0.55}\n",
      "{'loss': 2.0613, 'grad_norm': 0.15821976959705353, 'learning_rate': 0.0013284169124877089, 'epoch': 0.56}\n",
      "{'loss': 2.0233, 'grad_norm': 0.10738459974527359, 'learning_rate': 0.00131858407079646, 'epoch': 0.56}\n",
      "{'loss': 2.0238, 'grad_norm': 0.12968949973583221, 'learning_rate': 0.0013087512291052113, 'epoch': 0.56}\n",
      "{'loss': 2.1095, 'grad_norm': 0.13404960930347443, 'learning_rate': 0.0012989183874139627, 'epoch': 0.57}\n",
      "{'loss': 2.0192, 'grad_norm': 0.145272895693779, 'learning_rate': 0.001289085545722714, 'epoch': 0.57}\n",
      "{'loss': 2.1054, 'grad_norm': 0.11323117464780807, 'learning_rate': 0.001279252704031465, 'epoch': 0.57}\n",
      "{'loss': 2.0194, 'grad_norm': 0.21580497920513153, 'learning_rate': 0.0012694198623402163, 'epoch': 0.58}\n",
      "{'loss': 2.215, 'grad_norm': 0.10965774953365326, 'learning_rate': 0.0012595870206489675, 'epoch': 0.58}\n",
      "{'loss': 2.0988, 'grad_norm': 0.12492102384567261, 'learning_rate': 0.0012497541789577187, 'epoch': 0.58}\n",
      "{'loss': 2.152, 'grad_norm': 0.09959553182125092, 'learning_rate': 0.0012399213372664701, 'epoch': 0.59}\n",
      "{'loss': 1.9777, 'grad_norm': 0.19742460548877716, 'learning_rate': 0.0012300884955752213, 'epoch': 0.59}\n",
      "{'loss': 2.024, 'grad_norm': 0.2322075515985489, 'learning_rate': 0.0012202556538839725, 'epoch': 0.59}\n",
      "{'loss': 2.0886, 'grad_norm': 0.1504949927330017, 'learning_rate': 0.0012104228121927237, 'epoch': 0.6}\n",
      "{'loss': 2.1327, 'grad_norm': 0.14147457480430603, 'learning_rate': 0.001200589970501475, 'epoch': 0.6}\n",
      "{'loss': 2.1814, 'grad_norm': 0.2788655161857605, 'learning_rate': 0.0011907571288102263, 'epoch': 0.6}\n",
      "{'loss': 2.1243, 'grad_norm': 0.10360699146986008, 'learning_rate': 0.0011809242871189775, 'epoch': 0.61}\n",
      "{'loss': 2.1362, 'grad_norm': 0.11086824536323547, 'learning_rate': 0.0011710914454277287, 'epoch': 0.61}\n",
      "{'loss': 2.1015, 'grad_norm': 0.16342449188232422, 'learning_rate': 0.0011612586037364797, 'epoch': 0.61}\n",
      "{'loss': 1.9819, 'grad_norm': 0.07171619683504105, 'learning_rate': 0.001151425762045231, 'epoch': 0.62}\n",
      "{'loss': 2.0872, 'grad_norm': 0.1580081433057785, 'learning_rate': 0.0011415929203539823, 'epoch': 0.62}\n",
      "{'loss': 2.1059, 'grad_norm': 0.12287366390228271, 'learning_rate': 0.0011317600786627335, 'epoch': 0.62}\n",
      "{'loss': 2.1191, 'grad_norm': 0.17970195412635803, 'learning_rate': 0.0011219272369714847, 'epoch': 0.63}\n",
      "{'loss': 2.1195, 'grad_norm': 0.18622379004955292, 'learning_rate': 0.001112094395280236, 'epoch': 0.63}\n",
      "{'loss': 2.1318, 'grad_norm': 0.13700033724308014, 'learning_rate': 0.0011022615535889871, 'epoch': 0.63}\n",
      "{'loss': 2.1062, 'grad_norm': 0.1754702925682068, 'learning_rate': 0.0010924287118977386, 'epoch': 0.64}\n",
      "{'loss': 2.0574, 'grad_norm': 0.08014371991157532, 'learning_rate': 0.0010825958702064898, 'epoch': 0.64}\n",
      "{'loss': 2.0986, 'grad_norm': 0.11819598823785782, 'learning_rate': 0.001072763028515241, 'epoch': 0.64}\n",
      "{'loss': 2.211, 'grad_norm': 0.21947865188121796, 'learning_rate': 0.0010629301868239922, 'epoch': 0.65}\n",
      "{'loss': 1.9918, 'grad_norm': 0.10896942019462585, 'learning_rate': 0.0010530973451327434, 'epoch': 0.65}\n",
      "{'loss': 2.0538, 'grad_norm': 0.21349452435970306, 'learning_rate': 0.0010432645034414946, 'epoch': 0.65}\n",
      "{'loss': 2.0737, 'grad_norm': 0.20775486528873444, 'learning_rate': 0.001033431661750246, 'epoch': 0.66}\n",
      "{'loss': 2.0723, 'grad_norm': 0.08849681168794632, 'learning_rate': 0.0010235988200589972, 'epoch': 0.66}\n",
      "{'loss': 2.1219, 'grad_norm': 0.09209007769823074, 'learning_rate': 0.0010137659783677484, 'epoch': 0.66}\n",
      "{'loss': 2.1733, 'grad_norm': 0.1814626157283783, 'learning_rate': 0.0010039331366764996, 'epoch': 0.67}\n",
      "{'loss': 2.0379, 'grad_norm': 0.10022138059139252, 'learning_rate': 0.0009941002949852506, 'epoch': 0.67}\n",
      "{'loss': 2.0987, 'grad_norm': 0.20000521838665009, 'learning_rate': 0.000984267453294002, 'epoch': 0.67}\n",
      "{'loss': 2.149, 'grad_norm': 0.10778775811195374, 'learning_rate': 0.0009744346116027532, 'epoch': 0.68}\n",
      "{'loss': 2.0643, 'grad_norm': 0.1806810051202774, 'learning_rate': 0.0009646017699115044, 'epoch': 0.68}\n",
      "{'loss': 2.0238, 'grad_norm': 0.19696371257305145, 'learning_rate': 0.0009547689282202556, 'epoch': 0.68}\n",
      "{'loss': 1.9925, 'grad_norm': 0.1075703352689743, 'learning_rate': 0.0009449360865290069, 'epoch': 0.68}\n",
      "{'loss': 2.0787, 'grad_norm': 0.18371295928955078, 'learning_rate': 0.0009351032448377581, 'epoch': 0.69}\n",
      "{'loss': 1.8965, 'grad_norm': 0.17271913588047028, 'learning_rate': 0.0009252704031465093, 'epoch': 0.69}\n",
      "{'loss': 2.1215, 'grad_norm': 0.13600124418735504, 'learning_rate': 0.0009154375614552606, 'epoch': 0.69}\n",
      "{'loss': 2.0622, 'grad_norm': 0.2677861452102661, 'learning_rate': 0.0009056047197640118, 'epoch': 0.7}\n",
      "{'loss': 2.0979, 'grad_norm': 0.17261305451393127, 'learning_rate': 0.000895771878072763, 'epoch': 0.7}\n",
      "{'loss': 2.1881, 'grad_norm': 0.1758430302143097, 'learning_rate': 0.0008859390363815143, 'epoch': 0.7}\n",
      "{'loss': 2.1086, 'grad_norm': 0.17631912231445312, 'learning_rate': 0.0008761061946902655, 'epoch': 0.71}\n",
      "{'loss': 2.1247, 'grad_norm': 0.12669110298156738, 'learning_rate': 0.0008662733529990168, 'epoch': 0.71}\n",
      "{'loss': 2.1696, 'grad_norm': 0.14407815039157867, 'learning_rate': 0.000856440511307768, 'epoch': 0.71}\n",
      "{'loss': 2.1212, 'grad_norm': 0.2373429834842682, 'learning_rate': 0.0008466076696165192, 'epoch': 0.72}\n",
      "{'loss': 1.9779, 'grad_norm': 0.15270765125751495, 'learning_rate': 0.0008367748279252705, 'epoch': 0.72}\n",
      "{'loss': 2.0065, 'grad_norm': 0.18871189653873444, 'learning_rate': 0.0008269419862340217, 'epoch': 0.72}\n",
      "{'loss': 1.9524, 'grad_norm': 0.15414345264434814, 'learning_rate': 0.0008171091445427728, 'epoch': 0.73}\n",
      "{'loss': 2.1397, 'grad_norm': 0.20414860546588898, 'learning_rate': 0.000807276302851524, 'epoch': 0.73}\n",
      "{'loss': 2.0723, 'grad_norm': 0.13262373208999634, 'learning_rate': 0.0007974434611602752, 'epoch': 0.73}\n",
      "{'loss': 2.0616, 'grad_norm': 0.09791100770235062, 'learning_rate': 0.0007876106194690265, 'epoch': 0.74}\n",
      "{'loss': 2.195, 'grad_norm': 0.1950940191745758, 'learning_rate': 0.0007777777777777777, 'epoch': 0.74}\n",
      "{'loss': 1.9691, 'grad_norm': 0.2035345584154129, 'learning_rate': 0.000767944936086529, 'epoch': 0.74}\n",
      "{'loss': 2.0254, 'grad_norm': 0.141787588596344, 'learning_rate': 0.0007581120943952802, 'epoch': 0.75}\n",
      "{'loss': 1.9806, 'grad_norm': 0.10446630418300629, 'learning_rate': 0.0007482792527040314, 'epoch': 0.75}\n",
      "{'loss': 2.0304, 'grad_norm': 0.15824736654758453, 'learning_rate': 0.0007384464110127828, 'epoch': 0.75}\n",
      "{'loss': 1.9956, 'grad_norm': 0.2658950984477997, 'learning_rate': 0.000728613569321534, 'epoch': 0.76}\n",
      "{'loss': 2.103, 'grad_norm': 0.10241495817899704, 'learning_rate': 0.0007187807276302852, 'epoch': 0.76}\n",
      "{'loss': 2.0987, 'grad_norm': 0.19272403419017792, 'learning_rate': 0.0007089478859390365, 'epoch': 0.76}\n",
      "{'loss': 2.1084, 'grad_norm': 0.13389527797698975, 'learning_rate': 0.0006991150442477877, 'epoch': 0.77}\n",
      "{'loss': 2.074, 'grad_norm': 0.17638978362083435, 'learning_rate': 0.0006892822025565389, 'epoch': 0.77}\n",
      "{'loss': 2.1226, 'grad_norm': 0.25008803606033325, 'learning_rate': 0.0006794493608652901, 'epoch': 0.77}\n",
      "{'loss': 2.1789, 'grad_norm': 0.13113942742347717, 'learning_rate': 0.0006696165191740413, 'epoch': 0.78}\n",
      "{'loss': 2.0789, 'grad_norm': 0.16569678485393524, 'learning_rate': 0.0006597836774827926, 'epoch': 0.78}\n",
      "{'loss': 2.2061, 'grad_norm': 0.09409234672784805, 'learning_rate': 0.0006499508357915438, 'epoch': 0.78}\n",
      "{'loss': 2.0367, 'grad_norm': 0.08601563423871994, 'learning_rate': 0.000640117994100295, 'epoch': 0.79}\n",
      "{'loss': 1.9666, 'grad_norm': 0.13449932634830475, 'learning_rate': 0.0006302851524090463, 'epoch': 0.79}\n",
      "{'loss': 2.0664, 'grad_norm': 0.10624577850103378, 'learning_rate': 0.0006204523107177975, 'epoch': 0.79}\n",
      "{'loss': 2.0912, 'grad_norm': 0.10589089244604111, 'learning_rate': 0.0006106194690265487, 'epoch': 0.8}\n",
      "{'loss': 2.1084, 'grad_norm': 0.10377585887908936, 'learning_rate': 0.0006007866273352999, 'epoch': 0.8}\n",
      "{'loss': 2.0456, 'grad_norm': 0.1365300863981247, 'learning_rate': 0.0005909537856440511, 'epoch': 0.8}\n",
      "{'loss': 2.1183, 'grad_norm': 0.11632813513278961, 'learning_rate': 0.0005811209439528024, 'epoch': 0.81}\n",
      "{'loss': 2.0272, 'grad_norm': 0.12524496018886566, 'learning_rate': 0.0005712881022615536, 'epoch': 0.81}\n",
      "{'loss': 1.9835, 'grad_norm': 0.11931510269641876, 'learning_rate': 0.0005614552605703048, 'epoch': 0.81}\n",
      "{'loss': 2.0213, 'grad_norm': 0.11747539788484573, 'learning_rate': 0.0005516224188790561, 'epoch': 0.82}\n",
      "{'loss': 2.0575, 'grad_norm': 0.19808709621429443, 'learning_rate': 0.0005417895771878073, 'epoch': 0.82}\n",
      "{'loss': 2.2397, 'grad_norm': 0.1453760862350464, 'learning_rate': 0.0005319567354965585, 'epoch': 0.82}\n",
      "{'loss': 2.1577, 'grad_norm': 0.17106755077838898, 'learning_rate': 0.0005221238938053098, 'epoch': 0.83}\n",
      "{'loss': 2.0621, 'grad_norm': 0.11182639747858047, 'learning_rate': 0.0005122910521140609, 'epoch': 0.83}\n",
      "{'loss': 2.1054, 'grad_norm': 0.11173469573259354, 'learning_rate': 0.0005024582104228122, 'epoch': 0.83}\n",
      "{'loss': 2.181, 'grad_norm': 0.14349424839019775, 'learning_rate': 0.0004926253687315634, 'epoch': 0.84}\n",
      "{'loss': 2.056, 'grad_norm': 0.13732507824897766, 'learning_rate': 0.00048279252704031467, 'epoch': 0.84}\n",
      "{'loss': 2.0175, 'grad_norm': 0.12119109183549881, 'learning_rate': 0.00047295968534906587, 'epoch': 0.84}\n",
      "{'loss': 2.0155, 'grad_norm': 0.09347197413444519, 'learning_rate': 0.0004631268436578171, 'epoch': 0.85}\n",
      "{'loss': 2.0527, 'grad_norm': 0.13475273549556732, 'learning_rate': 0.0004532940019665684, 'epoch': 0.85}\n",
      "{'loss': 2.128, 'grad_norm': 0.1168619766831398, 'learning_rate': 0.00044346116027531963, 'epoch': 0.85}\n",
      "{'loss': 2.1079, 'grad_norm': 0.12046689540147781, 'learning_rate': 0.00043362831858407083, 'epoch': 0.86}\n",
      "{'loss': 2.098, 'grad_norm': 0.12203817069530487, 'learning_rate': 0.000423795476892822, 'epoch': 0.86}\n",
      "{'loss': 2.0234, 'grad_norm': 0.10036695748567581, 'learning_rate': 0.00041396263520157323, 'epoch': 0.86}\n",
      "{'loss': 2.1732, 'grad_norm': 0.07235629111528397, 'learning_rate': 0.0004041297935103245, 'epoch': 0.87}\n",
      "{'loss': 2.082, 'grad_norm': 0.11885232478380203, 'learning_rate': 0.0003942969518190757, 'epoch': 0.87}\n",
      "{'loss': 2.0978, 'grad_norm': 0.09314456582069397, 'learning_rate': 0.00038446411012782694, 'epoch': 0.87}\n",
      "{'loss': 2.0368, 'grad_norm': 0.10457373410463333, 'learning_rate': 0.0003746312684365782, 'epoch': 0.88}\n",
      "{'loss': 2.1225, 'grad_norm': 0.08589217066764832, 'learning_rate': 0.00036479842674532945, 'epoch': 0.88}\n",
      "{'loss': 2.123, 'grad_norm': 0.21741698682308197, 'learning_rate': 0.00035496558505408065, 'epoch': 0.88}\n",
      "{'loss': 2.0789, 'grad_norm': 0.19679197669029236, 'learning_rate': 0.00034513274336283185, 'epoch': 0.88}\n",
      "{'loss': 2.0923, 'grad_norm': 0.5002641677856445, 'learning_rate': 0.0003352999016715831, 'epoch': 0.89}\n",
      "{'loss': 2.009, 'grad_norm': 0.1491086483001709, 'learning_rate': 0.00032546705998033436, 'epoch': 0.89}\n",
      "{'loss': 2.0061, 'grad_norm': 0.2261446863412857, 'learning_rate': 0.00031563421828908556, 'epoch': 0.89}\n",
      "{'loss': 2.0597, 'grad_norm': 0.12652045488357544, 'learning_rate': 0.00030580137659783676, 'epoch': 0.9}\n",
      "{'loss': 2.0422, 'grad_norm': 0.12976567447185516, 'learning_rate': 0.000295968534906588, 'epoch': 0.9}\n",
      "{'loss': 2.1137, 'grad_norm': 0.10367868840694427, 'learning_rate': 0.00028613569321533927, 'epoch': 0.9}\n",
      "{'loss': 2.0812, 'grad_norm': 0.12171842157840729, 'learning_rate': 0.00027630285152409047, 'epoch': 0.91}\n",
      "{'loss': 2.1808, 'grad_norm': 0.16591085493564606, 'learning_rate': 0.00026647000983284167, 'epoch': 0.91}\n",
      "{'loss': 2.0156, 'grad_norm': 0.2485840767621994, 'learning_rate': 0.0002566371681415929, 'epoch': 0.91}\n",
      "{'loss': 2.0145, 'grad_norm': 0.11209743469953537, 'learning_rate': 0.0002468043264503442, 'epoch': 0.92}\n",
      "{'loss': 2.138, 'grad_norm': 0.15002062916755676, 'learning_rate': 0.0002369714847590954, 'epoch': 0.92}\n",
      "{'loss': 2.0805, 'grad_norm': 0.27771198749542236, 'learning_rate': 0.0002271386430678466, 'epoch': 0.92}\n",
      "{'loss': 2.0618, 'grad_norm': 0.12712696194648743, 'learning_rate': 0.00021730580137659783, 'epoch': 0.93}\n",
      "{'loss': 1.965, 'grad_norm': 0.09936794638633728, 'learning_rate': 0.00020747295968534906, 'epoch': 0.93}\n",
      "{'loss': 1.9701, 'grad_norm': 0.14131443202495575, 'learning_rate': 0.00019764011799410032, 'epoch': 0.93}\n",
      "{'loss': 2.035, 'grad_norm': 0.19454747438430786, 'learning_rate': 0.00018780727630285154, 'epoch': 0.94}\n",
      "{'loss': 2.0262, 'grad_norm': 0.06958210468292236, 'learning_rate': 0.00017797443461160277, 'epoch': 0.94}\n",
      "{'loss': 2.0953, 'grad_norm': 0.11009545624256134, 'learning_rate': 0.00016814159292035397, 'epoch': 0.94}\n",
      "{'loss': 2.12, 'grad_norm': 0.08732803910970688, 'learning_rate': 0.00015830875122910522, 'epoch': 0.95}\n",
      "{'loss': 2.0427, 'grad_norm': 0.1437341272830963, 'learning_rate': 0.00014847590953785645, 'epoch': 0.95}\n",
      "{'loss': 2.0682, 'grad_norm': 0.1754114180803299, 'learning_rate': 0.00013864306784660768, 'epoch': 0.95}\n",
      "{'loss': 2.0643, 'grad_norm': 0.18752145767211914, 'learning_rate': 0.0001288102261553589, 'epoch': 0.96}\n",
      "{'loss': 2.0774, 'grad_norm': 0.10066819936037064, 'learning_rate': 0.00011897738446411013, 'epoch': 0.96}\n",
      "{'loss': 2.0292, 'grad_norm': 0.10796288400888443, 'learning_rate': 0.00010914454277286135, 'epoch': 0.96}\n",
      "{'loss': 2.0548, 'grad_norm': 0.14496716856956482, 'learning_rate': 9.931170108161259e-05, 'epoch': 0.97}\n",
      "{'loss': 2.0841, 'grad_norm': 0.12287872284650803, 'learning_rate': 8.947885939036382e-05, 'epoch': 0.97}\n",
      "{'loss': 2.0994, 'grad_norm': 0.12004754692316055, 'learning_rate': 7.964601769911504e-05, 'epoch': 0.97}\n",
      "{'loss': 2.2112, 'grad_norm': 0.1857626885175705, 'learning_rate': 6.981317600786627e-05, 'epoch': 0.98}\n",
      "{'loss': 2.0944, 'grad_norm': 0.19573380053043365, 'learning_rate': 5.99803343166175e-05, 'epoch': 0.98}\n",
      "{'loss': 2.2206, 'grad_norm': 0.11798736453056335, 'learning_rate': 5.0147492625368727e-05, 'epoch': 0.98}\n",
      "{'loss': 2.1654, 'grad_norm': 0.21904225647449493, 'learning_rate': 4.031465093411996e-05, 'epoch': 0.99}\n",
      "{'loss': 2.1006, 'grad_norm': 0.10030063986778259, 'learning_rate': 3.0481809242871188e-05, 'epoch': 0.99}\n",
      "{'loss': 2.0839, 'grad_norm': 0.17804986238479614, 'learning_rate': 2.064896755162242e-05, 'epoch': 0.99}\n",
      "{'loss': 2.0418, 'grad_norm': 0.15318287909030914, 'learning_rate': 1.0816125860373648e-05, 'epoch': 1.0}\n",
      "{'loss': 1.9979, 'grad_norm': 0.20338872075080872, 'learning_rate': 9.83284169124877e-07, 'epoch': 1.0}\n",
      "{'train_runtime': 1757.6204, 'train_samples_per_second': 13.888, 'train_steps_per_second': 1.736, 'train_loss': 2.121190328279193, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3051, training_loss=2.121190328279193, metrics={'train_runtime': 1757.6204, 'train_samples_per_second': 13.888, 'train_steps_per_second': 1.736, 'train_loss': 2.121190328279193, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: 考试有哪些技巧？\\n\\nAssistant: 考试技巧通常包括以下几个部分:\\n\\n1. 如何分配答题时间：一般来说，建议大家将每道题目分为多个部分，并且每个部分不要超过10分钟的答题时间，这样有助于提高答题的效率。\\n\\n2. 保持积极的心态：在考试中面对紧张和巨大的压力时，良好的心态非常重要。保持积极的心态有助于保持冷静和集中注意力，从而提高考试的效率。\\n\\n3. 阅读题本：阅读题本有助于你快速浏览试题，了解各道题目的考察对象和试题'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "ipt = tokenizer(\"Human: {}\\n{}\".format(\"考试有哪些技巧？\", \"\").strip() + \"\\n\\nAssistant: \", return_tensors=\"pt\").to(model.device)\n",
    "tokenizer.decode(model.generate(**ipt, max_length=128, do_sample=True)[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
