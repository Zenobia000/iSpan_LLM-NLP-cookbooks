{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 17:38:23,899 - modelscope - INFO - PyTorch version 2.1.0+cu121 Found.\n",
      "2024-03-01 17:38:23,901 - modelscope - INFO - TensorFlow version 2.14.0 Found.\n",
      "2024-03-01 17:38:23,902 - modelscope - INFO - Loading ast index from C:\\Users\\user\\.cache\\modelscope\\ast_indexer\n",
      "2024-03-01 17:38:24,049 - modelscope - INFO - Loading done! Current index file version is 1.12.0, with md5 864828a934de107f4c8cbb2035d3ad5f and a total number of 964 components indexed\n"
     ]
    }
   ],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id 模型id\n",
    "# cache_dir 模型下载路径\n",
    "# ignore_file_pattern 忽略文件模式\n",
    "snapshot_download(model_id=\"Shanghai_AI_Laboratory/internlm-20b\", cache_dir=\"d:/Pretrained_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5781389e52614d4c813636fe08b5ad5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login() # 登录huggingface hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f6de4ae169419ab37ca4bc848138e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--yentinglin--Taiwan-LLM-7B-v2.1-chat. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a802ace30354241a076312bfbc6eddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088c28a01b3947799756f12c03a8c091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9add57e8fc433fa1c1993a59de841c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127f14410ecb4cbc850e353b842e9a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecab490f9ab846e3b78d004a5929a799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27658b138ab94c409c2e3e71f50517ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71973acbd1214ab3893b365bbdf8f125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yentinglin/Taiwan-LLM-7B-v2.1-chat\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"yentinglin/Taiwan-LLM-7B-v2.1-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722f1887f51943ae89ddc0ea45f31569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是一個人工智慧助理</s>USER: 東北季風如何影響台灣氣候？</s>ASSISTANT: 東北季風是一個季節性的風，在冬季從中國內地向台灣吹來。它們帶來了溼氣和水汽，導致降雨和溫暖的氣候。\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers>=4.34\n",
    "# pip install accelerate\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"yentinglin/Taiwan-LLM-7B-v2.1-chat\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"你是一個人工智慧助理\",\n",
    "        },\n",
    "\n",
    "    {   \"role\": \"user\", \n",
    "        \"content\": \"東北季風如何影響台灣氣候？\"\n",
    "        },\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是一個唱跳歌手</s>USER: 以蜂蜜牛奶創作一首歌？</s>ASSISTANT: 當然，我可以幫你創作一首以蜂蜜牛奶為靈感的歌曲。你想要一個旋律嗎？還是想要我創作一些歌詞？\n"
     ]
    }
   ],
   "source": [
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"你是一個唱跳歌手\",\n",
    "        },\n",
    "\n",
    "    {   \"role\": \"user\", \n",
    "        \"content\": \"以蜂蜜牛奶創作一首歌？\"\n",
    "        },\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好\n",
      "ASSISTANT: 你好,你好嗎?\n",
      "中國四大發明?\n",
      "ASSISTANT: 四大發明指的是中國古代發明的四種發明。 它們是造紙術、火藥、指南針和印刷術。\n",
      "請詳解是每一個產品的發明家是誰\n",
      "ASSISTANT: 你好!\n",
      "\n",
      "ASSISTANT: 蒸汽機, 造紙術, 火藥和指南針。Ћ\n",
      "\n",
      "ASSISTANT: 中國的四大發明是指在中國古代發明的四項重要發明，分別是指南針、火藥、印刷術和絲綢。\n",
      "\n",
      "指南針是中國古代的一項重要發明，它是用來指示方向的一種儀器。指南針的發明被認為是在公元前3世紀左右，當時中國的一位發明家楊衝發現了一種方法，可以使指南針指向北方。這項發明對航海和探險有著重大的影響，並幫助中國開啟了探索世界的旅程。\n",
      "\n",
      "火藥是中國古代的另\n",
      "退出\n",
      "對話結束。\n"
     ]
    }
   ],
   "source": [
    "def generate_response(messages):\n",
    "    \n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "\n",
    "    return outputs[0][\"generated_text\"]\n",
    "\n",
    "# 初始化對話列表\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"你是一個唱跳歌手\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "# 使用while循環來控制對話進行\n",
    "while True:\n",
    "    # 獲取用戶輸入\n",
    "    user_input = input()\n",
    "    print(user_input)\n",
    "    # 檢查是否需要停止對話\n",
    "    if user_input == \"退出\":\n",
    "        print(\"對話結束。\")\n",
    "        break\n",
    "    \n",
    "    # 將用戶輸入添加到對話列表\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # 生成回應\n",
    "    response = generate_response(messages)\n",
    "    print(response.split(\"</s>\")[-1])\n",
    "    \n",
    "    # 將生成的回應也添加到對話列表，這裡僅作示範，實際應用中應將生成的文本添加\n",
    "    messages.append({\"role\": \"assistance\", \"content\": response})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': '你是一個唱跳歌手'},\n",
       " {'role': 'user', 'content': '你好'},\n",
       " {'role': 'system',\n",
       "  'content': '你是一個唱跳歌手</s>USER: 你好</s>ASSISTANT: 你好，有什麼我能幫你的嗎?'},\n",
       " {'role': 'user', 'content': '唱首歌給我'},\n",
       " {'role': 'system',\n",
       "  'content': '你是一個唱跳歌手</s>USER: 你好</s>你是一個唱跳歌手</s>USER: 你好</s>ASSISTANT: 你好，有什麼我能幫你的嗎?</s>USER: 唱首歌給我</s>ASSISTANT: 你想聽什麼樣的歌?'},\n",
       " {'role': 'user', 'content': '周杰倫 盜香'},\n",
       " {'role': 'system',\n",
       "  'content': '你是一個唱跳歌手</s>USER: 你好</s>你是一個唱跳歌手</s>USER: 你好</s>ASSISTANT: 你好，有什麼我能幫你的嗎?</s>USER: 唱首歌給我</s>你是一個唱跳歌手</s>USER: 你好</s>你是一個唱跳歌手</s>USER: 你好</s>ASSISTANT: 你好，有什麼我能幫你的嗎?</s>USER: 唱首歌給我</s>ASSISTANT: 你想聽什麼樣的歌?</s>USER: 周杰倫 盜香</s>ASSISTANT: 你想要一首歌,還是一個旋律?'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
